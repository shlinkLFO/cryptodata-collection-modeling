{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Data Loading & Preprocessing ---\n",
      "Loading data from: C:\\Users\\mason\\AVP\\BTCUSDrec.csv\n",
      "Raw data loaded. Shape: (15177, 9)\n",
      "Using 'date' column for timestamp.\n",
      "Data prepared for feature engineering. Shape: (15177, 7)\n",
      "\n",
      "--- 2. Feature Engineering (Script B Logic) ---\n",
      "Generating features based on 'Simpler Script B' logic...\n",
      "  Generating hour features from 'timestamp'.\n",
      "  Generating day features from 'timestamp'.\n",
      "Feature generation complete.\n",
      "Feature calculation completed in 0.03 seconds.\n",
      "Total columns after feature engineering: 87\n",
      "Identified 74 potential features from Script B generation.\n",
      "\n",
      "--- 3.5 Cleaning Infinities (BEFORE Filtering) ---\n",
      "Found 10 infinite values. Replacing with NaN BEFORE filtering.\n",
      "Infinite values replaced with NaN.\n",
      "\n",
      "--- 4. Feature Selection: Variance Threshold (Threshold: 0.0001) ---\n",
      "Features before variance check: 74\n",
      "  Imputing 1697 NaNs with medians temporarily for VarianceThreshold fitting.\n",
      "Removed 2 features with variance <= 0.0001.\n",
      "  Dropped by Variance Threshold: ['garman_klass_12h', 'parkinson_3h']\n",
      "Features remaining after variance check: 72\n",
      "\n",
      "--- Feature Selection: Calculating VIF (Threshold: 4.19) ---\n",
      "Numeric features before VIF check: 72\n",
      "  Warning: Found 1684 NaNs. Imputing with column medians for VIF calculation ONLY.\n",
      "  Warning: Found NaN/Inf VIF for: ['hour_0', 'hour_1', 'hour_2', 'hour_3', 'hour_4', 'hour_5', 'hour_6', 'hour_7', 'hour_8', 'hour_9', 'hour_10', 'hour_11', 'hour_12', 'hour_13', 'hour_14', 'hour_15', 'hour_16', 'hour_17', 'hour_18', 'hour_19', 'hour_20', 'hour_21', 'hour_22', 'hour_23', 'day_0', 'day_1', 'day_2', 'day_3', 'day_4', 'day_5', 'day_6']. Removing first one.\n",
      "  Dropping feature 'hour_0' due to NaN/Inf VIF.\n",
      "  Dropping 'day_3' (VIF: 192514.24)\n",
      "  Dropping 'close_div_ma_48h' (VIF: 147298.01)\n",
      "  Dropping 'ma_6h' (VIF: 27719.38)\n",
      "  Dropping 'ma12_div_ma48' (VIF: 15184.19)\n",
      "  Dropping 'close_div_ma_168h' (VIF: 10804.30)\n",
      "  Dropping 'ma_12h' (VIF: 10378.54)\n",
      "  Dropping 'ma_48h' (VIF: 5043.08)\n",
      "  Dropping 'ma_24h' (VIF: 3738.32)\n",
      "  Dropping 'close_div_ma_24h' (VIF: 1389.08)\n",
      "  Dropping 'ma_72h' (VIF: 1292.23)\n",
      "  Dropping 'rolling_std_12h_sqrt' (VIF: 596.01)\n",
      "  Dropping 'ma_3h' (VIF: 282.35)\n",
      "  Dropping 'atr_24h' (VIF: 174.77)\n",
      "  Dropping 'rolling_std_72h' (VIF: 90.58)\n",
      "  Dropping 'ma24_div_ma168' (VIF: 73.76)\n",
      "  Dropping 'atr_48h' (VIF: 58.21)\n",
      "  Dropping 'rolling_std_12h' (VIF: 51.30)\n",
      "  Dropping 'rolling_std_48h' (VIF: 35.36)\n",
      "  Dropping 'ma_168h' (VIF: 19.41)\n",
      "  Dropping 'rolling_std_24h' (VIF: 17.01)\n",
      "  Dropping 'rolling_std_168h' (VIF: 15.15)\n",
      "  Dropping 'price_range_pct' (VIF: 13.18)\n",
      "  Dropping 'std12_div_std72' (VIF: 10.15)\n",
      "  Dropping 'rolling_std_3h' (VIF: 9.91)\n",
      "  Dropping 'rolling_std_6h' (VIF: 6.34)\n",
      "  Max VIF (3.46) is below threshold 4.19. Stopping.\n",
      "Removed 26 features based on VIF.\n",
      "  Features Dropped by VIF: ['atr_24h', 'atr_48h', 'close_div_ma_168h', 'close_div_ma_24h', 'close_div_ma_48h', 'day_3', 'hour_0', 'ma12_div_ma48', 'ma24_div_ma168', 'ma_12h', 'ma_168h', 'ma_24h', 'ma_3h', 'ma_48h', 'ma_6h', 'ma_72h', 'price_range_pct', 'rolling_std_12h', 'rolling_std_12h_sqrt', 'rolling_std_168h', 'rolling_std_24h', 'rolling_std_3h', 'rolling_std_48h', 'rolling_std_6h', 'rolling_std_72h', 'std12_div_std72']\n",
      "Features remaining after VIF check: 46\n",
      "--- VIF calculation finished. ---\n",
      "\n",
      "--- 6. Final Feature Selection Summary ---\n",
      "Total potential features from Script B: 74\n",
      "Features after Variance Threshold (0.0001): 72\n",
      "Features after VIF Threshold (4.19):      46\n",
      "\n",
      "Final list of selected feature names (to use in modeling script):\n",
      "\n",
      "SELECTED_FEATURE_NAMES = [\n",
      "    'atr_14h',\n",
      "    'day_0',\n",
      "    'day_1',\n",
      "    'day_2',\n",
      "    'day_4',\n",
      "    'day_5',\n",
      "    'day_6',\n",
      "    'hour_1',\n",
      "    'hour_10',\n",
      "    'hour_11',\n",
      "    'hour_12',\n",
      "    'hour_13',\n",
      "    'hour_14',\n",
      "    'hour_15',\n",
      "    'hour_16',\n",
      "    'hour_17',\n",
      "    'hour_18',\n",
      "    'hour_19',\n",
      "    'hour_2',\n",
      "    'hour_20',\n",
      "    'hour_21',\n",
      "    'hour_22',\n",
      "    'hour_23',\n",
      "    'hour_3',\n",
      "    'hour_4',\n",
      "    'hour_5',\n",
      "    'hour_6',\n",
      "    'hour_7',\n",
      "    'hour_8',\n",
      "    'hour_9',\n",
      "    'lag_12h_price_return',\n",
      "    'lag_12h_volume_return',\n",
      "    'lag_168h_price_return',\n",
      "    'lag_24h_price_return',\n",
      "    'lag_24h_volume_return',\n",
      "    'lag_3h_price_return',\n",
      "    'lag_3h_volume_return',\n",
      "    'lag_48h_price_return',\n",
      "    'lag_6h_price_return',\n",
      "    'lag_6h_volume_return',\n",
      "    'lag_72h_price_return',\n",
      "    'oc_change_pct',\n",
      "    'price_return_1h_sq',\n",
      "    'rolling_std_3h_sq',\n",
      "    'volume_btc_x_range',\n",
      "    'volume_return_1h',\n",
      "]\n",
      "\n",
      "Total selected features: 46\n",
      "\n",
      "Feature selection script finished.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "\n",
    "# Feature Engineering Imports\n",
    "import pandas_ta as ta  # Technical indicators (Needed for feature generation below)\n",
    "\n",
    "# Feature Selection Imports\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.sm_exceptions import PerfectSeparationError # For handling VIF errors\n",
    "from sklearn.impute import SimpleImputer # Needed for VIF imputation\n",
    "\n",
    "# --- Suppress Warnings ---\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)\n",
    "warnings.filterwarnings('ignore') # General suppression\n",
    "\n",
    "# --- Configuration ---\n",
    "CSV_FILE_PATH = r'C:\\Users\\mason\\AVP\\BTCUSDrec.csv' # Use raw string for Windows paths\n",
    "\n",
    "# Feature Selection Thresholds\n",
    "VARIANCE_THRESHOLD = 0.0001  # Remove features with zero or very low variance\n",
    "VIF_THRESHOLD = 4.19      # Remove features iteratively with VIF >= this value (Using 5 as a start)\n",
    "\n",
    "# Define columns that are NOT features (Identifiers, Raw Data)\n",
    "# Add any other non-feature columns generated by the function below\n",
    "NON_FEATURE_COLS_SCRIPT_B = [\n",
    "    'unix', 'date', 'symbol', 'open', 'high', 'low', 'close',\n",
    "    'Volume BTC', 'Volume USD', # Keep original names as they might be used\n",
    "    'timestamp', # Added if generated\n",
    "    'price_change_1h', # Intermediate\n",
    "    'price_change_12h', # Target from Script B\n",
    "    'price_return_1h_feat', # Intermediate if not dropped inside function\n",
    "    'prev_close', 'high_minus_low', 'high_minus_prev_close', 'low_minus_prev_close', 'true_range' # ATR intermediates\n",
    "]\n",
    "\n",
    "# --- Feature Engineering Function (Copied from your \"Simpler Script B\" - CORRECTED TIME FEATURES) ---\n",
    "def calculate_script_b_features(df):\n",
    "    \"\"\"\n",
    "    Generates the feature set used in the 'Simpler Script B'.\n",
    "    Based on the provided script structure.\n",
    "    \"\"\"\n",
    "    print(\"Generating features based on 'Simpler Script B' logic...\")\n",
    "    df = df.copy() # Work on a copy\n",
    "\n",
    "    # --- Ensure 'timestamp' exists EARLY ---\n",
    "    if 'timestamp' not in df.columns:\n",
    "        if 'date' in df.columns:\n",
    "             print(\"  Generating 'timestamp' from 'date' column.\")\n",
    "             df['timestamp'] = pd.to_datetime(df['date'])\n",
    "        elif 'unix' in df.columns:\n",
    "             print(\"  Generating 'timestamp' from 'unix' column.\")\n",
    "             df['timestamp'] = pd.to_datetime(df['unix'], unit='ms') # Assuming unix is ms\n",
    "        else:\n",
    "             print(\"Error: Cannot find suitable column ('timestamp', 'date', or 'unix') for time features.\")\n",
    "             return pd.DataFrame()\n",
    "    else:\n",
    "        # Ensure timestamp is datetime type if it already exists\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "    # Sort by timestamp after ensuring it exists and is datetime\n",
    "    df = df.sort_values('timestamp').reset_index(drop=True) # Reset index after sort\n",
    "\n",
    "    # --- Ensure necessary OHLCV columns exist ---\n",
    "    required = ['open', 'high', 'low', 'close', 'Volume BTC', 'Volume USD']\n",
    "    if not all(col in df.columns for col in required):\n",
    "        missing = [col for col in required if col not in df.columns]\n",
    "        print(f\"Error: Missing required base columns for feature generation: {missing}\")\n",
    "        return pd.DataFrame() # Return empty if base columns missing\n",
    "    # Convert to numeric AFTER ensuring they exist\n",
    "    for col in required:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    df = df.dropna(subset=required) # Drop if conversion creates NaNs\n",
    "    if df.empty: return pd.DataFrame()\n",
    "\n",
    "\n",
    "    # --- (Rest of the feature calculations: Basic, Volatility, MAs, Lags, etc. go here as before) ---\n",
    "    # ...\n",
    "    # Basic Features\n",
    "    df['price_change_1h'] = df['close'].pct_change() * 100\n",
    "    df['price_range_pct'] = (df['high'] - df['low']) / df['close'].replace(0, np.nan) * 100\n",
    "    df['oc_change_pct'] = (df['close'] - df['open']) / df['open'].replace(0, np.nan) * 100\n",
    "\n",
    "    # Volatility Features\n",
    "    def garman_klass_volatility(open_, high, low, close, window):\n",
    "        log_hl = np.log(np.maximum(high, 1e-9) / np.maximum(low, 1e-9))\n",
    "        log_co = np.log(np.maximum(close, 1e-9) / np.maximum(open_, 1e-9))\n",
    "        gk = 0.5 * (log_hl ** 2) - (2*np.log(2) - 1) * (log_co ** 2)\n",
    "        rolling_mean = gk.rolling(window=window).mean()\n",
    "        rolling_mean = rolling_mean.clip(lower=0)\n",
    "        return np.sqrt(rolling_mean)\n",
    "\n",
    "    def parkinson_volatility(high, low, window):\n",
    "        log_hl_sq = np.log(np.maximum(high, 1e-9) / np.maximum(low, 1e-9)) ** 2\n",
    "        rolling_sum = log_hl_sq.rolling(window=window).sum()\n",
    "        factor = 1 / (4 * np.log(2) * window)\n",
    "        return np.sqrt(factor * rolling_sum)\n",
    "\n",
    "    df['garman_klass_12h'] = garman_klass_volatility(df['open'], df['high'], df['low'], df['close'], window=12)\n",
    "    df['parkinson_3h'] = parkinson_volatility(df['high'], df['low'], window=3)\n",
    "\n",
    "    # Moving Averages & Standard Deviations\n",
    "    df['ma_3h'] = df['close'].rolling(window=3).mean()\n",
    "    df['rolling_std_3h'] = df['close'].rolling(window=3).std()\n",
    "\n",
    "    # Lagged Features\n",
    "    df['price_return_1h_feat'] = df['close'].pct_change()\n",
    "    lag_periods_price = [3, 6, 12, 24, 48, 72, 168]\n",
    "    for lag in lag_periods_price:\n",
    "        df[f'lag_{lag}h_price_return'] = df['price_return_1h_feat'].shift(lag) * 100 # Correct shift\n",
    "\n",
    "    df['volume_return_1h'] = df['Volume BTC'].pct_change() * 100\n",
    "    lag_periods_volume = [3, 6, 12, 24]\n",
    "    for lag in lag_periods_volume:\n",
    "         df[f'lag_{lag}h_volume_return'] = df['volume_return_1h'].shift(lag) # Correct shift\n",
    "\n",
    "    # Longer MAs and STDs\n",
    "    ma_periods = [6, 12, 24, 48, 72, 168]\n",
    "    for p in ma_periods:\n",
    "        df[f'ma_{p}h'] = df['close'].rolling(window=p).mean()\n",
    "    std_periods = [6, 12, 24, 48, 72, 168]\n",
    "    for p in std_periods:\n",
    "        df[f'rolling_std_{p}h'] = df['price_return_1h_feat'].rolling(window=p).std() * 100\n",
    "\n",
    "    # ATR\n",
    "    df['prev_close'] = df['close'].shift(1)\n",
    "    df['high_minus_low'] = df['high'] - df['low']\n",
    "    df['high_minus_prev_close'] = np.abs(df['high'] - df['prev_close'])\n",
    "    df['low_minus_prev_close'] = np.abs(df['low'] - df['prev_close'])\n",
    "    df['true_range'] = df[['high_minus_low', 'high_minus_prev_close', 'low_minus_prev_close']].max(axis=1)\n",
    "    atr_periods = [14, 24, 48]\n",
    "    for p in atr_periods:\n",
    "         df[f'atr_{p}h'] = df['true_range'].rolling(window=p).mean()\n",
    "    # Keep intermediate ATR columns out for now\n",
    "    # df = df.drop(columns=['prev_close', 'high_minus_low', 'high_minus_prev_close', 'low_minus_prev_close', 'true_range'])\n",
    "\n",
    "    # Trend/Interaction Features\n",
    "    epsilon = 1e-9\n",
    "    for p in [24, 48, 168]:\n",
    "        if f'ma_{p}h' in df.columns: df[f'close_div_ma_{p}h'] = df['close'] / (df[f'ma_{p}h'] + epsilon)\n",
    "    if 'ma_12h' in df.columns and 'ma_48h' in df.columns: df[f'ma12_div_ma48'] = df['ma_12h'] / (df['ma_48h'] + epsilon)\n",
    "    if 'ma_24h' in df.columns and 'ma_168h' in df.columns: df[f'ma24_div_ma168'] = df['ma_24h'] / (df['ma_168h'] + epsilon)\n",
    "    if 'rolling_std_12h' in df.columns and 'rolling_std_72h' in df.columns: df['std12_div_std72'] = df['rolling_std_12h'] / (df['rolling_std_72h'] + epsilon)\n",
    "    if 'price_range_pct' in df.columns: df['volume_btc_x_range'] = df['Volume BTC'] * df['price_range_pct']\n",
    "\n",
    "    # Non-linear Transformations\n",
    "    if 'rolling_std_3h' in df.columns: df['rolling_std_3h_sq'] = df['rolling_std_3h'] ** 2\n",
    "    if 'price_return_1h_feat' in df.columns: df['price_return_1h_sq'] = df['price_return_1h_feat'] ** 2 * 10000\n",
    "    if 'rolling_std_12h' in df.columns: df['rolling_std_12h_sqrt'] = np.sqrt(df['rolling_std_12h'].clip(lower=0) + epsilon)\n",
    "\n",
    "    # Drop intermediate price return AFTER it's used for std dev calc\n",
    "    if 'price_return_1h_feat' in df.columns:\n",
    "        df = df.drop(columns=['price_return_1h_feat'])\n",
    "\n",
    "    # --- CORRECTED Time Feature Generation ---\n",
    "    if 'timestamp' in df.columns: # Check if timestamp column exists\n",
    "        if not any(col.startswith('hour_') for col in df.columns):\n",
    "            print(\"  Generating hour features from 'timestamp'.\")\n",
    "            hour_of_day = df['timestamp'].dt.hour # Use .dt accessor on timestamp Series\n",
    "            for hour in range(24): df[f'hour_{hour}'] = (hour_of_day == hour).astype(int)\n",
    "        if not any(col.startswith('day_') for col in df.columns):\n",
    "            print(\"  Generating day features from 'timestamp'.\")\n",
    "            day_of_week = df['timestamp'].dt.dayofweek # Use .dt accessor on timestamp Series\n",
    "            for day in range(7): df[f'day_{day}'] = (day_of_week == day).astype(int)\n",
    "    else:\n",
    "        print(\"  Warning: Cannot generate time features, 'timestamp' column not found.\")\n",
    "    # --- END CORRECTION ---\n",
    "\n",
    "    print(\"Feature generation complete.\")\n",
    "    return df\n",
    "\n",
    "# --- (Keep the calculate_vif function as it was) ---\n",
    "def calculate_vif(df_features, vif_threshold=10.0, verbose=True):\n",
    "    # ... (VIF function code remains the same) ...\n",
    "    if not isinstance(df_features, pd.DataFrame) or df_features.empty:\n",
    "        print(\"VIF calculation skipped: Input is not a valid DataFrame or is empty.\")\n",
    "        return []\n",
    "\n",
    "    if verbose: print(f\"\\n--- Feature Selection: Calculating VIF (Threshold: {vif_threshold}) ---\")\n",
    "    # Select only numeric types for VIF calculation\n",
    "    features = df_features.select_dtypes(include=np.number).copy()\n",
    "    if features.empty:\n",
    "         print(\"VIF calculation skipped: No numeric features found.\")\n",
    "         return []\n",
    "\n",
    "    original_feature_count = features.shape[1]\n",
    "    if verbose: print(f\"Numeric features before VIF check: {original_feature_count}\")\n",
    "\n",
    "    # Handle NaNs robustly\n",
    "    num_nans_before = features.isnull().sum().sum()\n",
    "    if num_nans_before > 0:\n",
    "        if verbose: print(f\"  Warning: Found {num_nans_before} NaNs. Imputing with column medians for VIF calculation ONLY.\")\n",
    "        try:\n",
    "            imputer = SimpleImputer(strategy='median')\n",
    "            features_imputed_array = imputer.fit_transform(features)\n",
    "            features = pd.DataFrame(features_imputed_array, columns=features.columns, index=features.index)\n",
    "        except Exception as e_imp:\n",
    "             print(f\"  ERROR during NaN imputation: {e_imp}. VIF calculation aborted.\")\n",
    "             return df_features.columns.tolist() # Return original as fallback\n",
    "\n",
    "        num_nans_after = features.isnull().sum().sum()\n",
    "        if num_nans_after > 0:\n",
    "             print(f\"  ERROR: {num_nans_after} NaNs remain after median imputation. This should not happen.\")\n",
    "             return df_features.columns.tolist() # Fallback\n",
    "\n",
    "    # Check for infinities again after potential imputation edge cases\n",
    "    num_infs = np.isinf(features.values).sum()\n",
    "    if num_infs > 0:\n",
    "        print(f\"  ERROR: Found {num_infs} infinite values after imputation. Cannot calculate VIF. Aborting.\")\n",
    "        return df_features.columns.tolist()\n",
    "\n",
    "    # Check for constant columns after imputation\n",
    "    constant_cols = features.columns[features.apply(lambda x: x.nunique()) <= 1].tolist()\n",
    "    if constant_cols:\n",
    "        if verbose: print(f\"  Warning: Removing constant columns found after imputation before VIF: {constant_cols}\")\n",
    "        features = features.drop(columns=constant_cols)\n",
    "        if features.empty:\n",
    "            print(\"  ERROR: DataFrame empty after dropping constant columns post-imputation. VIF aborted.\")\n",
    "            return []\n",
    "\n",
    "    # Initial list of features to check\n",
    "    kept_features = features.columns.tolist()\n",
    "    dropped_features_vif = []\n",
    "\n",
    "    # Iterative VIF calculation\n",
    "    while len(kept_features) > 1:\n",
    "        # Add constant term for VIF calculation inside the loop with current features\n",
    "        X_vif = features[kept_features]\n",
    "        try:\n",
    "            # Calculate VIF\n",
    "            vif_series = pd.Series(\n",
    "                [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])],\n",
    "                index=kept_features,\n",
    "                dtype=float\n",
    "            )\n",
    "        except PerfectSeparationError:\n",
    "            print(\"  Error: Perfect separation detected during VIF calculation.\")\n",
    "            print(\"  Stopping VIF calculation. Manual inspection recommended.\")\n",
    "            break\n",
    "        except ValueError as ve:\n",
    "             print(f\"  ValueError during VIF calculation: {ve}.\")\n",
    "             break\n",
    "        except Exception as e:\n",
    "            print(f\"  Unexpected error calculating VIF: {e}\")\n",
    "            break\n",
    "\n",
    "        max_vif = vif_series.max()\n",
    "\n",
    "        # Check for NaN/Inf VIF values\n",
    "        is_nan_inf = vif_series.isna() | np.isinf(vif_series)\n",
    "        if is_nan_inf.any():\n",
    "             nan_inf_vif_features = vif_series[is_nan_inf].index.tolist()\n",
    "             if verbose: print(f\"  Warning: Found NaN/Inf VIF for: {nan_inf_vif_features}. Removing first one.\")\n",
    "             feature_to_drop_inf = nan_inf_vif_features[0]\n",
    "             if verbose: print(f\"  Dropping feature '{feature_to_drop_inf}' due to NaN/Inf VIF.\")\n",
    "             kept_features.remove(feature_to_drop_inf)\n",
    "             dropped_features_vif.append(feature_to_drop_inf)\n",
    "             continue # Recalculate\n",
    "\n",
    "        # Check threshold\n",
    "        if max_vif < vif_threshold:\n",
    "            if verbose: print(f\"  Max VIF ({max_vif:.2f}) is below threshold {vif_threshold}. Stopping.\")\n",
    "            break\n",
    "        else:\n",
    "            # Drop feature with highest VIF\n",
    "            feature_to_drop = vif_series.idxmax()\n",
    "            if verbose: print(f\"  Dropping '{feature_to_drop}' (VIF: {max_vif:.2f})\")\n",
    "            kept_features.remove(feature_to_drop)\n",
    "            dropped_features_vif.append(feature_to_drop)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Removed {len(dropped_features_vif)} features based on VIF.\")\n",
    "        if dropped_features_vif: print(f\"  Features Dropped by VIF: {sorted(list(dropped_features_vif))}\")\n",
    "        print(f\"Features remaining after VIF check: {len(kept_features)}\")\n",
    "        print(f\"--- VIF calculation finished. ---\")\n",
    "\n",
    "    # Return the list of features from the original input that survived\n",
    "    final_kept_feature_names = [col for col in df_features.columns if col in kept_features]\n",
    "    return final_kept_feature_names\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print(\"--- 1. Data Loading & Preprocessing ---\")\n",
    "    try:\n",
    "        print(f\"Loading data from: {CSV_FILE_PATH}\")\n",
    "        col_names = ['unix', 'date', 'symbol_csv', 'open', 'high', 'low', 'close', 'Volume BTC', 'Volume USD']\n",
    "        df_raw = pd.read_csv(CSV_FILE_PATH, header=0, names=col_names)\n",
    "        print(f\"Raw data loaded. Shape: {df_raw.shape}\")\n",
    "\n",
    "        # --- ENSURE timestamp IS CREATED CORRECTLY ---\n",
    "        if 'date' in df_raw.columns:\n",
    "             print(\"Using 'date' column for timestamp.\")\n",
    "             df_raw['timestamp'] = pd.to_datetime(df_raw['date'])\n",
    "             cols_to_drop_init = ['unix', 'date', 'symbol_csv']\n",
    "        elif 'unix' in df_raw.columns:\n",
    "             print(\"Using 'unix' column for timestamp (assuming milliseconds).\")\n",
    "             df_raw['timestamp'] = pd.to_datetime(df_raw['unix'], unit='ms')\n",
    "             cols_to_drop_init = ['unix', 'symbol_csv']\n",
    "        else:\n",
    "             print(\"Error: Neither 'date' nor 'unix' column found for timestamp generation.\")\n",
    "             exit()\n",
    "\n",
    "        df_raw = df_raw.drop(columns=cols_to_drop_init, errors='ignore')\n",
    "        df_raw = df_raw.sort_values('timestamp').reset_index(drop=True)\n",
    "        essential_raw = ['timestamp', 'open', 'high', 'low', 'close', 'Volume BTC', 'Volume USD']\n",
    "        essential_raw = [col for col in essential_raw if col in df_raw.columns]\n",
    "        df_raw = df_raw[essential_raw]\n",
    "\n",
    "        if df_raw.empty: exit(\"DataFrame empty after initial processing. Exiting.\")\n",
    "        print(f\"Data prepared for feature engineering. Shape: {df_raw.shape}\")\n",
    "\n",
    "    except Exception as e: print(f\"Error loading data: {e}\"); traceback.print_exc(); exit()\n",
    "\n",
    "\n",
    "    print(\"\\n--- 2. Feature Engineering (Script B Logic) ---\")\n",
    "    feature_calc_start = time.time()\n",
    "    df_full_features = calculate_script_b_features(df_raw) # Call your function\n",
    "    feature_calc_end = time.time()\n",
    "\n",
    "    if df_full_features.empty: exit(\"Feature calculation failed. Exiting.\")\n",
    "    print(f\"Feature calculation completed in {feature_calc_end - feature_calc_start:.2f} seconds.\")\n",
    "    print(f\"Total columns after feature engineering: {len(df_full_features.columns)}\")\n",
    "\n",
    "    # --- 3. Identify Potential Features ---\n",
    "    # Define NON_FEATURE_COLS based on Script B's potential output\n",
    "    NON_FEATURE_COLS_SCRIPT_B = [\n",
    "        'unix', 'date', 'symbol', 'open', 'high', 'low', 'close',\n",
    "        'Volume BTC', 'Volume USD', # Original names\n",
    "        'volume_btc', 'volume_usd', # Internal names if kept\n",
    "        'timestamp',\n",
    "        'price_change_1h', # Intermediate\n",
    "        'price_change_12h', # Target from Script B (might not be generated here)\n",
    "        # ATR intermediates (check if dropped in your function)\n",
    "        'prev_close', 'high_minus_low', 'high_minus_prev_close', 'low_minus_prev_close', 'true_range'\n",
    "    ]\n",
    "    all_potential_features = [col for col in df_full_features.columns if col not in NON_FEATURE_COLS_SCRIPT_B]\n",
    "    print(f\"Identified {len(all_potential_features)} potential features from Script B generation.\")\n",
    "\n",
    "    # Create DataFrame with only these potential features\n",
    "    df_features_only = df_full_features[all_potential_features].copy()\n",
    "\n",
    "    # --- ** NEW STEP 3.5: Handle Infinities BEFORE Filtering ** ---\n",
    "    print(\"\\n--- 3.5 Cleaning Infinities (BEFORE Filtering) ---\")\n",
    "    numeric_feature_cols = df_features_only.select_dtypes(include=np.number).columns\n",
    "    infinite_counts = np.isinf(df_features_only[numeric_feature_cols]).sum()\n",
    "    total_infinite = infinite_counts.sum()\n",
    "    if total_infinite > 0:\n",
    "        print(f\"Found {total_infinite} infinite values. Replacing with NaN BEFORE filtering.\")\n",
    "        # print(\"Infinite counts per column:\\n\", infinite_counts[infinite_counts > 0]) # Optional detail\n",
    "        df_features_only = df_features_only.replace([np.inf, -np.inf], np.nan)\n",
    "        print(\"Infinite values replaced with NaN.\")\n",
    "    else:\n",
    "        print(\"No infinite values found in features.\")\n",
    "    # --- END NEW STEP ---\n",
    "\n",
    "\n",
    "    # --- 4. Feature Selection: Variance Threshold ---\n",
    "    print(f\"\\n--- 4. Feature Selection: Variance Threshold (Threshold: {VARIANCE_THRESHOLD}) ---\")\n",
    "    features_before_variance = df_features_only.columns.tolist()\n",
    "    print(f\"Features before variance check: {len(features_before_variance)}\")\n",
    "\n",
    "    # Impute NaNs temporarily FOR FITTING ONLY\n",
    "    df_temp_imputed_var = df_features_only.copy()\n",
    "    num_nans_variance = df_temp_imputed_var.isnull().sum().sum()\n",
    "    if num_nans_variance > 0:\n",
    "        print(f\"  Imputing {num_nans_variance} NaNs with medians temporarily for VarianceThreshold fitting.\")\n",
    "        try:\n",
    "            # Impute only numeric columns\n",
    "            numeric_cols_var = df_temp_imputed_var.select_dtypes(include=np.number).columns\n",
    "            imputer_var = SimpleImputer(strategy='median')\n",
    "            df_temp_imputed_var[numeric_cols_var] = imputer_var.fit_transform(df_temp_imputed_var[numeric_cols_var])\n",
    "        except Exception as e_imp_var:\n",
    "             print(f\"  ERROR during NaN imputation for Variance Threshold: {e_imp_var}. Skipping VT.\")\n",
    "             features_after_variance = features_before_variance # Keep all if imputation fails\n",
    "             df_features_selected = df_features_only.copy() # Use original features\n",
    "             # Jump directly to VIF section\n",
    "             # Note: This means VIF will likely fail too if imputation failed here.\n",
    "             # Consider adding more robust error handling or stopping if imputation fails.\n",
    "\n",
    "    # Check for remaining NaNs after imputation (shouldn't happen with median if no all-NaN columns)\n",
    "    if df_temp_imputed_var.isnull().sum().sum() > 0:\n",
    "        print(\"  Warning: NaNs still present after median imputation for VT fit. Dropping all-NaN columns.\")\n",
    "        df_temp_imputed_var.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    # Check for constant columns AFTER imputation\n",
    "    constant_cols_var = df_temp_imputed_var.columns[df_temp_imputed_var.nunique() <= 1].tolist()\n",
    "    if constant_cols_var:\n",
    "        print(f\"  Removing constant columns found after imputation before VT fit: {constant_cols_var}\")\n",
    "        df_temp_imputed_var = df_temp_imputed_var.drop(columns=constant_cols_var)\n",
    "\n",
    "    # Apply Variance Threshold if imputation succeeded\n",
    "    if 'imputer_var' in locals(): # Check if imputation was attempted and likely succeeded\n",
    "        try:\n",
    "            selector = VarianceThreshold(threshold=VARIANCE_THRESHOLD)\n",
    "            # Fit only on columns remaining after NaN/constant checks\n",
    "            valid_cols_for_fit = df_temp_imputed_var.columns.tolist()\n",
    "            selector.fit(df_temp_imputed_var[valid_cols_for_fit])\n",
    "            # Apply mask back to the list of columns that were actually fit\n",
    "            features_after_variance = [col for col, support in zip(valid_cols_for_fit, selector.get_support()) if support]\n",
    "            # Determine dropped based on the set difference from before constant removal\n",
    "            dropped_variance = set(features_before_variance) - set(features_after_variance)\n",
    "            print(f\"Removed {len(dropped_variance)} features with variance <= {VARIANCE_THRESHOLD}.\")\n",
    "            if dropped_variance: print(f\"  Dropped by Variance Threshold: {sorted(list(dropped_variance))}\")\n",
    "        except Exception as e_var:\n",
    "            print(f\"Error during variance thresholding: {e_var}. Skipping VT.\")\n",
    "            features_after_variance = features_before_variance # Keep all if error\n",
    "    else: # Imputation failed earlier\n",
    "        features_after_variance = features_before_variance\n",
    "\n",
    "    print(f\"Features remaining after variance check: {len(features_after_variance)}\")\n",
    "    # Select columns in the original df_features_only that passed the variance check\n",
    "    df_features_selected = df_features_only[features_after_variance].copy()\n",
    "\n",
    "\n",
    "    # --- 5. Feature Selection: VIF Threshold ---\n",
    "    # Pass the DataFrame containing only features that passed the variance check\n",
    "    features_after_vif = calculate_vif(\n",
    "        df_features_selected, # Pass the dataframe with features selected so far\n",
    "        vif_threshold=VIF_THRESHOLD,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # --- 6. Final Selected Features ---\n",
    "    print(f\"\\n--- 6. Final Feature Selection Summary ---\")\n",
    "    print(f\"Total potential features from Script B: {len(all_potential_features)}\")\n",
    "    print(f\"Features after Variance Threshold ({VARIANCE_THRESHOLD}): {len(features_after_variance)}\")\n",
    "    print(f\"Features after VIF Threshold ({VIF_THRESHOLD}):      {len(features_after_vif)}\")\n",
    "    print(\"\\nFinal list of selected feature names (to use in modeling script):\")\n",
    "    if features_after_vif:\n",
    "        print(\"\\nSELECTED_FEATURE_NAMES = [\")\n",
    "        for i, feature in enumerate(sorted(features_after_vif)):\n",
    "             print(f\"    '{feature}',\" + (\"\" if i == len(features_after_vif) - 1 else \"\"))\n",
    "        print(\"]\")\n",
    "    else: print(\"  No features remained after filtering.\")\n",
    "    print(f\"\\nTotal selected features: {len(features_after_vif)}\")\n",
    "    print(\"\\nFeature selection script finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
