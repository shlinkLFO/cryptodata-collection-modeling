{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Ensemble_Method_C (Logistic Regression Meta-Learner) ---\n",
      "--- 1. Data Loading & Initial Prep ---\n",
      "Loading data from: C:\\Users\\mason\\AVP\\BTCUSDrec.csv\n",
      "Raw data loaded. Shape: (15177, 9)\n",
      "Initial data prep done. Shape: (15177, 7)\n",
      "\n",
      "--- 2. Feature Engineering (Simple_Predictor_B Features) ---\n",
      "Starting calculation for 49 target columns (incl. base)...\n",
      "  Calculating features...\n",
      "    Calculating ATR, Garman-Klass, and Parkinson volatility features...\n",
      "  Assembling final dataframe...\n",
      "Selected feature calculation finished. Returning 15177 rows, 51 columns (43 calculated features). Took 0.04s.\n",
      "Feature calculation completed in 0.04 seconds.\n",
      "Using 43 features for modeling.\n",
      "\n",
      "--- 3. Data Cleaning (Post-Features) ---\n",
      "Total NaNs found in 43 numeric feature columns: 1034.\n",
      "\n",
      "--- 4. Modeling Target & Final Prep ---\n",
      "Creating binary target: 12h future return >= 0.0%...\n",
      "Rows after removing NaN targets/close: 15165 (Removed 12)\n",
      "Note: 183 rows have NaNs in features. Models/Imputer will handle.\n",
      "\n",
      "Target variable distribution:\n",
      "  0 (< 0.0%): 47.72%\n",
      "  1 (>= 0.0%): 52.28%\n",
      "Final DataFrame shape for backtesting: (15165, 52)\n",
      "\n",
      "--- 5. Starting Walk-Forward Validation (Stacking - Logistic Regression Meta) ---\n",
      "Total rows: 15165, Train: 2016h (2016 rows), Eval: 504h (504 rows), Step: 24h (24 rows)\n",
      "Estimated iterations: 527\n",
      "Using 43 features.\n",
      "Stacking Folds (K): 5\n",
      "Meta Learner: Logistic Regression, Tuning C over: [0.01, 0.1, 1, 10, 100]\n",
      "Threshold Search Range: [0.1  0.15 0.2  0.25 0.3  0.35 0.4  0.45 0.5  0.55 0.6  0.65 0.7  0.75\n",
      " 0.8  0.85]\n",
      "------------------------------\n",
      "\n",
      "--- Iter 1/527 ---\n",
      "  Train Indices: [0:2015], Eval Indices: [2016:2519]\n",
      "  Train Target Dist: {0.0: 0.5178571428571429, 1.0: 0.48214285714285715}\n",
      "  Test Target Dist: {1.0: 0.6031746031746031, 0.0: 0.3968253968253968}\n",
      "  Using scale_pos_weight for XGB/LGBM: 1.0741\n",
      "  Grid searching base models...\n",
      "    Best XGB Params: {'max_depth': 4, 'n_estimators': 70, 'eta': 0.05, 'lambda': 1.5} (CV F1: 0.755)\n",
      "    Best LGBM Params: {'max_depth': 4, 'n_estimators': 95, 'learning_rate': 0.08, 'subsample': 0.75} (CV F1: 0.766)\n",
      "    Best SVM Params: {'C': 3.2, 'gamma': 'scale'} (CV F1: 0.703)\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0 OOF Generation Done. Meta Train Shape: (2016, 3)\n",
      "  Level 0: Training base models on full training data (2016 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (Logistic Regression) and Probability Threshold...\n",
      "    Tuning meta learner C over [0.01, 0.1, 1, 10, 100]...\n",
      "    Best Meta C Found: 100 (Validation F1: 0.8176)\n",
      "    Tuning threshold over range [0.1  0.15 0.2  0.25 0.3  0.35 0.4  0.45 0.5  0.55 0.6  0.65 0.7  0.75\n",
      " 0.8  0.85]...\n",
      "    Best Threshold Found: 0.60 (Validation F1: 0.8249)\n",
      "  Level 1: Training final Meta-Learner (Logistic Regression)...\n",
      "  Level 1 Final Meta Training Done.\n",
      "  Prediction: Generating final predictions on evaluation data...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 504h): Acc=0.3968, Prc=0.5000, Rec=0.2862, F1=0.3640\n",
      "  Iteration 1 finished in 8.32 seconds.\n",
      "--------------------\n",
      "\n",
      "--- Iter 2/527 ---\n",
      "  Train Indices: [24:2039], Eval Indices: [2040:2543]\n",
      "  Train Target Dist: {0.0: 0.5252976190476191, 1.0: 0.47470238095238093}\n",
      "  Test Target Dist: {1.0: 0.5972222222222222, 0.0: 0.4027777777777778}\n",
      "  Using scale_pos_weight for XGB/LGBM: 1.1066\n",
      "  Grid searching base models...\n",
      "    Best XGB Params: {'max_depth': 4, 'n_estimators': 70, 'eta': 0.05, 'lambda': 2.5} (CV F1: 0.752)\n",
      "    Best LGBM Params: {'max_depth': 4, 'n_estimators': 95, 'learning_rate': 0.08, 'subsample': 0.75} (CV F1: 0.761)\n",
      "    Best SVM Params: {'C': 3.2, 'gamma': 'scale'} (CV F1: 0.693)\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0 OOF Generation Done. Meta Train Shape: (2016, 3)\n",
      "  Level 0: Training base models on full training data (2016 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (Logistic Regression) and Probability Threshold...\n",
      "    Tuning meta learner C over [0.01, 0.1, 1, 10, 100]...\n",
      "    Best Meta C Found: 100 (Validation F1: 0.8056)\n",
      "    Tuning threshold over range [0.1  0.15 0.2  0.25 0.3  0.35 0.4  0.45 0.5  0.55 0.6  0.65 0.7  0.75\n",
      " 0.8  0.85]...\n",
      "    Best Threshold Found: 0.55 (Validation F1: 0.8094)\n",
      "  Level 1: Training final Meta-Learner (Logistic Regression)...\n",
      "  Level 1 Final Meta Training Done.\n",
      "  Prediction: Generating final predictions on evaluation data...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 504h): Acc=0.4266, Prc=0.5469, Rec=0.2326, F1=0.3263\n",
      "  Iteration 2 finished in 8.33 seconds.\n",
      "--------------------\n",
      "\n",
      "--- Iter 3/527 ---\n",
      "  Train Indices: [48:2063], Eval Indices: [2064:2567]\n",
      "  Train Target Dist: {0.0: 0.5163690476190477, 1.0: 0.4836309523809524}\n",
      "  Test Target Dist: {1.0: 0.5714285714285714, 0.0: 0.42857142857142855}\n",
      "  Using scale_pos_weight for XGB/LGBM: 1.0677\n",
      "  Grid searching base models...\n",
      "    Best XGB Params: {'max_depth': 4, 'n_estimators': 70, 'eta': 0.05, 'lambda': 1.5} (CV F1: 0.744)\n",
      "    Best LGBM Params: {'max_depth': 4, 'n_estimators': 95, 'learning_rate': 0.08, 'subsample': 0.75} (CV F1: 0.760)\n",
      "    Best SVM Params: {'C': 3.2, 'gamma': 'scale'} (CV F1: 0.672)\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0 OOF Generation Done. Meta Train Shape: (2016, 3)\n",
      "  Level 0: Training base models on full training data (2016 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (Logistic Regression) and Probability Threshold...\n",
      "    Tuning meta learner C over [0.01, 0.1, 1, 10, 100]...\n",
      "    Best Meta C Found: 1 (Validation F1: 0.8072)\n",
      "    Tuning threshold over range [0.1  0.15 0.2  0.25 0.3  0.35 0.4  0.45 0.5  0.55 0.6  0.65 0.7  0.75\n",
      " 0.8  0.85]...\n",
      "    Best Threshold Found: 0.60 (Validation F1: 0.8090)\n",
      "  Level 1: Training final Meta-Learner (Logistic Regression)...\n",
      "  Level 1 Final Meta Training Done.\n",
      "  Prediction: Generating final predictions on evaluation data...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 504h): Acc=0.4286, Prc=0.5000, Rec=0.2118, F1=0.2976\n",
      "  Iteration 3 finished in 8.32 seconds.\n",
      "--------------------\n",
      "\n",
      "--- Iter 4/527 ---\n",
      "  Train Indices: [72:2087], Eval Indices: [2088:2591]\n",
      "  Train Target Dist: {0.0: 0.5128968253968254, 1.0: 0.4871031746031746}\n",
      "  Test Target Dist: {1.0: 0.5813492063492064, 0.0: 0.41865079365079366}\n",
      "  Using scale_pos_weight for XGB/LGBM: 1.0530\n",
      "  Grid searching base models...\n",
      "    Best XGB Params: {'max_depth': 4, 'n_estimators': 70, 'eta': 0.05, 'lambda': 1.5} (CV F1: 0.751)\n",
      "    Best LGBM Params: {'max_depth': 4, 'n_estimators': 95, 'learning_rate': 0.08, 'subsample': 0.75} (CV F1: 0.764)\n",
      "    Best SVM Params: {'C': 3.2, 'gamma': 'scale'} (CV F1: 0.686)\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0 OOF Generation Done. Meta Train Shape: (2016, 3)\n",
      "  Level 0: Training base models on full training data (2016 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (Logistic Regression) and Probability Threshold...\n",
      "    Tuning meta learner C over [0.01, 0.1, 1, 10, 100]...\n",
      "    Best Meta C Found: 0.1 (Validation F1: 0.7994)\n",
      "    Tuning threshold over range [0.1  0.15 0.2  0.25 0.3  0.35 0.4  0.45 0.5  0.55 0.6  0.65 0.7  0.75\n",
      " 0.8  0.85]...\n",
      "    Best Threshold Found: 0.50 (Validation F1: 0.7994)\n",
      "  Level 1: Training final Meta-Learner (Logistic Regression)...\n",
      "  Level 1 Final Meta Training Done.\n",
      "  Prediction: Generating final predictions on evaluation data...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 504h): Acc=0.4246, Prc=0.5101, Rec=0.2594, F1=0.3439\n",
      "  Iteration 4 finished in 8.01 seconds.\n",
      "--------------------\n",
      "\n",
      "--- Iter 5/527 ---\n",
      "  Train Indices: [96:2111], Eval Indices: [2112:2615]\n",
      "  Train Target Dist: {0.0: 0.5124007936507936, 1.0: 0.48759920634920634}\n",
      "  Test Target Dist: {1.0: 0.5952380952380952, 0.0: 0.40476190476190477}\n",
      "  Using scale_pos_weight for XGB/LGBM: 1.0509\n",
      "  Grid searching base models...\n",
      "    Best XGB Params: {'max_depth': 4, 'n_estimators': 70, 'eta': 0.05, 'lambda': 1.5} (CV F1: 0.762)\n",
      "    Best LGBM Params: {'max_depth': 3, 'n_estimators': 95, 'learning_rate': 0.08, 'subsample': 0.75} (CV F1: 0.767)\n",
      "    Best SVM Params: {'C': 3.2, 'gamma': 'scale'} (CV F1: 0.693)\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0 OOF Generation Done. Meta Train Shape: (2016, 3)\n",
      "  Level 0: Training base models on full training data (2016 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (Logistic Regression) and Probability Threshold...\n",
      "    Tuning meta learner C over [0.01, 0.1, 1, 10, 100]...\n",
      "    Best Meta C Found: 0.1 (Validation F1: 0.7599)\n",
      "    Tuning threshold over range [0.1  0.15 0.2  0.25 0.3  0.35 0.4  0.45 0.5  0.55 0.6  0.65 0.7  0.75\n",
      " 0.8  0.85]...\n",
      "    Best Threshold Found: 0.35 (Validation F1: 0.7655)\n",
      "  Level 1: Training final Meta-Learner (Logistic Regression)...\n",
      "  Level 1 Final Meta Training Done.\n",
      "  Prediction: Generating final predictions on evaluation data...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 504h): Acc=0.3849, Prc=0.4706, Rec=0.2667, F1=0.3404\n",
      "  Iteration 5 finished in 8.12 seconds.\n",
      "--------------------\n",
      "\n",
      "--- Iter 6/527 ---\n",
      "  Train Indices: [120:2135], Eval Indices: [2136:2639]\n",
      "  Train Target Dist: {0.0: 0.5094246031746031, 1.0: 0.4905753968253968}\n",
      "  Test Target Dist: {1.0: 0.5992063492063492, 0.0: 0.4007936507936508}\n",
      "  Using scale_pos_weight for XGB/LGBM: 1.0384\n",
      "  Grid searching base models...\n",
      "    Best XGB Params: {'max_depth': 4, 'n_estimators': 70, 'eta': 0.05, 'lambda': 2.5} (CV F1: 0.753)\n",
      "    Best LGBM Params: {'max_depth': 4, 'n_estimators': 95, 'learning_rate': 0.08, 'subsample': 0.75} (CV F1: 0.758)\n",
      "    Best SVM Params: {'C': 3.2, 'gamma': 'scale'} (CV F1: 0.692)\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0 OOF Generation Done. Meta Train Shape: (2016, 3)\n",
      "  Level 0: Training base models on full training data (2016 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (Logistic Regression) and Probability Threshold...\n",
      "    Tuning meta learner C over [0.01, 0.1, 1, 10, 100]...\n",
      "    Best Meta C Found: 100 (Validation F1: 0.7730)\n",
      "    Tuning threshold over range [0.1  0.15 0.2  0.25 0.3  0.35 0.4  0.45 0.5  0.55 0.6  0.65 0.7  0.75\n",
      " 0.8  0.85]...\n",
      "    Best Threshold Found: 0.65 (Validation F1: 0.7808)\n",
      "  Level 1: Training final Meta-Learner (Logistic Regression)...\n",
      "  Level 1 Final Meta Training Done.\n",
      "  Prediction: Generating final predictions on evaluation data...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 504h): Acc=0.3988, Prc=0.4940, Rec=0.1358, F1=0.2130\n",
      "  Iteration 6 finished in 7.96 seconds.\n",
      "--------------------\n",
      "\n",
      "--- Iter 7/527 ---\n",
      "  Train Indices: [144:2159], Eval Indices: [2160:2663]\n",
      "  Train Target Dist: {0.0: 0.5089285714285714, 1.0: 0.49107142857142855}\n",
      "  Test Target Dist: {1.0: 0.6091269841269841, 0.0: 0.39087301587301587}\n",
      "  Using scale_pos_weight for XGB/LGBM: 1.0364\n",
      "  Grid searching base models...\n",
      "    Best XGB Params: {'max_depth': 4, 'n_estimators': 70, 'eta': 0.05, 'lambda': 1.5} (CV F1: 0.755)\n",
      "    Best LGBM Params: {'max_depth': 4, 'n_estimators': 95, 'learning_rate': 0.08, 'subsample': 0.75} (CV F1: 0.754)\n",
      "    Best SVM Params: {'C': 3.2, 'gamma': 'scale'} (CV F1: 0.697)\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0 OOF Generation Done. Meta Train Shape: (2016, 3)\n",
      "  Level 0: Training base models on full training data (2016 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (Logistic Regression) and Probability Threshold...\n",
      "    Tuning meta learner C over [0.01, 0.1, 1, 10, 100]...\n",
      "    Best Meta C Found: 0.01 (Validation F1: 0.7689)\n",
      "    Tuning threshold over range [0.1  0.15 0.2  0.25 0.3  0.35 0.4  0.45 0.5  0.55 0.6  0.65 0.7  0.75\n",
      " 0.8  0.85]...\n",
      "    Best Threshold Found: 0.45 (Validation F1: 0.7730)\n",
      "  Level 1: Training final Meta-Learner (Logistic Regression)...\n",
      "  Level 1 Final Meta Training Done.\n",
      "  Prediction: Generating final predictions on evaluation data...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 504h): Acc=0.4028, Prc=0.5312, Rec=0.1661, F1=0.2531\n",
      "  Iteration 7 finished in 8.07 seconds.\n",
      "--------------------\n",
      "\n",
      "--- Iter 8/527 ---\n",
      "  Train Indices: [168:2183], Eval Indices: [2184:2687]\n",
      "  Train Target Dist: {0.0: 0.5158730158730159, 1.0: 0.48412698412698413}\n",
      "  Test Target Dist: {1.0: 0.6369047619047619, 0.0: 0.3630952380952381}\n",
      "  Using scale_pos_weight for XGB/LGBM: 1.0656\n",
      "  Grid searching base models...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 466\u001b[0m\n\u001b[0;32m    464\u001b[0m best_xgb_params, xgb_score \u001b[38;5;241m=\u001b[39m grid_search_base_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxgb\u001b[39m\u001b[38;5;124m'\u001b[39m, BASE_XGB_PARAM_GRID, X_train_full, y_train_full, scale_pos_weight_val)\n\u001b[0;32m    465\u001b[0m best_lgbm_params, lgbm_score \u001b[38;5;241m=\u001b[39m grid_search_base_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlgbm\u001b[39m\u001b[38;5;124m'\u001b[39m, BASE_LGBM_PARAM_GRID, X_train_full, y_train_full, scale_pos_weight_val)\n\u001b[1;32m--> 466\u001b[0m best_svm_params, svm_score \u001b[38;5;241m=\u001b[39m \u001b[43mgrid_search_base_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msvm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBASE_SVM_PARAM_GRID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_full\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_full\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_pos_weight_val\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# scale_pos not used directly for SVM pipeline\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    Best XGB Params: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_xgb_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (CV F1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mxgb_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    Best LGBM Params: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_lgbm_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (CV F1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlgbm_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 324\u001b[0m, in \u001b[0;36mgrid_search_base_model\u001b[1;34m(model_type, base_param_grid, X, y, scale_pos_weight_val)\u001b[0m\n\u001b[0;32m    317\u001b[0m      model \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[0;32m    318\u001b[0m         (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimputer\u001b[39m\u001b[38;5;124m'\u001b[39m, SimpleImputer(strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmedian\u001b[39m\u001b[38;5;124m'\u001b[39m)),\n\u001b[0;32m    319\u001b[0m         (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaler\u001b[39m\u001b[38;5;124m'\u001b[39m, StandardScaler()),\n\u001b[0;32m    320\u001b[0m         (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvm\u001b[39m\u001b[38;5;124m'\u001b[39m, SVC(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfixed_params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msvm_grid_params)) \u001b[38;5;66;03m# Pass fixed and grid params here\u001b[39;00m\n\u001b[0;32m    321\u001b[0m      ])\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 324\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_inner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_inner\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m y_pred_inner \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_val_inner)\n\u001b[0;32m    326\u001b[0m score \u001b[38;5;241m=\u001b[39m f1_score(y_val_inner, y_pred_inner, zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\pipeline.py:427\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    425\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    426\u001b[0m         fit_params_last_step \u001b[38;5;241m=\u001b[39m fit_params_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m--> 427\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_last_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\svm\\_base.py:250\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LibSVM]\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    249\u001b[0m seed \u001b[38;5;241m=\u001b[39m rnd\u001b[38;5;241m.\u001b[39mrandint(np\u001b[38;5;241m.\u001b[39miinfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmax)\n\u001b[1;32m--> 250\u001b[0m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# see comment on the other call to np.iinfo in this file\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape_fit_ \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m (n_samples,)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\svm\\_base.py:329\u001b[0m, in \u001b[0;36mBaseLibSVM._dense_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    315\u001b[0m libsvm\u001b[38;5;241m.\u001b[39mset_verbosity_wrap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[0;32m    317\u001b[0m \u001b[38;5;66;03m# we don't pass **self.get_params() to allow subclasses to\u001b[39;00m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;66;03m# add other parameters to __init__\u001b[39;00m\n\u001b[0;32m    319\u001b[0m (\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_,\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_vectors_,\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_support,\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdual_coef_,\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_,\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probA,\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probB,\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_status_,\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_iter,\n\u001b[1;32m--> 329\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mlibsvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43msvm_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# TODO(1.4): Replace \"_class_weight\" with \"class_weight_\"\u001b[39;49;00m\n\u001b[0;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_class_weight\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mC\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprobability\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdegree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdegree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshrinking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshrinking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoef0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoef0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_from_fit_status()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Ensemble_Method_C.py\n",
    "# Combined Script: Load CSV -> Feature Engineering -> Rolling Origin Stacking\n",
    "# Uses feature set from Simple_Predictor_B\n",
    "# Meta-Learner: Regularized Logistic Regression\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "\n",
    "# Feature Engineering Imports\n",
    "# import pandas_ta as ta # No longer needed if using custom functions below\n",
    "\n",
    "# Modeling Imports\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression # <-- Added Meta Learner\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import ParameterGrid, StratifiedKFold\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# --- Suppress Warnings ---\n",
    "warnings.filterwarnings('ignore', category=UndefinedMetricWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "# Data Loading\n",
    "CSV_FILE_PATH = r'C:\\Users\\mason\\AVP\\BTCUSDrec.csv'\n",
    "SYMBOL_NAME = 'BTCUSD'\n",
    "\n",
    "# Feature Selection (From Simple_Predictor_B)\n",
    "SELECTED_FEATURE_NAMES = [\n",
    "    'open', 'high', 'low', 'close', 'Volume BTC', 'Volume USD',\n",
    "    'price_range_pct', 'oc_change_pct', 'garman_klass_12h', 'parkinson_3h',\n",
    "    'ma_3h', 'rolling_std_3h', 'lag_3h_price_return', 'lag_6h_price_return',\n",
    "    'lag_12h_price_return', 'lag_24h_price_return', 'lag_48h_price_return',\n",
    "    'lag_72h_price_return', 'lag_168h_price_return', 'volume_return_1h',\n",
    "    'lag_3h_volume_return', 'lag_6h_volume_return', 'lag_12h_volume_return',\n",
    "    'lag_24h_volume_return', 'ma_6h', 'ma_12h', 'ma_24h', 'ma_48h',\n",
    "    'ma_72h', 'ma_168h', 'rolling_std_6h', 'rolling_std_12h',\n",
    "    'rolling_std_24h', 'rolling_std_48h', 'rolling_std_72h',\n",
    "    'rolling_std_168h', 'atr_14h', 'atr_24h', 'atr_48h', 'close_div_ma_24h',\n",
    "    'close_div_ma_48h', 'close_div_ma_168h', 'ma12_div_ma48',\n",
    "    'ma24_div_ma168', 'std12_div_std72', 'volume_btc_x_range',\n",
    "    'rolling_std_3h_sq', 'price_return_1h_sq', 'rolling_std_12h_sqrt'\n",
    "]\n",
    "MODEL_FEATURE_COLS = [f for f in SELECTED_FEATURE_NAMES if f not in ['open', 'high', 'low', 'close', 'Volume BTC', 'Volume USD']]\n",
    "\n",
    "# Modeling & Walk-Forward\n",
    "TARGET_THRESHOLD_PCT = 0.0\n",
    "PREDICTION_WINDOW_HOURS = 12\n",
    "PREDICTION_WINDOW_ROWS = PREDICTION_WINDOW_HOURS\n",
    "\n",
    "# Walk-forward params (Adjusted slightly from B for variation)\n",
    "TRAIN_WINDOW_HOURS = int(24 * 7 * 12) # ~12 weeks\n",
    "TEST_WINDOW_HOURS = 24 * 21          # Evaluate over 3 weeks\n",
    "STEP_HOURS = 24                      # Retrain daily\n",
    "\n",
    "TRAIN_WINDOW_ROWS = TRAIN_WINDOW_HOURS\n",
    "TEST_WINDOW_ROWS = TEST_WINDOW_HOURS\n",
    "STEP_ROWS = STEP_HOURS\n",
    "\n",
    "# Stacking Configuration\n",
    "N_STACKING_FOLDS = 5\n",
    "\n",
    "# --- Base Model Parameter Grids (For per-iteration tuning) ---\n",
    "BASE_XGB_PARAM_GRID = {\n",
    "    'max_depth': [2, 4],\n",
    "    'n_estimators': [30, 70],\n",
    "    'eta': [0.03, 0.05],\n",
    "    'lambda': [1.5, 2.5]\n",
    "}\n",
    "BASE_LGBM_PARAM_GRID = {\n",
    "    'max_depth': [3, 4],\n",
    "    'n_estimators': [55, 95],\n",
    "    'learning_rate': [0.04, 0.08],\n",
    "    'subsample': [0.75, 0.9]\n",
    "}\n",
    "BASE_SVM_PARAM_GRID = {\n",
    "    'C': [1.6, 3.2],\n",
    "    'gamma': ['scale', 'auto'] # Keep gamma tuning\n",
    "}\n",
    "\n",
    "# --- Base Model Static/Fixed Hyperparameters ---\n",
    "XGB_BASE_PARAMS = { # Defaults not covered by grid\n",
    "    'objective': 'binary:logistic', 'eval_metric': 'logloss',\n",
    "    'subsample': 0.8, 'colsample_bytree': 0.7, 'min_child_weight': 3,\n",
    "    'gamma': 0.1, 'alpha': 0.1, 'random_state': 42, 'n_jobs': -1,\n",
    "    'tree_method': 'hist', 'use_label_encoder': False,\n",
    "}\n",
    "LGBM_BASE_PARAMS = { # Defaults not covered by grid\n",
    "    'objective': 'binary', 'metric': 'logloss', 'num_leaves': 8,\n",
    "    'colsample_bytree': 0.7, 'min_child_samples': 5, 'reg_alpha': 0.1,\n",
    "    'reg_lambda': 1.5, 'random_state': 42, 'n_jobs': -1,\n",
    "    'boosting_type': 'gbdt', 'verbose': -1\n",
    "}\n",
    "SVM_BASE_PARAMS = { # Fixed SVM params\n",
    "    'kernel': 'rbf', 'probability': True, 'max_iter': 5000,\n",
    "    'random_state': 42, 'class_weight': 'balanced'\n",
    "}\n",
    "\n",
    "# --- Meta Learner Configuration (Logistic Regression) ---\n",
    "META_LR_PARAM_GRID = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100] # Regularization strength (inverse)\n",
    "}\n",
    "META_LR_FIXED_PARAMS = {\n",
    "    'penalty': 'l2', # Ridge regularization (can try 'l1' with 'liblinear')\n",
    "    'solver': 'liblinear', # Good choice for L1/L2 and smaller datasets\n",
    "    'class_weight': 'balanced', # Important for potentially imbalanced meta-features\n",
    "    'max_iter': 1000, # Increase iterations for convergence\n",
    "    'random_state': 123,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "# --- Probability Threshold Tuning Configuration ---\n",
    "THRESHOLD_SEARCH_RANGE = np.arange(0.10, 0.90, 0.05)\n",
    "META_VALIDATION_PCT = 0.25\n",
    "\n",
    "# --- Feature Engineering Functions (Copied from request) ---\n",
    "def garman_klass_volatility(open_, high, low, close, window):\n",
    "    log_hl = np.log(high / low)\n",
    "    log_co = np.log(close / open_)\n",
    "    gk = 0.5 * (log_hl ** 2) - (2 * np.log(2) - 1) * (log_co ** 2)\n",
    "    # Need to handle potential NaNs before rolling mean if inputs cause them\n",
    "    gk = gk.fillna(0) # Or use rolling(..., min_periods=1) below? Let's fill first.\n",
    "    rolling_mean = gk.rolling(window=window, min_periods=max(1, window // 2)).mean() # Allow fewer periods\n",
    "    rolling_mean = rolling_mean.clip(lower=0) # Clip negative variances before sqrt\n",
    "    return np.sqrt(rolling_mean)\n",
    "\n",
    "def parkinson_volatility(high, low, window):\n",
    "    log_hl_sq = np.log(high / low) ** 2\n",
    "    # Handle potential NaNs\n",
    "    log_hl_sq = log_hl_sq.fillna(0)\n",
    "    rolling_sum = log_hl_sq.rolling(window=window, min_periods=max(1, window // 2)).sum()\n",
    "    factor = 1 / (4 * np.log(2) * window)\n",
    "    return np.sqrt(factor * rolling_sum)\n",
    "\n",
    "def calculate_selected_features(df, symbol):\n",
    "    \"\"\"\n",
    "    Calculates the features required by Simple_Predictor_B using custom methods.\n",
    "    \"\"\"\n",
    "    print(f\"Starting calculation for {len(SELECTED_FEATURE_NAMES)} target columns (incl. base)...\")\n",
    "    start_time = time.time()\n",
    "    if df is None or len(df) < 3: return pd.DataFrame()\n",
    "    df = df.copy()\n",
    "    df['symbol'] = symbol\n",
    "\n",
    "    # --- Timestamp and Index ---\n",
    "    if 'timestamp' not in df.columns: print(\"Error: 'timestamp' column not found.\"); return pd.DataFrame()\n",
    "    try: df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    except Exception as e: print(f\"Error converting timestamp: {e}\"); return pd.DataFrame()\n",
    "    df = df.sort_values('timestamp').dropna(subset=['timestamp'])\n",
    "    df = df.set_index('timestamp', drop=False)\n",
    "\n",
    "    # --- Volume Columns ---\n",
    "    original_vol_btc_name = 'Volume BTC'; original_vol_usd_name = 'Volume USD'\n",
    "    if original_vol_btc_name not in df.columns: df[original_vol_btc_name] = 0\n",
    "    if original_vol_usd_name not in df.columns: df[original_vol_usd_name] = 0\n",
    "    df[original_vol_btc_name] = pd.to_numeric(df[original_vol_btc_name], errors='coerce').fillna(0)\n",
    "    df[original_vol_usd_name] = pd.to_numeric(df[original_vol_usd_name], errors='coerce').fillna(0)\n",
    "\n",
    "    # --- Basic Checks (OHLC) ---\n",
    "    required_ohlc = ['open', 'high', 'low', 'close']\n",
    "    if not all(col in df.columns for col in required_ohlc): print(f\"Error: Missing required OHLC columns: {required_ohlc}\"); return pd.DataFrame()\n",
    "    for col in required_ohlc: df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    if df[required_ohlc].isnull().any().any(): print(\"Warning: NaNs found in OHLC data. Dropping affected rows.\"); df = df.dropna(subset=required_ohlc)\n",
    "    if df.empty: print(\"DataFrame empty after OHLC checks.\"); return pd.DataFrame()\n",
    "\n",
    "    print(\"  Calculating features...\")\n",
    "    min_periods_rolling = 2 # Default min periods for rolling std/ma unless specified\n",
    "\n",
    "    # --- Basic Calcs ---\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        df['price_range_pct'] = (df['high'] - df['low']) / df['close'].replace(0, np.nan) # Use close as denominator? Okay.\n",
    "        df['oc_change_pct'] = (df['close'] - df['open']) / df['open'].replace(0, np.nan)\n",
    "        df['price_return_1h_temp'] = df['close'].pct_change() # Temporary for std calc and sq feature\n",
    "        df['volume_return_1h'] = df[original_vol_btc_name].pct_change()\n",
    "\n",
    "    # --- Lagged Returns ---\n",
    "    lag_price_hours = [3, 6, 12, 24, 48, 72, 168]\n",
    "    lag_volume_hours = [3, 6, 12, 24]\n",
    "    for hours in lag_price_hours: df[f'lag_{hours}h_price_return'] = df['close'].pct_change(periods=hours)\n",
    "    for hours in lag_volume_hours: df[f'lag_{hours}h_volume_return'] = df[original_vol_btc_name].pct_change(periods=hours)\n",
    "\n",
    "    # --- Moving Averages ---\n",
    "    ma_hours = [3, 6, 12, 24, 48, 72, 168]\n",
    "    for hours in ma_hours:\n",
    "        df[f'ma_{hours}h'] = df['close'].rolling(window=hours, min_periods=max(min_periods_rolling, hours // 2)).mean()\n",
    "\n",
    "    # --- Rolling Standard Deviations (Using price_return_1h_temp) ---\n",
    "    std_hours = [3, 6, 12, 24, 48, 72, 168]\n",
    "    if 'price_return_1h_temp' in df.columns:\n",
    "        for hours in std_hours:\n",
    "            # Multiply by 100 as per original code? Let's assume yes.\n",
    "            df[f'rolling_std_{hours}h'] = df['price_return_1h_temp'].rolling(window=hours, min_periods=max(min_periods_rolling, hours // 2)).std() * 100\n",
    "    else:\n",
    "        for hours in std_hours: df[f'rolling_std_{hours}h'] = np.nan\n",
    "\n",
    "    # --- ATR Calculation (Custom) ---\n",
    "    print(\"    Calculating ATR, Garman-Klass, and Parkinson volatility features...\")\n",
    "    df['prev_close'] = df['close'].shift(1)\n",
    "    df['high_minus_low'] = df['high'] - df['low']\n",
    "    df['high_minus_prev_close'] = np.abs(df['high'] - df['prev_close'])\n",
    "    df['low_minus_prev_close'] = np.abs(df['low'] - df['prev_close'])\n",
    "    df['true_range'] = df[['high_minus_low', 'high_minus_prev_close', 'low_minus_prev_close']].max(axis=1)\n",
    "    for p in [14, 24, 48]:\n",
    "         df[f'atr_{p}h'] = df['true_range'].rolling(window=p, min_periods=max(1, p // 2)).mean() # Allow fewer periods\n",
    "    df = df.drop(columns=['prev_close', 'high_minus_low', 'high_minus_prev_close', 'low_minus_prev_close', 'true_range'])\n",
    "\n",
    "    # --- Garman-Klass and Parkinson Volatility (Custom) ---\n",
    "    df['garman_klass_12h'] = garman_klass_volatility(df['open'], df['high'], df['low'], df['close'], window=12)\n",
    "    df['parkinson_3h'] = parkinson_volatility(df['high'], df['low'], window=3)\n",
    "\n",
    "    # --- Ratio Features ---\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        for hours in [24, 48, 168]:\n",
    "            ma_col = f'ma_{hours}h'\n",
    "            if ma_col in df.columns: df[f'close_div_ma_{hours}h'] = df['close'] / df[ma_col].replace(0, np.nan)\n",
    "            else: df[f'close_div_ma_{hours}h'] = np.nan\n",
    "        if 'ma_12h' in df.columns and 'ma_48h' in df.columns: df['ma12_div_ma48'] = df['ma_12h'] / df['ma_48h'].replace(0, np.nan)\n",
    "        else: df['ma12_div_ma48'] = np.nan\n",
    "        if 'ma_24h' in df.columns and 'ma_168h' in df.columns: df['ma24_div_ma168'] = df['ma_24h'] / df['ma_168h'].replace(0, np.nan)\n",
    "        else: df['ma24_div_ma168'] = np.nan\n",
    "        if 'rolling_std_12h' in df.columns and 'rolling_std_72h' in df.columns: df['std12_div_std72'] = df['rolling_std_12h'] / df['rolling_std_72h'].replace(0, np.nan)\n",
    "        else: df['std12_div_std72'] = np.nan\n",
    "        # Interaction\n",
    "        if original_vol_btc_name in df.columns and 'price_range_pct' in df.columns: df['volume_btc_x_range'] = df[original_vol_btc_name] * df['price_range_pct']\n",
    "        else: df['volume_btc_x_range'] = np.nan\n",
    "\n",
    "    # --- Non-linear Transformations ---\n",
    "    if 'rolling_std_3h' in df.columns: df['rolling_std_3h_sq'] = df['rolling_std_3h'] ** 2\n",
    "    else: df['rolling_std_3h_sq'] = np.nan\n",
    "    if 'price_return_1h_temp' in df.columns: df['price_return_1h_sq'] = (df['price_return_1h_temp'] ** 2) * 10000 # Scaling factor from original?\n",
    "    else: df['price_return_1h_sq'] = np.nan\n",
    "    if 'rolling_std_12h' in df.columns:\n",
    "        epsilon = 1e-9 # Add epsilon before sqrt for stability\n",
    "        df['rolling_std_12h_sqrt'] = np.sqrt(df['rolling_std_12h'].clip(lower=0) + epsilon)\n",
    "    else: df['rolling_std_12h_sqrt'] = np.nan\n",
    "\n",
    "    # --- Cleanup ---\n",
    "    if 'price_return_1h_temp' in df.columns: df = df.drop(columns=['price_return_1h_temp'])\n",
    "\n",
    "    print(\"  Assembling final dataframe...\")\n",
    "    final_cols_present = [col for col in SELECTED_FEATURE_NAMES if col in df.columns]\n",
    "    df_final = df[final_cols_present + ['timestamp', 'symbol']].copy()\n",
    "    missing_final_cols = set(SELECTED_FEATURE_NAMES) - set(df_final.columns)\n",
    "    if missing_final_cols: print(f\"  Final Warning: {len(missing_final_cols)} target columns missing: {missing_final_cols}\")\n",
    "    df_final = df_final.reset_index(drop=True)\n",
    "    df_final = df_final.replace([np.inf, -np.inf], np.nan)\n",
    "    end_time = time.time()\n",
    "    actual_feature_count = len([col for col in df_final.columns if col not in ['timestamp', 'symbol', 'open', 'high', 'low', 'close', 'Volume BTC', 'Volume USD']])\n",
    "    print(f\"Selected feature calculation finished. Returning {len(df_final)} rows, {len(df_final.columns)} columns \"\n",
    "          f\"({actual_feature_count} calculated features). Took {end_time - start_time:.2f}s.\")\n",
    "    return df_final\n",
    "\n",
    "# --- Helper: Grid Search for Base Models (Copied from request) ---\n",
    "def grid_search_base_model(model_type, base_param_grid, X, y, scale_pos_weight_val):\n",
    "    \"\"\"Performs a simple 3-fold grid search using F1 score as metric.\"\"\"\n",
    "    best_score = -np.inf\n",
    "    best_params = None\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Combine fixed and grid params for iteration\n",
    "    fixed_params = {}\n",
    "    if model_type == 'xgb': fixed_params = XGB_BASE_PARAMS\n",
    "    elif model_type == 'lgbm': fixed_params = LGBM_BASE_PARAMS\n",
    "    elif model_type == 'svm': fixed_params = SVM_BASE_PARAMS # SVM params handled slightly differently in pipeline\n",
    "\n",
    "    full_param_list = []\n",
    "    for grid_p in ParameterGrid(base_param_grid):\n",
    "        p = fixed_params.copy()\n",
    "        p.update(grid_p)\n",
    "        full_param_list.append(p)\n",
    "        \n",
    "    if not full_param_list: # Handle case where grid is empty\n",
    "        full_param_list.append(fixed_params)\n",
    "\n",
    "    for params in full_param_list:\n",
    "        scores = []\n",
    "        # Check if y has at least 2 classes before splitting\n",
    "        if len(np.unique(y)) < 2:\n",
    "            print(f\"    Skipping CV for {model_type}: Target has only one class.\")\n",
    "            return params, 0.0 # Return default params, score 0\n",
    "            \n",
    "        for train_idx, val_idx in cv.split(X, y):\n",
    "            # Ensure validation split also has both classes\n",
    "            y_val_inner = y.iloc[val_idx]\n",
    "            if len(np.unique(y_val_inner)) < 2:\n",
    "                scores.append(0) # Assign 0 if validation fold is single class\n",
    "                continue\n",
    "                \n",
    "            X_train_inner, y_train_inner = X.iloc[train_idx], y.iloc[train_idx]\n",
    "            X_val_inner = X.iloc[val_idx] # y_val_inner already defined\n",
    "\n",
    "            try:\n",
    "                if model_type == 'xgb':\n",
    "                    # Remove SVM specific params if they sneaked in\n",
    "                    params_xgb = {k: v for k, v in params.items() if k not in ['C', 'gamma', 'kernel', 'probability', 'max_iter', 'class_weight']}\n",
    "                    model = XGBClassifier(**params_xgb, scale_pos_weight=scale_pos_weight_val)\n",
    "                elif model_type == 'lgbm':\n",
    "                     params_lgbm = {k: v for k, v in params.items() if k not in ['C', 'gamma', 'kernel', 'probability', 'max_iter', 'class_weight']}\n",
    "                     model = LGBMClassifier(**params_lgbm, scale_pos_weight=scale_pos_weight_val)\n",
    "                elif model_type == 'svm':\n",
    "                     # Extract SVM specific params from the combined dict\n",
    "                     svm_grid_params = {k: params[k] for k in base_param_grid if k in params}\n",
    "                     model = Pipeline([\n",
    "                        ('imputer', SimpleImputer(strategy='median')),\n",
    "                        ('scaler', StandardScaler()),\n",
    "                        ('svm', SVC(**fixed_params, **svm_grid_params)) # Pass fixed and grid params here\n",
    "                     ])\n",
    "                else: continue\n",
    "\n",
    "                model.fit(X_train_inner, y_train_inner)\n",
    "                y_pred_inner = model.predict(X_val_inner)\n",
    "                score = f1_score(y_val_inner, y_pred_inner, zero_division=0)\n",
    "                scores.append(score)\n",
    "            except Exception as e:\n",
    "                # print(f\"      Error during {model_type} CV fit/predict with params {params}: {e}\") # Optional debug\n",
    "                scores.append(0) # Assign 0 score on error\n",
    "\n",
    "        mean_score = np.mean(scores) if scores else 0\n",
    "        if mean_score > best_score:\n",
    "            best_score = mean_score\n",
    "            # Store only the grid parameters that led to the best score\n",
    "            best_params = {k: params[k] for k in base_param_grid if k in params}\n",
    "\n",
    "    # Handle case where no parameters were found (e.g., all folds failed)\n",
    "    if best_params is None and base_param_grid:\n",
    "       best_params = list(ParameterGrid(base_param_grid))[0] # Return first grid combo as fallback\n",
    "\n",
    "    return best_params, best_score\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"--- Ensemble_Method_C (Logistic Regression Meta-Learner) ---\")\n",
    "    print(\"--- 1. Data Loading & Initial Prep ---\")\n",
    "    try:\n",
    "        print(f\"Loading data from: {CSV_FILE_PATH}\")\n",
    "        col_names = ['unix', 'date', 'symbol_csv', 'open', 'high', 'low', 'close', 'Volume BTC', 'Volume USD']\n",
    "        df_raw = pd.read_csv(CSV_FILE_PATH, header=0, names=col_names)\n",
    "        print(f\"Raw data loaded. Shape: {df_raw.shape}\")\n",
    "        df_raw['timestamp'] = pd.to_datetime(df_raw['date'])\n",
    "        df_raw = df_raw.drop(['unix', 'date', 'symbol_csv'], axis=1)\n",
    "        df_raw = df_raw.sort_values('timestamp').reset_index(drop=True)\n",
    "        if df_raw.empty: exit(\"DataFrame empty after loading. Exiting.\")\n",
    "        print(f\"Initial data prep done. Shape: {df_raw.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or processing CSV: {e}\"); traceback.print_exc(); exit()\n",
    "\n",
    "    print(\"\\n--- 2. Feature Engineering (Simple_Predictor_B Features) ---\")\n",
    "    feature_calc_start = time.time()\n",
    "    df_features = calculate_selected_features(df_raw, symbol=SYMBOL_NAME)\n",
    "    feature_calc_end = time.time()\n",
    "    if df_features.empty: exit(\"Feature calculation failed. Exiting.\")\n",
    "    print(f\"Feature calculation completed in {feature_calc_end - feature_calc_start:.2f} seconds.\")\n",
    "    CURRENT_FEATURE_COLS = [f for f in MODEL_FEATURE_COLS if f in df_features.columns]\n",
    "    if not CURRENT_FEATURE_COLS: exit(\"ERROR: No modeling features found after calculation.\")\n",
    "    if len(CURRENT_FEATURE_COLS) < len(MODEL_FEATURE_COLS): print(f\"Warning: Only {len(CURRENT_FEATURE_COLS)}/{len(MODEL_FEATURE_COLS)} modeling features generated.\")\n",
    "    print(f\"Using {len(CURRENT_FEATURE_COLS)} features for modeling.\")\n",
    "\n",
    "    print(\"\\n--- 3. Data Cleaning (Post-Features) ---\")\n",
    "    numeric_feature_cols = df_features[CURRENT_FEATURE_COLS].select_dtypes(include=np.number).columns.tolist()\n",
    "    df_features[numeric_feature_cols] = df_features[numeric_feature_cols].replace([np.inf, -np.inf], np.nan)\n",
    "    nan_check = df_features[numeric_feature_cols].isnull().sum()\n",
    "    total_nans = nan_check.sum()\n",
    "    print(f\"Total NaNs found in {len(numeric_feature_cols)} numeric feature columns: {total_nans}.\")\n",
    "\n",
    "    print(\"\\n--- 4. Modeling Target & Final Prep ---\")\n",
    "    TARGET_COLUMN = 'target'\n",
    "    df = df_features.copy()\n",
    "    df = df.sort_values('timestamp')\n",
    "    if 'close' not in df.columns: exit(\"ERROR: 'close' column missing.\")\n",
    "    print(f\"Creating binary target: {PREDICTION_WINDOW_HOURS}h future return >= {TARGET_THRESHOLD_PCT}%...\")\n",
    "    df['future_price'] = df['close'].shift(-PREDICTION_WINDOW_ROWS)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "         df['price_return_future'] = (df['future_price'] - df['close']) / df['close'].replace(0, np.nan) * 100\n",
    "    df[TARGET_COLUMN] = np.where(df['price_return_future'] >= TARGET_THRESHOLD_PCT, 1, 0)\n",
    "    df.loc[df['price_return_future'].isnull(), TARGET_COLUMN] = np.nan\n",
    "    df = df.drop(['future_price', 'price_return_future'], axis=1)\n",
    "    initial_rows = len(df)\n",
    "    essential_check_cols = ['close', TARGET_COLUMN]\n",
    "    df = df.dropna(subset=essential_check_cols)\n",
    "    print(f\"Rows after removing NaN targets/close: {len(df)} (Removed {initial_rows - len(df)})\")\n",
    "    rows_before_feature_nan_check = len(df)\n",
    "    rows_after_feature_nan_dropna = len(df.dropna(subset=CURRENT_FEATURE_COLS))\n",
    "    potential_feature_nan_loss = rows_before_feature_nan_check - rows_after_feature_nan_dropna\n",
    "    if potential_feature_nan_loss > 0: print(f\"Note: {potential_feature_nan_loss} rows have NaNs in features. Models/Imputer will handle.\")\n",
    "    if df.empty: exit(\"DataFrame empty after target creation/NaN drop.\")\n",
    "    target_counts = df[TARGET_COLUMN].value_counts(normalize=True) * 100\n",
    "    print(\"\\nTarget variable distribution:\")\n",
    "    print(f\"  0 (< {TARGET_THRESHOLD_PCT}%): {target_counts.get(0, 0):.2f}%\")\n",
    "    print(f\"  1 (>= {TARGET_THRESHOLD_PCT}%): {target_counts.get(1, 0):.2f}%\")\n",
    "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "    print(f\"Final DataFrame shape for backtesting: {df.shape}\")\n",
    "\n",
    "    # --- 5. Walk-Forward Validation ---\n",
    "    print(\"\\n--- 5. Starting Walk-Forward Validation (Stacking - Logistic Regression Meta) ---\")\n",
    "    all_metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
    "    all_best_thresholds = []\n",
    "    meta_feature_names = ['xgb_pred', 'lgbm_pred', 'svm_pred']\n",
    "    # Store meta-feature importances (coefficients for LR)\n",
    "    meta_feature_importances = {meta_feat: [] for meta_feat in meta_feature_names}\n",
    "    iteration_count = 0\n",
    "    n_rows_total = len(df)\n",
    "    current_train_start_idx = 0\n",
    "    total_iterations_estimate = max(0, (n_rows_total - TRAIN_WINDOW_ROWS - TEST_WINDOW_ROWS) // STEP_ROWS + 1) if STEP_ROWS > 0 else 0\n",
    "\n",
    "    print(f\"Total rows: {n_rows_total}, Train: {TRAIN_WINDOW_HOURS}h ({TRAIN_WINDOW_ROWS} rows), Eval: {TEST_WINDOW_HOURS}h ({TEST_WINDOW_ROWS} rows), Step: {STEP_HOURS}h ({STEP_ROWS} rows)\")\n",
    "    print(f\"Estimated iterations: {total_iterations_estimate}\")\n",
    "    print(f\"Using {len(CURRENT_FEATURE_COLS)} features.\")\n",
    "    print(f\"Stacking Folds (K): {N_STACKING_FOLDS}\")\n",
    "    print(f\"Meta Learner: Logistic Regression, Tuning C over: {META_LR_PARAM_GRID['C']}\")\n",
    "    print(f\"Threshold Search Range: {THRESHOLD_SEARCH_RANGE}\")\n",
    "    print(\"-\" * 30)\n",
    "    start_loop_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        train_end_idx = current_train_start_idx + TRAIN_WINDOW_ROWS\n",
    "        test_start_idx = train_end_idx\n",
    "        test_end_idx = test_start_idx + TEST_WINDOW_ROWS\n",
    "        if test_end_idx > n_rows_total: print(f\"\\nStopping: Eval window end ({test_end_idx}) > total rows ({n_rows_total}).\"); break\n",
    "        if current_train_start_idx >= n_rows_total: print(f\"\\nStopping: Train start index ({current_train_start_idx}) reached end.\"); break\n",
    "\n",
    "        train_df = df.iloc[current_train_start_idx : train_end_idx].copy()\n",
    "        test_df = df.iloc[test_start_idx : test_end_idx].copy()\n",
    "\n",
    "        min_train_samples = max(50, int(0.1 * TRAIN_WINDOW_ROWS), N_STACKING_FOLDS * 5) # Need more for robust CV\n",
    "        min_test_samples = 10\n",
    "        if len(train_df) < min_train_samples or len(test_df) < min_test_samples:\n",
    "            print(f\"Skipping iter {iteration_count + 1}: Insufficient data train ({len(train_df)}/{min_train_samples}) or test ({len(test_df)}/{min_test_samples}).\")\n",
    "            current_train_start_idx += STEP_ROWS; continue\n",
    "\n",
    "        X_train_full = train_df[CURRENT_FEATURE_COLS]\n",
    "        y_train_full = train_df[TARGET_COLUMN]\n",
    "        X_test = test_df[CURRENT_FEATURE_COLS]\n",
    "        y_test = test_df[TARGET_COLUMN]\n",
    "\n",
    "        if len(y_train_full.unique()) < 2: print(f\"Skipping iter {iteration_count + 1}: Train data has only one class: {y_train_full.unique()}.\"); current_train_start_idx += STEP_ROWS; continue\n",
    "        if len(y_test.unique()) < 2: print(f\"Warning iter {iteration_count + 1}: Eval test data (size {len(test_df)}) has only one class: {y_test.unique()}. Metrics affected.\")\n",
    "\n",
    "        neg_count = y_train_full.value_counts().get(0, 0); pos_count = y_train_full.value_counts().get(1, 0)\n",
    "        scale_pos_weight_val = neg_count / pos_count if pos_count > 0 else 1.0\n",
    "\n",
    "        iter_start_time = time.time()\n",
    "        print(f\"\\n--- Iter {iteration_count + 1}/{total_iterations_estimate} ---\")\n",
    "        print(f\"  Train Indices: [{current_train_start_idx}:{train_end_idx-1}], Eval Indices: [{test_start_idx}:{test_end_idx-1}]\")\n",
    "        print(f\"  Train Target Dist: {dict(y_train_full.value_counts(normalize=True))}\")\n",
    "        print(f\"  Test Target Dist: {dict(y_test.value_counts(normalize=True))}\")\n",
    "        print(f\"  Using scale_pos_weight for XGB/LGBM: {scale_pos_weight_val:.4f}\")\n",
    "\n",
    "        # --- Grid Search for Base Models ---\n",
    "        print(\"  Grid searching base models...\")\n",
    "        best_xgb_params, xgb_score = grid_search_base_model('xgb', BASE_XGB_PARAM_GRID, X_train_full, y_train_full, scale_pos_weight_val)\n",
    "        best_lgbm_params, lgbm_score = grid_search_base_model('lgbm', BASE_LGBM_PARAM_GRID, X_train_full, y_train_full, scale_pos_weight_val)\n",
    "        best_svm_params, svm_score = grid_search_base_model('svm', BASE_SVM_PARAM_GRID, X_train_full, y_train_full, scale_pos_weight_val) # scale_pos not used directly for SVM pipeline\n",
    "        print(f\"    Best XGB Params: {best_xgb_params} (CV F1: {xgb_score:.3f})\")\n",
    "        print(f\"    Best LGBM Params: {best_lgbm_params} (CV F1: {lgbm_score:.3f})\")\n",
    "        print(f\"    Best SVM Params: {best_svm_params} (CV F1: {svm_score:.3f})\")\n",
    "\n",
    "        # --- Define Base Models using Best Parameters from Grid Search ---\n",
    "        xgb_iter_params = XGB_BASE_PARAMS.copy(); xgb_iter_params.update(best_xgb_params or {})\n",
    "        model_xgb_base = XGBClassifier(**xgb_iter_params, scale_pos_weight=scale_pos_weight_val)\n",
    "\n",
    "        lgbm_iter_params = LGBM_BASE_PARAMS.copy(); lgbm_iter_params.update(best_lgbm_params or {})\n",
    "        model_lgbm_base = LGBMClassifier(**lgbm_iter_params, scale_pos_weight=scale_pos_weight_val)\n",
    "\n",
    "        svm_iter_grid_params = best_svm_params or {}\n",
    "        pipeline_svm_base = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('svm', SVC(**SVM_BASE_PARAMS, **svm_iter_grid_params)) # Combine fixed & best grid params\n",
    "        ])\n",
    "        models_oof = {'xgb': model_xgb_base, 'lgbm': model_lgbm_base, 'svm': pipeline_svm_base}\n",
    "        oof_arrays = {'xgb': np.full(len(train_df), np.nan), 'lgbm': np.full(len(train_df), np.nan), 'svm': np.full(len(train_df), np.nan)}\n",
    "\n",
    "        # --- K-Fold OOF Generation ---\n",
    "        print(f\"  Level 0: Generating OOF predictions using {N_STACKING_FOLDS}-Fold CV...\")\n",
    "        skf = StratifiedKFold(n_splits=N_STACKING_FOLDS, shuffle=True, random_state=42 + iteration_count)\n",
    "        for fold, (train_idx_k, val_idx_k) in enumerate(skf.split(X_train_full, y_train_full)):\n",
    "            X_train_k, y_train_k = X_train_full.iloc[train_idx_k], y_train_full.iloc[train_idx_k]\n",
    "            X_val_k, y_val_k = X_train_full.iloc[val_idx_k], y_train_full.iloc[val_idx_k]\n",
    "\n",
    "            if len(np.unique(y_train_k)) < 2 or len(np.unique(y_val_k)) < 2:\n",
    "                print(f\"    Warning: Fold {fold+1} has single class. Assigning prior.\")\n",
    "                prior = y_train_full.mean(); [oof_arrays[key].__setitem__(val_idx_k, prior) for key in oof_arrays]; continue\n",
    "\n",
    "            for name, model in models_oof.items():\n",
    "                try:\n",
    "                    fit_params_k = {}\n",
    "                    if name == 'lgbm': fit_params_k = {'callbacks': [early_stopping(10, verbose=False), log_evaluation(0)], 'eval_metric': 'logloss', 'eval_set': [(X_val_k, y_val_k)]}\n",
    "                    elif name == 'xgb': fit_params_k = {'eval_set': [(X_val_k, y_val_k)], 'early_stopping_rounds': 10, 'verbose': False}\n",
    "                    model.fit(X_train_k, y_train_k, **fit_params_k)\n",
    "                    oof_arrays[name][val_idx_k] = model.predict_proba(X_val_k)[:, 1]\n",
    "                except Exception as e_kfold:\n",
    "                    print(f\"    Error in K-Fold {fold+1} for {name}: {e_kfold}\"); prior = y_train_full.mean(); oof_arrays[name][val_idx_k] = prior\n",
    "\n",
    "        X_meta_train_dict = {}\n",
    "        models_failed_oof = []\n",
    "        for name in models_oof:\n",
    "            oof_array = oof_arrays[name]\n",
    "            if np.isnan(oof_array).all(): models_failed_oof.append(name)\n",
    "            mean_oof = np.nanmean(oof_array)\n",
    "            if pd.isna(mean_oof): mean_oof = 0.5 # Fallback if all NaN\n",
    "            oof_array = np.nan_to_num(oof_array, nan=mean_oof)\n",
    "            if np.isnan(oof_arrays[name]).any(): print(f\"    Imputed NaNs in OOF for {name} with mean {mean_oof:.4f}\")\n",
    "            X_meta_train_dict[f'{name}_pred'] = oof_array\n",
    "\n",
    "        if models_failed_oof: print(f\"  ERROR: Base models {models_failed_oof} failed OOF generation. Skipping iter.\"); current_train_start_idx += STEP_ROWS; continue\n",
    "\n",
    "        X_meta_train = pd.DataFrame(X_meta_train_dict, index=X_train_full.index)\n",
    "        y_meta_train = y_train_full\n",
    "        print(f\"  Level 0 OOF Generation Done. Meta Train Shape: {X_meta_train.shape}\")\n",
    "\n",
    "        # --- Train Base Models on Full Training Data ---\n",
    "        print(f\"  Level 0: Training base models on full training data ({len(train_df)} rows)...\")\n",
    "        models_full = {}\n",
    "        all_base_trained = True\n",
    "        for name, model in models_oof.items():\n",
    "            try:\n",
    "                params = {}\n",
    "                if name == 'xgb': params['verbose'] = False\n",
    "                model.fit(X_train_full, y_train_full, **params)\n",
    "                models_full[name] = model\n",
    "            except Exception as e_full_fit: print(f\"  ERROR: Failed to train base model '{name}': {e_full_fit}\"); all_base_trained = False; break\n",
    "        if not all_base_trained: print(\"  Skipping iteration due to base model training failure.\"); current_train_start_idx += STEP_ROWS; continue\n",
    "        print(\"  Level 0 Full Training Done.\")\n",
    "\n",
    "        # --- Level 1: Meta Learner (Logistic Regression) Tuning & Threshold Tuning ---\n",
    "        print(\"  Level 1: Tuning Meta-Learner (Logistic Regression) and Probability Threshold...\")\n",
    "        best_meta_C = None\n",
    "        best_meta_score = -np.inf\n",
    "        best_meta_model_for_thresh = None\n",
    "        best_threshold_iter = 0.5\n",
    "        best_thresh_f1_score = -np.inf\n",
    "\n",
    "        # --- Scale Meta Features ---\n",
    "        meta_scaler = StandardScaler()\n",
    "        X_meta_train_scaled = meta_scaler.fit_transform(X_meta_train)\n",
    "        X_meta_train_scaled = pd.DataFrame(X_meta_train_scaled, index=X_meta_train.index, columns=X_meta_train.columns)\n",
    "\n",
    "        meta_val_size = int(len(X_meta_train_scaled) * META_VALIDATION_PCT)\n",
    "        if meta_val_size < 10 or (len(X_meta_train_scaled) - meta_val_size) < 10:\n",
    "            print(f\"  Warning: Meta dataset (size {len(X_meta_train_scaled)}) too small for validation split. Using default C=1.\")\n",
    "            best_meta_C = 1.0 # Default C\n",
    "        else:\n",
    "            X_meta_train_sub = X_meta_train_scaled[:-meta_val_size]\n",
    "            y_meta_train_sub = y_meta_train[:-meta_val_size]\n",
    "            X_meta_val = X_meta_train_scaled[-meta_val_size:]\n",
    "            y_meta_val = y_meta_train[-meta_val_size:]\n",
    "\n",
    "            if len(y_meta_val.unique()) < 2 or len(y_meta_train_sub.unique()) < 2:\n",
    "                print(\"  Warning: Meta train/val split has single class. Using default C=1.\")\n",
    "                best_meta_C = 1.0\n",
    "            else:\n",
    "                # Meta Grid Search (Tuning C for Logistic Regression)\n",
    "                print(f\"    Tuning meta learner C over {META_LR_PARAM_GRID['C']}...\")\n",
    "                for c_val in META_LR_PARAM_GRID['C']:\n",
    "                    try:\n",
    "                        current_meta_params = {**META_LR_FIXED_PARAMS, 'C': c_val}\n",
    "                        model_meta_cv = LogisticRegression(**current_meta_params)\n",
    "                        model_meta_cv.fit(X_meta_train_sub, y_meta_train_sub) # Fit on scaled sub-train\n",
    "                        y_pred_meta_val_cv = model_meta_cv.predict(X_meta_val) # Predict on scaled val\n",
    "                        meta_score = f1_score(y_meta_val, y_pred_meta_val_cv, average='binary', pos_label=1, zero_division=0)\n",
    "\n",
    "                        if meta_score >= best_meta_score: # Use >= to prefer simpler models (higher C = less regularization) if score is equal\n",
    "                            best_meta_score = meta_score\n",
    "                            best_meta_C = c_val\n",
    "                            best_meta_model_for_thresh = model_meta_cv # Keep best model for threshold tuning\n",
    "                    except Exception as e_meta_cv:\n",
    "                        print(f\"    Error during Meta CV with C={c_val}: {e_meta_cv}\")\n",
    "                        if best_meta_C is None: best_meta_C = 1.0 # Fallback if all fail\n",
    "\n",
    "                if best_meta_C is None: best_meta_C = 1.0 # Ensure we have a C if loop failed\n",
    "                print(f\"    Best Meta C Found: {best_meta_C} (Validation F1: {best_meta_score:.4f})\")\n",
    "\n",
    "                # Threshold Tuning using the best LR model found\n",
    "                if best_meta_model_for_thresh is not None:\n",
    "                    print(f\"    Tuning threshold over range {THRESHOLD_SEARCH_RANGE}...\")\n",
    "                    try:\n",
    "                        y_meta_proba_val = best_meta_model_for_thresh.predict_proba(X_meta_val)[:, 1] # Predict proba on scaled val\n",
    "                        f1_scores_thresh = {}\n",
    "                        for t in THRESHOLD_SEARCH_RANGE:\n",
    "                            y_pred_meta_val_t = (y_meta_proba_val >= t).astype(int)\n",
    "                            current_f1 = f1_score(y_meta_val, y_pred_meta_val_t, average='binary', pos_label=1, zero_division=0)\n",
    "                            f1_scores_thresh[t] = current_f1\n",
    "                            if current_f1 >= best_thresh_f1_score:\n",
    "                                best_thresh_f1_score = current_f1; best_threshold_iter = t\n",
    "                        print(f\"    Best Threshold Found: {best_threshold_iter:.2f} (Validation F1: {best_thresh_f1_score:.4f})\")\n",
    "                    except Exception as e_thresh: print(f\"    Error during threshold tuning: {e_thresh}. Using default {best_threshold_iter:.2f}.\")\n",
    "                else: print(f\"    Skipping threshold tuning. Using default {best_threshold_iter:.2f}.\")\n",
    "\n",
    "        # --- Level 1: Train Final Meta Learner (Logistic Regression) ---\n",
    "        print(\"  Level 1: Training final Meta-Learner (Logistic Regression)...\")\n",
    "        try:\n",
    "             final_meta_params = {**META_LR_FIXED_PARAMS, 'C': best_meta_C or 1.0} # Use best C or default\n",
    "             meta_model_final = LogisticRegression(**final_meta_params)\n",
    "             # Fit on the ENTIRE SCALED OOF training data\n",
    "             meta_model_final.fit(X_meta_train_scaled, y_meta_train)\n",
    "             print(\"  Level 1 Final Meta Training Done.\")\n",
    "        except Exception as e_meta_final: print(f\"  ERROR: Failed to train final meta-learner: {e_meta_final}\"); current_train_start_idx += STEP_ROWS; continue\n",
    "\n",
    "        # --- Prediction Phase ---\n",
    "        print(\"  Prediction: Generating final predictions on evaluation data...\")\n",
    "        try:\n",
    "            # Predict with Level 0 models on Eval data\n",
    "            pred_xgb_test = models_full['xgb'].predict_proba(X_test)[:, 1]\n",
    "            pred_lgbm_test = models_full['lgbm'].predict_proba(X_test)[:, 1]\n",
    "            pred_svm_test = models_full['svm'].predict_proba(X_test)[:, 1] # SVM pipeline handles impute/scale\n",
    "\n",
    "            # Create Meta Features for Test Data\n",
    "            X_meta_test = pd.DataFrame({'xgb_pred': pred_xgb_test, 'lgbm_pred': pred_lgbm_test, 'svm_pred': pred_svm_test}, index=X_test.index)\n",
    "\n",
    "            # --- Scale Meta Test Features using the SAME scaler ---\n",
    "            X_meta_test_scaled = meta_scaler.transform(X_meta_test)\n",
    "\n",
    "            # Predict Probabilities with Final Meta Learner on SCALED meta test features\n",
    "            y_proba_test = meta_model_final.predict_proba(X_meta_test_scaled)[:, 1]\n",
    "\n",
    "            # Apply Tuned Threshold\n",
    "            y_pred = (y_proba_test >= best_threshold_iter).astype(int)\n",
    "            print(\"  Prediction Done.\")\n",
    "\n",
    "        except Exception as e_pred:\n",
    "             print(f\"  ERROR during prediction phase: {e_pred}\");\n",
    "             [all_metrics[key].append(np.nan) for key in all_metrics]; all_best_thresholds.append(np.nan);\n",
    "             [meta_feature_importances[key].append(np.nan) for key in meta_feature_names] # Add NaN for importance too\n",
    "             current_train_start_idx += STEP_ROWS; continue\n",
    "\n",
    "        # --- Evaluation ---\n",
    "        if len(np.unique(y_test)) < 2:\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            precision = precision_score(y_test, y_pred, average='binary', pos_label=1, zero_division=0)\n",
    "            recall = recall_score(y_test, y_pred, average='binary', pos_label=1, zero_division=0)\n",
    "            f1 = f1_score(y_test, y_pred, average='binary', pos_label=1, zero_division=0)\n",
    "            print(f\"  Evaluation Metrics (Test Window Size: {TEST_WINDOW_HOURS}h, SINGLE CLASS {y_test.unique()[0]}): Acc={accuracy:.4f}, Prc={precision:.4f}, Rec={recall:.4f}, F1={f1:.4f}\")\n",
    "        else:\n",
    "             accuracy = accuracy_score(y_test, y_pred)\n",
    "             precision = precision_score(y_test, y_pred, average='binary', pos_label=1, zero_division=0)\n",
    "             recall = recall_score(y_test, y_pred, average='binary', pos_label=1, zero_division=0)\n",
    "             f1 = f1_score(y_test, y_pred, average='binary', pos_label=1, zero_division=0)\n",
    "             print(f\"  Evaluation Metrics (Test Window Size: {TEST_WINDOW_HOURS}h): Acc={accuracy:.4f}, Prc={precision:.4f}, Rec={recall:.4f}, F1={f1:.4f}\")\n",
    "\n",
    "        all_metrics['accuracy'].append(accuracy)\n",
    "        all_metrics['precision'].append(precision)\n",
    "        all_metrics['recall'].append(recall)\n",
    "        all_metrics['f1'].append(f1)\n",
    "        all_best_thresholds.append(best_threshold_iter)\n",
    "\n",
    "        # --- Store Meta-Learner Feature Importances (Logistic Regression Coefficients) ---\n",
    "        try:\n",
    "            if isinstance(meta_model_final, LogisticRegression) and hasattr(meta_model_final, 'coef_'):\n",
    "                # Get absolute coefficient values as importance\n",
    "                abs_coeffs = np.abs(meta_model_final.coef_[0])\n",
    "                if len(abs_coeffs) == len(meta_feature_names):\n",
    "                    for i, name in enumerate(meta_feature_names):\n",
    "                        meta_feature_importances[name].append(abs_coeffs[i])\n",
    "                else:\n",
    "                     print(\"  Warning: Meta-feature names count mismatch with LR coefficients.\")\n",
    "                     [meta_feature_importances[key].append(np.nan) for key in meta_feature_names]\n",
    "            else:\n",
    "                 print(\"  Meta-learner is not Logistic Regression or has no coefficients.\")\n",
    "                 [meta_feature_importances[key].append(np.nan) for key in meta_feature_names]\n",
    "        except Exception as e_imp:\n",
    "            print(f\"  Warning: Could not get meta-feature importance (LR Coeffs): {e_imp}\")\n",
    "            [meta_feature_importances[key].append(np.nan) for key in meta_feature_names]\n",
    "\n",
    "        iteration_count += 1\n",
    "        iter_end_time = time.time()\n",
    "        print(f\"  Iteration {iteration_count} finished in {iter_end_time - iter_start_time:.2f} seconds.\")\n",
    "        print(\"-\" * 20)\n",
    "        current_train_start_idx += STEP_ROWS\n",
    "\n",
    "    # --- End of Walk-Forward Loop ---\n",
    "    end_loop_time = time.time()\n",
    "    loop_duration_minutes = (end_loop_time - start_loop_time) / 60\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Walk-Forward Validation (Stacking - Ensemble_Method_C) finished in {end_loop_time - start_loop_time:.2f} seconds ({loop_duration_minutes:.2f} minutes).\")\n",
    "\n",
    "    # --- 6. Aggregate and Display Results ---\n",
    "    print(\"\\n--- 6. Final Results (Ensemble_Method_C) ---\")\n",
    "    if iteration_count > 0 and len(all_metrics['f1']) > 0:\n",
    "        valid_indices = [i for i, f1 in enumerate(all_metrics['f1']) if not pd.isna(f1)]\n",
    "        if valid_indices:\n",
    "            valid_accuracy = [all_metrics['accuracy'][i] for i in valid_indices]\n",
    "            valid_precision = [all_metrics['precision'][i] for i in valid_indices]\n",
    "            valid_recall = [all_metrics['recall'][i] for i in valid_indices]\n",
    "            valid_f1 = [all_metrics['f1'][i] for i in valid_indices]\n",
    "            valid_thresholds = [all_best_thresholds[i] for i in valid_indices if not pd.isna(all_best_thresholds[i])]\n",
    "\n",
    "            avg_accuracy = np.mean(valid_accuracy); avg_precision = np.mean(valid_precision)\n",
    "            avg_recall = np.mean(valid_recall); avg_f1 = np.mean(valid_f1)\n",
    "\n",
    "            print(\"\\n--- Average Walk-Forward Validation Results ---\")\n",
    "            print(f\"Total Iterations Run: {iteration_count}, Successful Evaluations: {len(valid_indices)}\")\n",
    "            print(f\"Target: >= {TARGET_THRESHOLD_PCT}% increase over {PREDICTION_WINDOW_HOURS} hours\")\n",
    "            print(f\"Train: {TRAIN_WINDOW_HOURS}h, Eval: {TEST_WINDOW_HOURS}h, Step: {STEP_HOURS}h\")\n",
    "            print(f\"Stacking Folds: {N_STACKING_FOLDS}, Meta-Learner: Logistic Regression\")\n",
    "            print(f\"Average Accuracy:  {avg_accuracy:.4f}\")\n",
    "            print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "            print(f\"Average Recall:    {avg_recall:.4f}\")\n",
    "            print(f\"Average F1-Score:  {avg_f1:.4f}\")\n",
    "\n",
    "            std_accuracy = np.std(valid_accuracy); std_precision = np.std(valid_precision)\n",
    "            std_recall = np.std(valid_recall); std_f1 = np.std(valid_f1)\n",
    "            print(\"\\n--- Standard Deviation of Metrics ---\")\n",
    "            print(f\"Std Dev Accuracy:  {std_accuracy:.4f}\")\n",
    "            print(f\"Std Dev Precision: {std_precision:.4f}\")\n",
    "            print(f\"Std Dev Recall:    {std_recall:.4f}\")\n",
    "            print(f\"Std Dev F1-Score:  {std_f1:.4f}\")\n",
    "\n",
    "            if valid_thresholds:\n",
    "                avg_threshold = np.mean(valid_thresholds); std_threshold = np.std(valid_thresholds)\n",
    "                print(f\"\\nAverage Best Threshold: {avg_threshold:.3f} (StdDev: {std_threshold:.3f}) over {len(valid_thresholds)} folds\")\n",
    "            else: print(\"\\nCould not determine average threshold.\")\n",
    "\n",
    "            print(\"\\n--- Average Meta-Feature Importances (Abs Coefficient Magnitude) ---\")\n",
    "            avg_meta_importances = {}\n",
    "            for f, imp_list in meta_feature_importances.items():\n",
    "                 valid_imps = [imp for imp in imp_list if not pd.isna(imp)]\n",
    "                 avg_meta_importances[f] = np.mean(valid_imps) if valid_imps else 0.0\n",
    "\n",
    "            total_importance = sum(avg_meta_importances.values())\n",
    "            if total_importance > 1e-9:\n",
    "                 normalized_importances = {f: (imp / total_importance) * 100 for f, imp in avg_meta_importances.items()}\n",
    "                 sorted_meta_importances = sorted(normalized_importances.items(), key=lambda item: item[1], reverse=True)\n",
    "                 print(\"  (Normalized % Magnitude)\")\n",
    "                 for i, (feature, importance) in enumerate(sorted_meta_importances): print(f\"  {i+1}. {feature}: {importance:.2f}%\")\n",
    "            else: print(\"  Meta-feature importance data (coefficients) could not be calculated or was always zero.\")\n",
    "        else: print(\"\\nNo valid metrics recorded.\")\n",
    "    else: print(\"\\nNo iterations were successfully completed or no metrics generated.\")\n",
    "\n",
    "    print(\"\\nScript Ensemble_Method_C finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
