{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Data Loading & Initial Prep ---\n",
      "Loading data from: C:\\Users\\mason\\AVP\\BTCUSDrec.csv\n",
      "Raw data loaded. Shape: (15177, 9)\n",
      "Initial data prep done. Shape: (15177, 7)\n",
      "\n",
      "--- 2. Feature Engineering (Selected Features) ---\n",
      "Starting calculation for 40 VIF-filtered features...\n",
      "  Calculating necessary prerequisites...\n",
      "  Calculating final target features...\n",
      "  Assembling final dataframe...\n",
      "Selected feature calculation finished. Returning 15177 rows, 46 total columns (40 features). Took 0.11s.\n",
      "Feature calculation completed in 0.11 seconds.\n",
      "Using 40 features found in DataFrame for modeling.\n",
      "\n",
      "--- 3. Data Cleaning (Post-Features) ---\n",
      "Total NaNs found in feature columns: 330.\n",
      "\n",
      "--- 4. Modeling Target & Final Prep ---\n",
      "Creating binary target based on 6-hour future return >= 0.25%...\n",
      "Rows after removing NaN targets/close: 15177 (Removed 0)\n",
      "\n",
      "Target variable distribution:\n",
      "  0 (< 0.25% return): 62.56%\n",
      "  1 (>= 0.25% return): 37.44%\n",
      "Final DataFrame shape for backtesting: (15177, 47)\n",
      "\n",
      "--- 5. Starting Walk-Forward Validation (Stacking Ensemble) ---\n",
      "Total rows: 15177, Train Window: 1344h, Prediction Horizon: 6h, Evaluation (Test) Window: 168h, Step: 12h\n",
      "Estimated iterations: 1139\n",
      "Stacking Folds (K): 5\n",
      "Meta Learner Grid: {'max_depth': [2, 3], 'n_estimators': [40, 60], 'eta': [0.03, 0.05], 'lambda': [1.5, 2.5], 'subsample': [0.8, 1.0], 'colsample_bytree': [0.9, 1.0]}\n",
      "Threshold Search Range: [0.1  0.15 0.2  0.25 0.3  0.35 0.4  0.45 0.5  0.55 0.6  0.65 0.7  0.75\n",
      " 0.8  0.85]\n",
      "------------------------------\n",
      "\n",
      "--- Iter 1/1139 ---\n",
      "  Train Indices: [0:1343], Evaluation Indices: [1344:1511]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.05, 'lambda': 1.5, 'max_depth': 3, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.5098)\n",
      "    Best Threshold: 0.50 (Val F1: 0.5098)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.7560, Prc=0.6098, Rec=0.5000, F1=0.5495\n",
      "  Iteration 1 finished in 2.85 seconds.\n",
      "\n",
      "--- Iter 2/1139 ---\n",
      "  Train Indices: [12:1355], Evaluation Indices: [1356:1523]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.05, 'lambda': 1.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.4746)\n",
      "    Best Threshold: 0.60 (Val F1: 0.4923)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.7619, Prc=0.6207, Rec=0.3830, F1=0.4737\n",
      "  Iteration 2 finished in 2.93 seconds.\n",
      "\n",
      "--- Iter 3/1139 ---\n",
      "  Train Indices: [24:1367], Evaluation Indices: [1368:1535]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.03, 'lambda': 2.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.5679)\n",
      "    Best Threshold: 0.50 (Val F1: 0.5679)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.6964, Prc=0.5000, Rec=0.4902, F1=0.4950\n",
      "  Iteration 3 finished in 2.75 seconds.\n",
      "\n",
      "--- Iter 4/1139 ---\n",
      "  Train Indices: [36:1379], Evaluation Indices: [1380:1547]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.05, 'lambda': 1.5, 'max_depth': 2, 'n_estimators': 60, 'subsample': 0.8} (Val F1: 0.5287)\n",
      "    Best Threshold: 0.50 (Val F1: 0.5287)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.6726, Prc=0.4286, Rec=0.5106, F1=0.4660\n",
      "  Iteration 4 finished in 2.74 seconds.\n",
      "\n",
      "--- Iter 5/1139 ---\n",
      "  Train Indices: [48:1391], Evaluation Indices: [1392:1559]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.03, 'lambda': 2.5, 'max_depth': 3, 'n_estimators': 40, 'subsample': 1.0} (Val F1: 0.4737)\n",
      "    Best Threshold: 0.50 (Val F1: 0.4737)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.6726, Prc=0.4407, Rec=0.5417, F1=0.4860\n",
      "  Iteration 5 finished in 2.67 seconds.\n",
      "\n",
      "--- Iter 6/1139 ---\n",
      "  Train Indices: [60:1403], Evaluation Indices: [1404:1571]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.03, 'lambda': 2.5, 'max_depth': 3, 'n_estimators': 40, 'subsample': 1.0} (Val F1: 0.5081)\n",
      "    Best Threshold: 0.50 (Val F1: 0.5081)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.6429, Prc=0.4308, Rec=0.5490, F1=0.4828\n",
      "  Iteration 6 finished in 2.69 seconds.\n",
      "\n",
      "--- Iter 7/1139 ---\n",
      "  Train Indices: [72:1415], Evaluation Indices: [1416:1583]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.03, 'lambda': 1.5, 'max_depth': 3, 'n_estimators': 40, 'subsample': 1.0} (Val F1: 0.4848)\n",
      "    Best Threshold: 0.50 (Val F1: 0.4848)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.7262, Prc=0.5472, Rec=0.5686, F1=0.5577\n",
      "  Iteration 7 finished in 2.56 seconds.\n",
      "\n",
      "--- Iter 8/1139 ---\n",
      "  Train Indices: [84:1427], Evaluation Indices: [1428:1595]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.05, 'lambda': 1.5, 'max_depth': 3, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.4551)\n",
      "    Best Threshold: 0.55 (Val F1: 0.4714)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.7202, Prc=0.5714, Rec=0.3846, F1=0.4598\n",
      "  Iteration 8 finished in 2.76 seconds.\n",
      "\n",
      "--- Iter 9/1139 ---\n",
      "  Train Indices: [96:1439], Evaluation Indices: [1440:1607]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.03, 'lambda': 1.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 1.0} (Val F1: 0.5763)\n",
      "    Best Threshold: 0.55 (Val F1: 0.5763)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.7083, Prc=0.5455, Rec=0.2353, F1=0.3288\n",
      "  Iteration 9 finished in 2.57 seconds.\n",
      "\n",
      "--- Iter 10/1139 ---\n",
      "  Train Indices: [108:1451], Evaluation Indices: [1452:1619]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.05, 'lambda': 2.5, 'max_depth': 2, 'n_estimators': 60, 'subsample': 0.8} (Val F1: 0.4971)\n",
      "    Best Threshold: 0.65 (Val F1: 0.5113)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.6845, Prc=0.5263, Rec=0.1852, F1=0.2740\n",
      "  Iteration 10 finished in 2.66 seconds.\n",
      "\n",
      "--- Iter 11/1139 ---\n",
      "  Train Indices: [120:1463], Evaluation Indices: [1464:1631]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.03, 'lambda': 1.5, 'max_depth': 3, 'n_estimators': 60, 'subsample': 0.8} (Val F1: 0.5180)\n",
      "    Best Threshold: 0.50 (Val F1: 0.5180)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.7024, Prc=0.6774, Rec=0.3443, F1=0.4565\n",
      "  Iteration 11 finished in 2.58 seconds.\n",
      "\n",
      "--- Iter 12/1139 ---\n",
      "  Train Indices: [132:1475], Evaluation Indices: [1476:1643]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.05, 'lambda': 1.5, 'max_depth': 2, 'n_estimators': 60, 'subsample': 1.0} (Val F1: 0.5644)\n",
      "    Best Threshold: 0.50 (Val F1: 0.5644)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.7024, Prc=0.6500, Rec=0.2321, F1=0.3421\n",
      "  Iteration 12 finished in 2.69 seconds.\n",
      "\n",
      "--- Iter 13/1139 ---\n",
      "  Train Indices: [144:1487], Evaluation Indices: [1488:1655]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.03, 'lambda': 2.5, 'max_depth': 3, 'n_estimators': 60, 'subsample': 0.8} (Val F1: 0.5424)\n",
      "    Best Threshold: 0.50 (Val F1: 0.5424)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.6845, Prc=0.6316, Rec=0.2069, F1=0.3117\n",
      "  Iteration 13 finished in 2.62 seconds.\n",
      "\n",
      "--- Iter 14/1139 ---\n",
      "  Train Indices: [156:1499], Evaluation Indices: [1500:1667]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.03, 'lambda': 2.5, 'max_depth': 2, 'n_estimators': 60, 'subsample': 1.0} (Val F1: 0.5263)\n",
      "    Best Threshold: 0.50 (Val F1: 0.5263)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.7083, Prc=0.6190, Rec=0.4407, F1=0.5149\n",
      "  Iteration 14 finished in 2.61 seconds.\n",
      "\n",
      "--- Iter 15/1139 ---\n",
      "  Train Indices: [168:1511], Evaluation Indices: [1512:1679]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.03, 'lambda': 2.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.5960)\n",
      "    Best Threshold: 0.50 (Val F1: 0.5960)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.6905, Prc=0.5806, Rec=0.3158, F1=0.4091\n",
      "  Iteration 15 finished in 2.65 seconds.\n",
      "\n",
      "--- Iter 16/1139 ---\n",
      "  Train Indices: [180:1523], Evaluation Indices: [1524:1691]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.05, 'lambda': 1.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.5647)\n",
      "    Best Threshold: 0.50 (Val F1: 0.5647)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.6726, Prc=0.5000, Rec=0.2000, F1=0.2857\n",
      "  Iteration 16 finished in 2.59 seconds.\n",
      "\n",
      "--- Iter 17/1139 ---\n",
      "  Train Indices: [192:1535], Evaluation Indices: [1536:1703]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.03, 'lambda': 2.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 1.0} (Val F1: 0.6012)\n",
      "    Best Threshold: 0.50 (Val F1: 0.6012)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.6667, Prc=0.4107, Rec=0.5000, F1=0.4510\n",
      "  Iteration 17 finished in 2.71 seconds.\n",
      "\n",
      "--- Iter 18/1139 ---\n",
      "  Train Indices: [204:1547], Evaluation Indices: [1548:1715]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.05, 'lambda': 1.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.6023)\n",
      "    Best Threshold: 0.50 (Val F1: 0.6023)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.6667, Prc=0.4000, Rec=0.4348, F1=0.4167\n",
      "  Iteration 18 finished in 2.81 seconds.\n",
      "\n",
      "--- Iter 19/1139 ---\n",
      "  Train Indices: [216:1559], Evaluation Indices: [1560:1727]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.05, 'lambda': 2.5, 'max_depth': 2, 'n_estimators': 60, 'subsample': 1.0} (Val F1: 0.6502)\n",
      "    Best Threshold: 0.50 (Val F1: 0.6502)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.6964, Prc=0.4146, Rec=0.3864, F1=0.4000\n",
      "  Iteration 19 finished in 2.73 seconds.\n",
      "\n",
      "--- Iter 20/1139 ---\n",
      "  Train Indices: [228:1571], Evaluation Indices: [1572:1739]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.03, 'lambda': 1.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 1.0} (Val F1: 0.6738)\n",
      "    Best Threshold: 0.50 (Val F1: 0.6738)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.7024, Prc=0.3571, Rec=0.3947, F1=0.3750\n",
      "  Iteration 20 finished in 2.81 seconds.\n",
      "\n",
      "--- Iter 21/1139 ---\n",
      "  Train Indices: [240:1583], Evaluation Indices: [1584:1751]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.03, 'lambda': 1.5, 'max_depth': 3, 'n_estimators': 60, 'subsample': 1.0} (Val F1: 0.6517)\n",
      "    Best Threshold: 0.50 (Val F1: 0.6517)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.7500, Prc=0.4167, Rec=0.2632, F1=0.3226\n",
      "  Iteration 21 finished in 2.77 seconds.\n",
      "\n",
      "--- Iter 22/1139 ---\n",
      "  Train Indices: [252:1595], Evaluation Indices: [1596:1763]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.05, 'lambda': 1.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.5445)\n",
      "    Best Threshold: 0.55 (Val F1: 0.5476)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.7560, Prc=0.4000, Rec=0.2162, F1=0.2807\n",
      "  Iteration 22 finished in 2.65 seconds.\n",
      "\n",
      "--- Iter 23/1139 ---\n",
      "  Train Indices: [264:1607], Evaluation Indices: [1608:1775]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.05, 'lambda': 1.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.6277)\n",
      "    Best Threshold: 0.50 (Val F1: 0.6277)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.7440, Prc=0.3684, Rec=0.1842, F1=0.2456\n",
      "  Iteration 23 finished in 2.75 seconds.\n",
      "\n",
      "--- Iter 24/1139 ---\n",
      "  Train Indices: [276:1619], Evaluation Indices: [1620:1787]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.03, 'lambda': 2.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.5865)\n",
      "    Best Threshold: 0.45 (Val F1: 0.5911)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.6726, Prc=0.2500, Rec=0.3871, F1=0.3038\n",
      "  Iteration 24 finished in 2.78 seconds.\n",
      "\n",
      "--- Iter 25/1139 ---\n",
      "  Train Indices: [288:1631], Evaluation Indices: [1632:1799]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.05, 'lambda': 1.5, 'max_depth': 2, 'n_estimators': 60, 'subsample': 0.8} (Val F1: 0.6126)\n",
      "    Best Threshold: 0.50 (Val F1: 0.6126)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.6905, Prc=0.3103, Rec=0.6000, F1=0.4091\n",
      "  Iteration 25 finished in 2.99 seconds.\n",
      "\n",
      "--- Iter 26/1139 ---\n",
      "  Train Indices: [300:1643], Evaluation Indices: [1644:1811]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.05, 'lambda': 2.5, 'max_depth': 3, 'n_estimators': 60, 'subsample': 0.8} (Val F1: 0.6188)\n",
      "    Best Threshold: 0.50 (Val F1: 0.6188)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.7500, Prc=0.3542, Rec=0.6071, F1=0.4474\n",
      "  Iteration 26 finished in 2.95 seconds.\n",
      "\n",
      "--- Iter 27/1139 ---\n",
      "  Train Indices: [312:1655], Evaluation Indices: [1656:1823]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.03, 'lambda': 2.5, 'max_depth': 2, 'n_estimators': 60, 'subsample': 1.0} (Val F1: 0.6606)\n",
      "    Best Threshold: 0.50 (Val F1: 0.6606)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.7143, Prc=0.2128, Rec=0.4762, F1=0.2941\n",
      "  Iteration 27 finished in 2.99 seconds.\n",
      "\n",
      "--- Iter 28/1139 ---\n",
      "  Train Indices: [324:1667], Evaluation Indices: [1668:1835]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.03, 'lambda': 2.5, 'max_depth': 3, 'n_estimators': 60, 'subsample': 1.0} (Val F1: 0.6250)\n",
      "    Best Threshold: 0.50 (Val F1: 0.6250)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.6845, Prc=0.2222, Rec=0.5217, F1=0.3117\n",
      "  Iteration 28 finished in 2.93 seconds.\n",
      "\n",
      "--- Iter 29/1139 ---\n",
      "  Train Indices: [336:1679], Evaluation Indices: [1680:1847]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.03, 'lambda': 1.5, 'max_depth': 3, 'n_estimators': 60, 'subsample': 1.0} (Val F1: 0.6008)\n",
      "    Best Threshold: 0.50 (Val F1: 0.6008)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.6369, Prc=0.1695, Rec=0.4545, F1=0.2469\n",
      "  Iteration 29 finished in 2.90 seconds.\n",
      "\n",
      "--- Iter 30/1139 ---\n",
      "  Train Indices: [348:1691], Evaluation Indices: [1692:1859]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.03, 'lambda': 2.5, 'max_depth': 2, 'n_estimators': 60, 'subsample': 1.0} (Val F1: 0.5600)\n",
      "    Best Threshold: 0.45 (Val F1: 0.5792)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.6429, Prc=0.2817, Rec=0.6897, F1=0.4000\n",
      "  Iteration 30 finished in 3.02 seconds.\n",
      "\n",
      "--- Iter 31/1139 ---\n",
      "  Train Indices: [360:1703], Evaluation Indices: [1704:1871]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.05, 'lambda': 1.5, 'max_depth': 3, 'n_estimators': 40, 'subsample': 1.0} (Val F1: 0.5900)\n",
      "    Best Threshold: 0.50 (Val F1: 0.5900)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.6369, Prc=0.3125, Rec=0.5405, F1=0.3960\n",
      "  Iteration 31 finished in 2.87 seconds.\n",
      "\n",
      "--- Iter 32/1139 ---\n",
      "  Train Indices: [372:1715], Evaluation Indices: [1716:1883]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.05, 'lambda': 2.5, 'max_depth': 3, 'n_estimators': 60, 'subsample': 0.8} (Val F1: 0.5780)\n",
      "    Best Threshold: 0.50 (Val F1: 0.5780)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.6845, Prc=0.3871, Rec=0.6154, F1=0.4752\n",
      "  Iteration 32 finished in 3.01 seconds.\n",
      "\n",
      "--- Iter 33/1139 ---\n",
      "  Train Indices: [384:1727], Evaluation Indices: [1728:1895]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.05, 'lambda': 2.5, 'max_depth': 3, 'n_estimators': 60, 'subsample': 1.0} (Val F1: 0.4693)\n",
      "    Best Threshold: 0.30 (Val F1: 0.5425)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.5774, Prc=0.3333, Rec=0.8205, F1=0.4741\n",
      "  Iteration 33 finished in 2.87 seconds.\n",
      "\n",
      "--- Iter 34/1139 ---\n",
      "  Train Indices: [396:1739], Evaluation Indices: [1740:1907]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.03, 'lambda': 2.5, 'max_depth': 3, 'n_estimators': 60, 'subsample': 1.0} (Val F1: 0.6000)\n",
      "    Best Threshold: 0.50 (Val F1: 0.6000)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.6250, Prc=0.3231, Rec=0.5250, F1=0.4000\n",
      "  Iteration 34 finished in 2.70 seconds.\n",
      "\n",
      "--- Iter 35/1139 ---\n",
      "  Train Indices: [408:1751], Evaluation Indices: [1752:1919]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.03, 'lambda': 1.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 1.0} (Val F1: 0.5435)\n",
      "    Best Threshold: 0.40 (Val F1: 0.5512)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.5238, Prc=0.3053, Rec=0.6744, F1=0.4203\n",
      "  Iteration 35 finished in 2.69 seconds.\n",
      "\n",
      "--- Iter 36/1139 ---\n",
      "  Train Indices: [420:1763], Evaluation Indices: [1764:1931]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.03, 'lambda': 1.5, 'max_depth': 3, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.5894)\n",
      "    Best Threshold: 0.50 (Val F1: 0.5894)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.5595, Prc=0.3088, Rec=0.4375, F1=0.3621\n",
      "  Iteration 36 finished in 2.56 seconds.\n",
      "\n",
      "--- Iter 37/1139 ---\n",
      "  Train Indices: [432:1775], Evaluation Indices: [1776:1943]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.03, 'lambda': 1.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 1.0} (Val F1: 0.5785)\n",
      "    Best Threshold: 0.40 (Val F1: 0.5962)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.4583, Prc=0.3010, Rec=0.6200, F1=0.4052\n",
      "  Iteration 37 finished in 2.65 seconds.\n",
      "\n",
      "--- Iter 38/1139 ---\n",
      "  Train Indices: [444:1787], Evaluation Indices: [1788:1955]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.05, 'lambda': 1.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.5579)\n",
      "    Best Threshold: 0.50 (Val F1: 0.5579)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.5298, Prc=0.3333, Rec=0.2951, F1=0.3130\n",
      "  Iteration 38 finished in 2.56 seconds.\n",
      "\n",
      "--- Iter 39/1139 ---\n",
      "  Train Indices: [456:1799], Evaluation Indices: [1800:1967]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.03, 'lambda': 2.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.5446)\n",
      "    Best Threshold: 0.50 (Val F1: 0.5446)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.5179, Prc=0.3103, Rec=0.3051, F1=0.3077\n",
      "  Iteration 39 finished in 2.64 seconds.\n",
      "\n",
      "--- Iter 40/1139 ---\n",
      "  Train Indices: [468:1811], Evaluation Indices: [1812:1979]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.03, 'lambda': 1.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 1.0} (Val F1: 0.5196)\n",
      "    Best Threshold: 0.45 (Val F1: 0.5259)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.4762, Prc=0.3158, Rec=0.4000, F1=0.3529\n",
      "  Iteration 40 finished in 2.56 seconds.\n",
      "\n",
      "--- Iter 41/1139 ---\n",
      "  Train Indices: [480:1823], Evaluation Indices: [1824:1991]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.05, 'lambda': 1.5, 'max_depth': 3, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.5070)\n",
      "    Best Threshold: 0.45 (Val F1: 0.5085)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.4821, Prc=0.3205, Rec=0.4237, F1=0.3650\n",
      "  Iteration 41 finished in 2.61 seconds.\n",
      "\n",
      "--- Iter 42/1139 ---\n",
      "  Train Indices: [492:1835], Evaluation Indices: [1836:2003]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.05, 'lambda': 1.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.5490)\n",
      "    Best Threshold: 0.50 (Val F1: 0.5490)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.4464, Prc=0.2963, Rec=0.4000, F1=0.3404\n",
      "  Iteration 42 finished in 2.55 seconds.\n",
      "\n",
      "--- Iter 43/1139 ---\n",
      "  Train Indices: [504:1847], Evaluation Indices: [1848:2015]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.05, 'lambda': 2.5, 'max_depth': 3, 'n_estimators': 60, 'subsample': 0.8} (Val F1: 0.5161)\n",
      "    Best Threshold: 0.50 (Val F1: 0.5161)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.5060, Prc=0.3636, Rec=0.4516, F1=0.4029\n",
      "  Iteration 43 finished in 2.60 seconds.\n",
      "\n",
      "--- Iter 44/1139 ---\n",
      "  Train Indices: [516:1859], Evaluation Indices: [1860:2027]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.05, 'lambda': 2.5, 'max_depth': 3, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.4767)\n",
      "    Best Threshold: 0.25 (Val F1: 0.5130)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.4167, Prc=0.3485, Rec=0.7931, F1=0.4842\n",
      "  Iteration 44 finished in 2.55 seconds.\n",
      "\n",
      "--- Iter 45/1139 ---\n",
      "  Train Indices: [528:1871], Evaluation Indices: [1872:2039]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.05, 'lambda': 2.5, 'max_depth': 2, 'n_estimators': 60, 'subsample': 1.0} (Val F1: 0.4973)\n",
      "    Best Threshold: 0.35 (Val F1: 0.5214)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.4107, Prc=0.3025, Rec=0.6923, F1=0.4211\n",
      "  Iteration 45 finished in 2.58 seconds.\n",
      "\n",
      "--- Iter 46/1139 ---\n",
      "  Train Indices: [540:1883], Evaluation Indices: [1884:2051]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.03, 'lambda': 1.5, 'max_depth': 3, 'n_estimators': 40, 'subsample': 1.0} (Val F1: 0.4607)\n",
      "    Best Threshold: 0.35 (Val F1: 0.5278)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.3929, Prc=0.2857, Rec=0.5926, F1=0.3855\n",
      "  Iteration 46 finished in 2.59 seconds.\n",
      "\n",
      "--- Iter 47/1139 ---\n",
      "  Train Indices: [552:1895], Evaluation Indices: [1896:2063]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.03, 'lambda': 1.5, 'max_depth': 3, 'n_estimators': 40, 'subsample': 1.0} (Val F1: 0.4571)\n",
      "    Best Threshold: 0.40 (Val F1: 0.5134)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.4226, Prc=0.3607, Rec=0.6984, F1=0.4757\n",
      "  Iteration 47 finished in 2.55 seconds.\n",
      "\n",
      "--- Iter 48/1139 ---\n",
      "  Train Indices: [564:1907], Evaluation Indices: [1908:2075]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.05, 'lambda': 1.5, 'max_depth': 3, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.4211)\n",
      "    Best Threshold: 0.25 (Val F1: 0.4721)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.4643, Prc=0.4097, Rec=0.9219, F1=0.5673\n",
      "  Iteration 48 finished in 2.71 seconds.\n",
      "\n",
      "--- Iter 49/1139 ---\n",
      "  Train Indices: [576:1919], Evaluation Indices: [1920:2087]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.03, 'lambda': 2.5, 'max_depth': 2, 'n_estimators': 60, 'subsample': 0.8} (Val F1: 0.4279)\n",
      "    Best Threshold: 0.35 (Val F1: 0.4604)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.4583, Prc=0.3729, Rec=0.7213, F1=0.4916\n",
      "  Iteration 49 finished in 2.54 seconds.\n",
      "\n",
      "--- Iter 50/1139 ---\n",
      "  Train Indices: [588:1931], Evaluation Indices: [1932:2099]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.03, 'lambda': 1.5, 'max_depth': 3, 'n_estimators': 40, 'subsample': 1.0} (Val F1: 0.3889)\n",
      "    Best Threshold: 0.35 (Val F1: 0.4567)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.3988, Prc=0.3586, Rec=0.8667, F1=0.5073\n",
      "  Iteration 50 finished in 2.55 seconds.\n",
      "\n",
      "--- Iter 51/1139 ---\n",
      "  Train Indices: [600:1943], Evaluation Indices: [1944:2111]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.03, 'lambda': 1.5, 'max_depth': 3, 'n_estimators': 40, 'subsample': 1.0} (Val F1: 0.4022)\n",
      "    Best Threshold: 0.35 (Val F1: 0.5017)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.3869, Prc=0.3382, Rec=0.7797, F1=0.4718\n",
      "  Iteration 51 finished in 2.69 seconds.\n",
      "\n",
      "--- Iter 52/1139 ---\n",
      "  Train Indices: [612:1955], Evaluation Indices: [1956:2123]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.03, 'lambda': 2.5, 'max_depth': 3, 'n_estimators': 40, 'subsample': 1.0} (Val F1: 0.4292)\n",
      "    Best Threshold: 0.35 (Val F1: 0.4916)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.4048, Prc=0.3143, Rec=0.9167, F1=0.4681\n",
      "  Iteration 52 finished in 2.61 seconds.\n",
      "\n",
      "--- Iter 53/1139 ---\n",
      "  Train Indices: [624:1967], Evaluation Indices: [1968:2135]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.03, 'lambda': 2.5, 'max_depth': 3, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.4550)\n",
      "    Best Threshold: 0.50 (Val F1: 0.4550)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.6310, Prc=0.4156, Rec=0.6531, F1=0.5079\n",
      "  Iteration 53 finished in 2.77 seconds.\n",
      "\n",
      "--- Iter 54/1139 ---\n",
      "  Train Indices: [636:1979], Evaluation Indices: [1980:2147]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.05, 'lambda': 2.5, 'max_depth': 3, 'n_estimators': 40, 'subsample': 1.0} (Val F1: 0.4267)\n",
      "    Best Threshold: 0.35 (Val F1: 0.4897)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.4167, Prc=0.3185, Rec=0.8776, F1=0.4674\n",
      "  Iteration 54 finished in 2.64 seconds.\n",
      "\n",
      "--- Iter 55/1139 ---\n",
      "  Train Indices: [648:1991], Evaluation Indices: [1992:2159]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.03, 'lambda': 1.5, 'max_depth': 3, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.3315)\n",
      "    Best Threshold: 0.25 (Val F1: 0.4164)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.2917, Prc=0.2917, Rec=1.0000, F1=0.4516\n",
      "  Iteration 55 finished in 2.56 seconds.\n",
      "\n",
      "--- Iter 56/1139 ---\n",
      "  Train Indices: [660:2003], Evaluation Indices: [2004:2171]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.03, 'lambda': 2.5, 'max_depth': 2, 'n_estimators': 60, 'subsample': 0.8} (Val F1: 0.4144)\n",
      "    Best Threshold: 0.35 (Val F1: 0.4851)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.4702, Prc=0.3280, Rec=0.8913, F1=0.4795\n",
      "  Iteration 56 finished in 2.63 seconds.\n",
      "\n",
      "--- Iter 57/1139 ---\n",
      "  Train Indices: [672:2015], Evaluation Indices: [2016:2183]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.03, 'lambda': 2.5, 'max_depth': 3, 'n_estimators': 40, 'subsample': 1.0} (Val F1: 0.4259)\n",
      "    Best Threshold: 0.35 (Val F1: 0.4422)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.3274, Prc=0.2653, Rec=0.8864, F1=0.4084\n",
      "  Iteration 57 finished in 2.56 seconds.\n",
      "\n",
      "--- Iter 58/1139 ---\n",
      "  Train Indices: [684:2027], Evaluation Indices: [2028:2195]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.05, 'lambda': 1.5, 'max_depth': 3, 'n_estimators': 60, 'subsample': 1.0} (Val F1: 0.4828)\n",
      "    Best Threshold: 0.55 (Val F1: 0.4845)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.6548, Prc=0.4091, Rec=0.5870, F1=0.4821\n",
      "  Iteration 58 finished in 2.80 seconds.\n",
      "\n",
      "--- Iter 59/1139 ---\n",
      "  Train Indices: [696:2039], Evaluation Indices: [2040:2207]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.03, 'lambda': 1.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.4184)\n",
      "    Best Threshold: 0.30 (Val F1: 0.4877)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.3155, Prc=0.2710, Rec=0.9545, F1=0.4221\n",
      "  Iteration 59 finished in 2.69 seconds.\n",
      "\n",
      "--- Iter 60/1139 ---\n",
      "  Train Indices: [708:2051], Evaluation Indices: [2052:2219]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.05, 'lambda': 2.5, 'max_depth': 3, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.4554)\n",
      "    Best Threshold: 0.35 (Val F1: 0.5018)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.4881, Prc=0.2991, Rec=0.7442, F1=0.4267\n",
      "  Iteration 60 finished in 2.60 seconds.\n",
      "\n",
      "--- Iter 61/1139 ---\n",
      "  Train Indices: [720:2063], Evaluation Indices: [2064:2231]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.03, 'lambda': 2.5, 'max_depth': 2, 'n_estimators': 60, 'subsample': 0.8} (Val F1: 0.5323)\n",
      "    Best Threshold: 0.50 (Val F1: 0.5323)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.5893, Prc=0.2714, Rec=0.5135, F1=0.3551\n",
      "  Iteration 61 finished in 2.60 seconds.\n",
      "\n",
      "--- Iter 62/1139 ---\n",
      "  Train Indices: [732:2075], Evaluation Indices: [2076:2243]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.03, 'lambda': 1.5, 'max_depth': 3, 'n_estimators': 60, 'subsample': 0.8} (Val F1: 0.5188)\n",
      "    Best Threshold: 0.45 (Val F1: 0.5252)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.5774, Prc=0.2895, Rec=0.5641, F1=0.3826\n",
      "  Iteration 62 finished in 2.57 seconds.\n",
      "\n",
      "--- Iter 63/1139 ---\n",
      "  Train Indices: [744:2087], Evaluation Indices: [2088:2255]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.03, 'lambda': 1.5, 'max_depth': 3, 'n_estimators': 60, 'subsample': 1.0} (Val F1: 0.5263)\n",
      "    Best Threshold: 0.50 (Val F1: 0.5263)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.6845, Prc=0.3654, Rec=0.4872, F1=0.4176\n",
      "  Iteration 63 finished in 2.61 seconds.\n",
      "\n",
      "--- Iter 64/1139 ---\n",
      "  Train Indices: [756:2099], Evaluation Indices: [2100:2267]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.03, 'lambda': 1.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 1.0} (Val F1: 0.4803)\n",
      "    Best Threshold: 0.35 (Val F1: 0.5146)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.5060, Prc=0.2551, Rec=0.7143, F1=0.3759\n",
      "  Iteration 64 finished in 2.51 seconds.\n",
      "\n",
      "--- Iter 65/1139 ---\n",
      "  Train Indices: [768:2111], Evaluation Indices: [2112:2279]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.05, 'lambda': 1.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.5130)\n",
      "    Best Threshold: 0.50 (Val F1: 0.5130)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.6964, Prc=0.3818, Rec=0.5526, F1=0.4516\n",
      "  Iteration 65 finished in 2.46 seconds.\n",
      "\n",
      "--- Iter 66/1139 ---\n",
      "  Train Indices: [780:2123], Evaluation Indices: [2124:2291]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.05, 'lambda': 1.5, 'max_depth': 3, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.4576)\n",
      "    Best Threshold: 0.35 (Val F1: 0.5294)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.5060, Prc=0.3451, Rec=0.8125, F1=0.4845\n",
      "  Iteration 66 finished in 2.64 seconds.\n",
      "\n",
      "--- Iter 67/1139 ---\n",
      "  Train Indices: [792:2135], Evaluation Indices: [2136:2303]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.05, 'lambda': 1.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.5062)\n",
      "    Best Threshold: 0.40 (Val F1: 0.5104)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.6250, Prc=0.4471, Rec=0.7037, F1=0.5468\n",
      "  Iteration 67 finished in 2.46 seconds.\n",
      "\n",
      "--- Iter 68/1139 ---\n",
      "  Train Indices: [804:2147], Evaluation Indices: [2148:2315]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.05, 'lambda': 2.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.5036)\n",
      "    Best Threshold: 0.35 (Val F1: 0.5248)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.5833, Prc=0.4231, Rec=0.8148, F1=0.5570\n",
      "  Iteration 68 finished in 2.50 seconds.\n",
      "\n",
      "--- Iter 69/1139 ---\n",
      "  Train Indices: [816:2159], Evaluation Indices: [2160:2327]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.03, 'lambda': 1.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 1.0} (Val F1: 0.5054)\n",
      "    Best Threshold: 0.40 (Val F1: 0.5150)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.6310, Prc=0.4835, Rec=0.7458, F1=0.5867\n",
      "  Iteration 69 finished in 2.50 seconds.\n",
      "\n",
      "--- Iter 70/1139 ---\n",
      "  Train Indices: [828:2171], Evaluation Indices: [2172:2339]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.05, 'lambda': 2.5, 'max_depth': 3, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.5394)\n",
      "    Best Threshold: 0.50 (Val F1: 0.5394)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.5655, Prc=0.4098, Rec=0.4032, F1=0.4065\n",
      "  Iteration 70 finished in 2.57 seconds.\n",
      "\n",
      "--- Iter 71/1139 ---\n",
      "  Train Indices: [840:2183], Evaluation Indices: [2184:2351]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.05, 'lambda': 1.5, 'max_depth': 3, 'n_estimators': 40, 'subsample': 1.0} (Val F1: 0.5551)\n",
      "    Best Threshold: 0.50 (Val F1: 0.5551)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.5298, Prc=0.3600, Rec=0.4655, F1=0.4060\n",
      "  Iteration 71 finished in 2.58 seconds.\n",
      "\n",
      "--- Iter 72/1139 ---\n",
      "  Train Indices: [852:2195], Evaluation Indices: [2196:2363]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.03, 'lambda': 1.5, 'max_depth': 3, 'n_estimators': 40, 'subsample': 1.0} (Val F1: 0.5294)\n",
      "    Best Threshold: 0.50 (Val F1: 0.5294)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.5298, Prc=0.2745, Rec=0.2500, F1=0.2617\n",
      "  Iteration 72 finished in 2.52 seconds.\n",
      "\n",
      "--- Iter 73/1139 ---\n",
      "  Train Indices: [864:2207], Evaluation Indices: [2208:2375]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.03, 'lambda': 2.5, 'max_depth': 3, 'n_estimators': 40, 'subsample': 1.0} (Val F1: 0.5043)\n",
      "    Best Threshold: 0.50 (Val F1: 0.5043)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.5179, Prc=0.3333, Rec=0.2462, F1=0.2832\n",
      "  Iteration 73 finished in 2.40 seconds.\n",
      "\n",
      "--- Iter 74/1139 ---\n",
      "  Train Indices: [876:2219], Evaluation Indices: [2220:2387]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.05, 'lambda': 1.5, 'max_depth': 3, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.4751)\n",
      "    Best Threshold: 0.50 (Val F1: 0.4751)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.5000, Prc=0.3846, Rec=0.3623, F1=0.3731\n",
      "  Iteration 74 finished in 2.37 seconds.\n",
      "\n",
      "--- Iter 75/1139 ---\n",
      "  Train Indices: [888:2231], Evaluation Indices: [2232:2399]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.03, 'lambda': 1.5, 'max_depth': 3, 'n_estimators': 40, 'subsample': 1.0} (Val F1: 0.4956)\n",
      "    Best Threshold: 0.40 (Val F1: 0.5256)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.5060, Prc=0.4444, Rec=0.7429, F1=0.5561\n",
      "  Iteration 75 finished in 2.60 seconds.\n",
      "\n",
      "--- Iter 76/1139 ---\n",
      "  Train Indices: [900:2243], Evaluation Indices: [2244:2411]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.05, 'lambda': 1.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.5702)\n",
      "    Best Threshold: 0.50 (Val F1: 0.5702)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.5060, Prc=0.4179, Rec=0.3889, F1=0.4029\n",
      "  Iteration 76 finished in 2.71 seconds.\n",
      "\n",
      "--- Iter 77/1139 ---\n",
      "  Train Indices: [912:2255], Evaluation Indices: [2256:2423]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.05, 'lambda': 1.5, 'max_depth': 2, 'n_estimators': 60, 'subsample': 0.8} (Val F1: 0.5679)\n",
      "    Best Threshold: 0.50 (Val F1: 0.5679)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.4643, Prc=0.4328, Rec=0.3580, F1=0.3919\n",
      "  Iteration 77 finished in 2.83 seconds.\n",
      "\n",
      "--- Iter 78/1139 ---\n",
      "  Train Indices: [924:2267], Evaluation Indices: [2268:2435]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.05, 'lambda': 2.5, 'max_depth': 3, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.5636)\n",
      "    Best Threshold: 0.50 (Val F1: 0.5636)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.4524, Prc=0.4394, Rec=0.3452, F1=0.3867\n",
      "  Iteration 78 finished in 2.68 seconds.\n",
      "\n",
      "--- Iter 79/1139 ---\n",
      "  Train Indices: [936:2279], Evaluation Indices: [2280:2447]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.03, 'lambda': 1.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.5417)\n",
      "    Best Threshold: 0.45 (Val F1: 0.5506)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.4345, Prc=0.4074, Rec=0.4125, F1=0.4099\n",
      "  Iteration 79 finished in 2.73 seconds.\n",
      "\n",
      "--- Iter 80/1139 ---\n",
      "  Train Indices: [948:2291], Evaluation Indices: [2292:2459]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.03, 'lambda': 1.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.5817)\n",
      "    Best Threshold: 0.50 (Val F1: 0.5817)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.4464, Prc=0.4235, Rec=0.4500, F1=0.4364\n",
      "  Iteration 80 finished in 2.62 seconds.\n",
      "\n",
      "--- Iter 81/1139 ---\n",
      "  Train Indices: [960:2303], Evaluation Indices: [2304:2471]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.05, 'lambda': 1.5, 'max_depth': 3, 'n_estimators': 60, 'subsample': 1.0} (Val F1: 0.5984)\n",
      "    Best Threshold: 0.50 (Val F1: 0.5984)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.4524, Prc=0.3750, Rec=0.2692, F1=0.3134\n",
      "  Iteration 81 finished in 2.78 seconds.\n",
      "\n",
      "--- Iter 82/1139 ---\n",
      "  Train Indices: [972:2315], Evaluation Indices: [2316:2483]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.05, 'lambda': 2.5, 'max_depth': 2, 'n_estimators': 60, 'subsample': 1.0} (Val F1: 0.6065)\n",
      "    Best Threshold: 0.50 (Val F1: 0.6065)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.4524, Prc=0.3600, Rec=0.1059, F1=0.1636\n",
      "  Iteration 82 finished in 2.77 seconds.\n",
      "\n",
      "--- Iter 83/1139 ---\n",
      "  Train Indices: [984:2327], Evaluation Indices: [2328:2495]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.03, 'lambda': 1.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.5862)\n",
      "    Best Threshold: 0.40 (Val F1: 0.5938)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.4881, Prc=0.4894, Rec=0.2706, F1=0.3485\n",
      "  Iteration 83 finished in 2.79 seconds.\n",
      "\n",
      "--- Iter 84/1139 ---\n",
      "  Train Indices: [996:2339], Evaluation Indices: [2340:2507]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.05, 'lambda': 1.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.5809)\n",
      "    Best Threshold: 0.45 (Val F1: 0.5933)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.4881, Prc=0.5179, Rec=0.3295, F1=0.4028\n",
      "  Iteration 84 finished in 2.83 seconds.\n",
      "\n",
      "--- Iter 85/1139 ---\n",
      "  Train Indices: [1008:2351], Evaluation Indices: [2352:2519]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.03, 'lambda': 1.5, 'max_depth': 3, 'n_estimators': 40, 'subsample': 1.0} (Val F1: 0.5964)\n",
      "    Best Threshold: 0.50 (Val F1: 0.5964)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.4345, Prc=0.6667, Rec=0.0412, F1=0.0777\n",
      "  Iteration 85 finished in 2.87 seconds.\n",
      "\n",
      "--- Iter 86/1139 ---\n",
      "  Train Indices: [1020:2363], Evaluation Indices: [2364:2531]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.03, 'lambda': 2.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.5548)\n",
      "    Best Threshold: 0.50 (Val F1: 0.5548)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.5357, Prc=0.7368, Rec=0.2917, F1=0.4179\n",
      "  Iteration 86 finished in 2.76 seconds.\n",
      "\n",
      "--- Iter 87/1139 ---\n",
      "  Train Indices: [1032:2375], Evaluation Indices: [2376:2543]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.05, 'lambda': 1.5, 'max_depth': 3, 'n_estimators': 60, 'subsample': 0.8} (Val F1: 0.5646)\n",
      "    Best Threshold: 0.50 (Val F1: 0.5646)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.5238, Prc=0.5500, Rec=0.3837, F1=0.4521\n",
      "  Iteration 87 finished in 2.84 seconds.\n",
      "\n",
      "--- Iter 88/1139 ---\n",
      "  Train Indices: [1044:2387], Evaluation Indices: [2388:2555]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.03, 'lambda': 1.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 1.0} (Val F1: 0.5679)\n",
      "    Best Threshold: 0.40 (Val F1: 0.5770)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.5179, Prc=0.5000, Rec=0.7654, F1=0.6049\n",
      "  Iteration 88 finished in 2.90 seconds.\n",
      "\n",
      "--- Iter 89/1139 ---\n",
      "  Train Indices: [1056:2399], Evaluation Indices: [2400:2567]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.05, 'lambda': 1.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.5474)\n",
      "    Best Threshold: 0.40 (Val F1: 0.5619)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.5655, Prc=0.5234, Rec=0.7179, F1=0.6054\n",
      "  Iteration 89 finished in 2.85 seconds.\n",
      "\n",
      "--- Iter 90/1139 ---\n",
      "  Train Indices: [1068:2411], Evaluation Indices: [2412:2579]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.03, 'lambda': 1.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 1.0} (Val F1: 0.5928)\n",
      "    Best Threshold: 0.50 (Val F1: 0.5928)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.5357, Prc=0.5155, Rec=0.6173, F1=0.5618\n",
      "  Iteration 90 finished in 2.97 seconds.\n",
      "\n",
      "--- Iter 91/1139 ---\n",
      "  Train Indices: [1080:2423], Evaluation Indices: [2424:2591]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.03, 'lambda': 1.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 1.0} (Val F1: 0.5861)\n",
      "    Best Threshold: 0.45 (Val F1: 0.6000)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.5179, Prc=0.4815, Rec=0.8553, F1=0.6161\n",
      "  Iteration 91 finished in 2.79 seconds.\n",
      "\n",
      "--- Iter 92/1139 ---\n",
      "  Train Indices: [1092:2435], Evaluation Indices: [2436:2603]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.03, 'lambda': 2.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 1.0} (Val F1: 0.5785)\n",
      "    Best Threshold: 0.35 (Val F1: 0.5815)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.4524, Prc=0.4491, Rec=1.0000, F1=0.6198\n",
      "  Iteration 92 finished in 2.92 seconds.\n",
      "\n",
      "--- Iter 93/1139 ---\n",
      "  Train Indices: [1104:2447], Evaluation Indices: [2448:2615]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.03, 'lambda': 2.5, 'max_depth': 3, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.5279)\n",
      "    Best Threshold: 0.35 (Val F1: 0.5650)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.5238, Prc=0.5157, Rec=0.9647, F1=0.6721\n",
      "  Iteration 93 finished in 2.96 seconds.\n",
      "\n",
      "--- Iter 94/1139 ---\n",
      "  Train Indices: [1116:2459], Evaluation Indices: [2460:2627]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.03, 'lambda': 1.5, 'max_depth': 3, 'n_estimators': 60, 'subsample': 1.0} (Val F1: 0.5836)\n",
      "    Best Threshold: 0.40 (Val F1: 0.6170)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.4881, Prc=0.4713, Rec=0.9610, F1=0.6325\n",
      "  Iteration 94 finished in 2.96 seconds.\n",
      "\n",
      "--- Iter 95/1139 ---\n",
      "  Train Indices: [1128:2471], Evaluation Indices: [2472:2639]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.03, 'lambda': 1.5, 'max_depth': 2, 'n_estimators': 60, 'subsample': 0.8} (Val F1: 0.5825)\n",
      "    Best Threshold: 0.35 (Val F1: 0.6241)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.4583, Prc=0.4465, Rec=0.9595, F1=0.6094\n",
      "  Iteration 95 finished in 3.00 seconds.\n",
      "\n",
      "--- Iter 96/1139 ---\n",
      "  Train Indices: [1140:2483], Evaluation Indices: [2484:2651]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.03, 'lambda': 2.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 1.0} (Val F1: 0.6628)\n",
      "    Best Threshold: 0.50 (Val F1: 0.6628)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.4286, Prc=0.4088, Rec=0.9701, F1=0.5752\n",
      "  Iteration 96 finished in 2.93 seconds.\n",
      "\n",
      "--- Iter 97/1139 ---\n",
      "  Train Indices: [1152:2495], Evaluation Indices: [2496:2663]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.03, 'lambda': 2.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.6012)\n",
      "    Best Threshold: 0.40 (Val F1: 0.6553)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.4226, Prc=0.4099, Rec=0.9706, F1=0.5764\n",
      "  Iteration 97 finished in 2.90 seconds.\n",
      "\n",
      "--- Iter 98/1139 ---\n",
      "  Train Indices: [1164:2507], Evaluation Indices: [2508:2675]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.05, 'lambda': 1.5, 'max_depth': 2, 'n_estimators': 60, 'subsample': 1.0} (Val F1: 0.6409)\n",
      "    Best Threshold: 0.30 (Val F1: 0.6712)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.3810, Prc=0.3704, Rec=0.9677, F1=0.5357\n",
      "  Iteration 98 finished in 2.91 seconds.\n",
      "\n",
      "--- Iter 99/1139 ---\n",
      "  Train Indices: [1176:2519], Evaluation Indices: [2520:2687]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.03, 'lambda': 2.5, 'max_depth': 3, 'n_estimators': 40, 'subsample': 1.0} (Val F1: 0.6831)\n",
      "    Best Threshold: 0.45 (Val F1: 0.6833)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.3929, Prc=0.3537, Rec=0.8814, F1=0.5049\n",
      "  Iteration 99 finished in 2.98 seconds.\n",
      "\n",
      "--- Iter 100/1139 ---\n",
      "  Train Indices: [1188:2531], Evaluation Indices: [2532:2699]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.03, 'lambda': 2.5, 'max_depth': 2, 'n_estimators': 60, 'subsample': 0.8} (Val F1: 0.6770)\n",
      "    Best Threshold: 0.40 (Val F1: 0.6838)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.3810, Prc=0.3642, Rec=0.8730, F1=0.5140\n",
      "  Iteration 100 finished in 2.98 seconds.\n",
      "\n",
      "--- Iter 101/1139 ---\n",
      "  Train Indices: [1200:2543], Evaluation Indices: [2544:2711]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.03, 'lambda': 1.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 1.0} (Val F1: 0.6521)\n",
      "    Best Threshold: 0.40 (Val F1: 0.6636)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.3929, Prc=0.3889, Rec=0.9545, F1=0.5526\n",
      "  Iteration 101 finished in 2.81 seconds.\n",
      "\n",
      "--- Iter 102/1139 ---\n",
      "  Train Indices: [1212:2555], Evaluation Indices: [2556:2723]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.05, 'lambda': 2.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.6686)\n",
      "    Best Threshold: 0.45 (Val F1: 0.6853)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.3988, Prc=0.3650, Rec=0.7812, F1=0.4975\n",
      "  Iteration 102 finished in 2.83 seconds.\n",
      "\n",
      "--- Iter 103/1139 ---\n",
      "  Train Indices: [1224:2567], Evaluation Indices: [2568:2735]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.03, 'lambda': 2.5, 'max_depth': 2, 'n_estimators': 60, 'subsample': 0.8} (Val F1: 0.6316)\n",
      "    Best Threshold: 0.35 (Val F1: 0.6560)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.4167, Prc=0.4076, Rec=0.9275, F1=0.5664\n",
      "  Iteration 103 finished in 2.93 seconds.\n",
      "\n",
      "--- Iter 104/1139 ---\n",
      "  Train Indices: [1236:2579], Evaluation Indices: [2580:2747]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.05, 'lambda': 2.5, 'max_depth': 3, 'n_estimators': 60, 'subsample': 0.8} (Val F1: 0.6371)\n",
      "    Best Threshold: 0.35 (Val F1: 0.6560)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.3929, Prc=0.3669, Rec=0.7846, F1=0.5000\n",
      "  Iteration 104 finished in 3.01 seconds.\n",
      "\n",
      "--- Iter 105/1139 ---\n",
      "  Train Indices: [1248:2591], Evaluation Indices: [2592:2759]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.03, 'lambda': 2.5, 'max_depth': 3, 'n_estimators': 60, 'subsample': 0.8} (Val F1: 0.6390)\n",
      "    Best Threshold: 0.35 (Val F1: 0.6609)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.4464, Prc=0.3852, Rec=0.8387, F1=0.5279\n",
      "  Iteration 105 finished in 2.80 seconds.\n",
      "\n",
      "--- Iter 106/1139 ---\n",
      "  Train Indices: [1260:2603], Evaluation Indices: [2604:2771]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.03, 'lambda': 2.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.6484)\n",
      "    Best Threshold: 0.40 (Val F1: 0.6552)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.4405, Prc=0.3934, Rec=0.7059, F1=0.5053\n",
      "  Iteration 106 finished in 2.93 seconds.\n",
      "\n",
      "--- Iter 107/1139 ---\n",
      "  Train Indices: [1272:2615], Evaluation Indices: [2616:2783]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.03, 'lambda': 1.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 1.0} (Val F1: 0.6283)\n",
      "    Best Threshold: 0.45 (Val F1: 0.6638)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.3988, Prc=0.2941, Rec=0.5085, F1=0.3727\n",
      "  Iteration 107 finished in 2.60 seconds.\n",
      "\n",
      "--- Iter 108/1139 ---\n",
      "  Train Indices: [1284:2627], Evaluation Indices: [2628:2795]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.05, 'lambda': 2.5, 'max_depth': 3, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.6632)\n",
      "    Best Threshold: 0.50 (Val F1: 0.6632)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.4583, Prc=0.3000, Rec=0.4068, F1=0.3453\n",
      "  Iteration 108 finished in 2.79 seconds.\n",
      "\n",
      "--- Iter 109/1139 ---\n",
      "  Train Indices: [1296:2639], Evaluation Indices: [2640:2807]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.05, 'lambda': 2.5, 'max_depth': 3, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.5968)\n",
      "    Best Threshold: 0.45 (Val F1: 0.6237)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.4643, Prc=0.2778, Rec=0.3448, F1=0.3077\n",
      "  Iteration 109 finished in 2.61 seconds.\n",
      "\n",
      "--- Iter 110/1139 ---\n",
      "  Train Indices: [1308:2651], Evaluation Indices: [2652:2819]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.05, 'lambda': 1.5, 'max_depth': 3, 'n_estimators': 40, 'subsample': 1.0} (Val F1: 0.6229)\n",
      "    Best Threshold: 0.35 (Val F1: 0.6261)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.4345, Prc=0.3273, Rec=0.6316, F1=0.4311\n",
      "  Iteration 110 finished in 2.49 seconds.\n",
      "\n",
      "--- Iter 111/1139 ---\n",
      "  Train Indices: [1320:2663], Evaluation Indices: [2664:2831]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.05, 'lambda': 2.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.6436)\n",
      "    Best Threshold: 0.50 (Val F1: 0.6436)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.5238, Prc=0.2857, Rec=0.2857, F1=0.2857\n",
      "  Iteration 111 finished in 2.77 seconds.\n",
      "\n",
      "--- Iter 112/1139 ---\n",
      "  Train Indices: [1332:2675], Evaluation Indices: [2676:2843]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.03, 'lambda': 2.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 1.0} (Val F1: 0.5975)\n",
      "    Best Threshold: 0.35 (Val F1: 0.6186)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.3750, Prc=0.3237, Rec=0.8036, F1=0.4615\n",
      "  Iteration 112 finished in 2.47 seconds.\n",
      "\n",
      "--- Iter 113/1139 ---\n",
      "  Train Indices: [1344:2687], Evaluation Indices: [2688:2855]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.03, 'lambda': 2.5, 'max_depth': 3, 'n_estimators': 40, 'subsample': 1.0} (Val F1: 0.6236)\n",
      "    Best Threshold: 0.40 (Val F1: 0.6381)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.4583, Prc=0.3196, Rec=0.5536, F1=0.4052\n",
      "  Iteration 113 finished in 2.68 seconds.\n",
      "\n",
      "--- Iter 114/1139 ---\n",
      "  Train Indices: [1356:2699], Evaluation Indices: [2700:2867]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.05, 'lambda': 2.5, 'max_depth': 3, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.6147)\n",
      "    Best Threshold: 0.40 (Val F1: 0.6488)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.4821, Prc=0.3879, Rec=0.7377, F1=0.5085\n",
      "  Iteration 114 finished in 2.68 seconds.\n",
      "\n",
      "--- Iter 115/1139 ---\n",
      "  Train Indices: [1368:2711], Evaluation Indices: [2712:2879]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.03, 'lambda': 1.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 1.0} (Val F1: 0.6351)\n",
      "    Best Threshold: 0.50 (Val F1: 0.6351)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.6190, Prc=0.4923, Rec=0.5079, F1=0.5000\n",
      "  Iteration 115 finished in 2.55 seconds.\n",
      "\n",
      "--- Iter 116/1139 ---\n",
      "  Train Indices: [1380:2723], Evaluation Indices: [2724:2891]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.05, 'lambda': 1.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.6238)\n",
      "    Best Threshold: 0.50 (Val F1: 0.6238)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.5952, Prc=0.5000, Rec=0.4559, F1=0.4769\n",
      "  Iteration 116 finished in 2.64 seconds.\n",
      "\n",
      "--- Iter 117/1139 ---\n",
      "  Train Indices: [1392:2735], Evaluation Indices: [2736:2903]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.03, 'lambda': 1.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 1.0} (Val F1: 0.6178)\n",
      "    Best Threshold: 0.50 (Val F1: 0.6178)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.6012, Prc=0.5325, Rec=0.5694, F1=0.5503\n",
      "  Iteration 117 finished in 2.47 seconds.\n",
      "\n",
      "--- Iter 118/1139 ---\n",
      "  Train Indices: [1404:2747], Evaluation Indices: [2748:2915]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.03, 'lambda': 2.5, 'max_depth': 3, 'n_estimators': 40, 'subsample': 1.0} (Val F1: 0.6022)\n",
      "    Best Threshold: 0.45 (Val F1: 0.6158)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.5536, Prc=0.4632, Rec=0.6471, F1=0.5399\n",
      "  Iteration 118 finished in 2.34 seconds.\n",
      "\n",
      "--- Iter 119/1139 ---\n",
      "  Train Indices: [1416:2759], Evaluation Indices: [2760:2927]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.05, 'lambda': 2.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.5885)\n",
      "    Best Threshold: 0.40 (Val F1: 0.5940)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.5119, Prc=0.4495, Rec=0.6901, F1=0.5444\n",
      "  Iteration 119 finished in 2.42 seconds.\n",
      "\n",
      "--- Iter 120/1139 ---\n",
      "  Train Indices: [1428:2771], Evaluation Indices: [2772:2939]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.05, 'lambda': 2.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.5914)\n",
      "    Best Threshold: 0.40 (Val F1: 0.6189)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.4762, Prc=0.4037, Rec=0.6567, F1=0.5000\n",
      "  Iteration 120 finished in 2.46 seconds.\n",
      "\n",
      "--- Iter 121/1139 ---\n",
      "  Train Indices: [1440:2783], Evaluation Indices: [2784:2951]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.05, 'lambda': 1.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.6079)\n",
      "    Best Threshold: 0.40 (Val F1: 0.6105)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.5119, Prc=0.4340, Rec=0.6765, F1=0.5287\n",
      "  Iteration 121 finished in 2.72 seconds.\n",
      "\n",
      "--- Iter 122/1139 ---\n",
      "  Train Indices: [1452:2795], Evaluation Indices: [2796:2963]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.03, 'lambda': 1.5, 'max_depth': 3, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.6018)\n",
      "    Best Threshold: 0.50 (Val F1: 0.6018)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.5833, Prc=0.4783, Rec=0.4925, F1=0.4853\n",
      "  Iteration 122 finished in 2.74 seconds.\n",
      "\n",
      "--- Iter 123/1139 ---\n",
      "  Train Indices: [1464:2807], Evaluation Indices: [2808:2975]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.05, 'lambda': 1.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.5418)\n",
      "    Best Threshold: 0.45 (Val F1: 0.5827)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.5298, Prc=0.3896, Rec=0.4839, F1=0.4317\n",
      "  Iteration 123 finished in 2.65 seconds.\n",
      "\n",
      "--- Iter 124/1139 ---\n",
      "  Train Indices: [1476:2819], Evaluation Indices: [2820:2987]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.03, 'lambda': 1.5, 'max_depth': 3, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.5566)\n",
      "    Best Threshold: 0.50 (Val F1: 0.5566)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.5000, Prc=0.3684, Rec=0.4375, F1=0.4000\n",
      "  Iteration 124 finished in 2.52 seconds.\n",
      "\n",
      "--- Iter 125/1139 ---\n",
      "  Train Indices: [1488:2831], Evaluation Indices: [2832:2999]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.05, 'lambda': 2.5, 'max_depth': 3, 'n_estimators': 40, 'subsample': 1.0} (Val F1: 0.4984)\n",
      "    Best Threshold: 0.40 (Val F1: 0.5519)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.4048, Prc=0.3386, Rec=0.7288, F1=0.4624\n",
      "  Iteration 125 finished in 2.54 seconds.\n",
      "\n",
      "--- Iter 126/1139 ---\n",
      "  Train Indices: [1500:2843], Evaluation Indices: [2844:3011]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.03, 'lambda': 1.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.4954)\n",
      "    Best Threshold: 0.45 (Val F1: 0.5508)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.4286, Prc=0.3667, Rec=0.6875, F1=0.4783\n",
      "  Iteration 126 finished in 2.30 seconds.\n",
      "\n",
      "--- Iter 127/1139 ---\n",
      "  Train Indices: [1512:2855], Evaluation Indices: [2856:3023]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.03, 'lambda': 2.5, 'max_depth': 3, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.4531)\n",
      "    Best Threshold: 0.35 (Val F1: 0.5134)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.4643, Prc=0.4286, Rec=0.8571, F1=0.5714\n",
      "  Iteration 127 finished in 2.37 seconds.\n",
      "\n",
      "--- Iter 128/1139 ---\n",
      "  Train Indices: [1524:2867], Evaluation Indices: [2868:3035]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.03, 'lambda': 1.5, 'max_depth': 2, 'n_estimators': 60, 'subsample': 0.8} (Val F1: 0.5646)\n",
      "    Best Threshold: 0.50 (Val F1: 0.5646)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.4821, Prc=0.3725, Rec=0.6230, F1=0.4663\n",
      "  Iteration 128 finished in 2.65 seconds.\n",
      "\n",
      "--- Iter 129/1139 ---\n",
      "  Train Indices: [1536:2879], Evaluation Indices: [2880:3047]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.03, 'lambda': 1.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.5183)\n",
      "    Best Threshold: 0.40 (Val F1: 0.5575)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.4048, Prc=0.3167, Rec=0.6786, F1=0.4318\n",
      "  Iteration 129 finished in 2.40 seconds.\n",
      "\n",
      "--- Iter 130/1139 ---\n",
      "  Train Indices: [1548:2891], Evaluation Indices: [2892:3059]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.03, 'lambda': 2.5, 'max_depth': 3, 'n_estimators': 40, 'subsample': 1.0} (Val F1: 0.5389)\n",
      "    Best Threshold: 0.45 (Val F1: 0.5729)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.4702, Prc=0.3559, Rec=0.7636, F1=0.4855\n",
      "  Iteration 130 finished in 2.41 seconds.\n",
      "\n",
      "--- Iter 131/1139 ---\n",
      "  Train Indices: [1560:2903], Evaluation Indices: [2904:3071]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.03, 'lambda': 2.5, 'max_depth': 3, 'n_estimators': 40, 'subsample': 1.0} (Val F1: 0.5399)\n",
      "    Best Threshold: 0.40 (Val F1: 0.5964)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.4286, Prc=0.3258, Rec=0.8600, F1=0.4725\n",
      "  Iteration 131 finished in 2.52 seconds.\n",
      "\n",
      "--- Iter 132/1139 ---\n",
      "  Train Indices: [1572:2915], Evaluation Indices: [2916:3083]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.03, 'lambda': 1.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 1.0} (Val F1: 0.5192)\n",
      "    Best Threshold: 0.40 (Val F1: 0.5812)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.4405, Prc=0.3359, Rec=0.8269, F1=0.4778\n",
      "  Iteration 132 finished in 2.43 seconds.\n",
      "\n",
      "--- Iter 133/1139 ---\n",
      "  Train Indices: [1584:2927], Evaluation Indices: [2928:3095]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 0.9, 'eta': 0.03, 'lambda': 1.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 1.0} (Val F1: 0.5015)\n",
      "    Best Threshold: 0.45 (Val F1: 0.5714)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.4702, Prc=0.3565, Rec=0.7321, F1=0.4795\n",
      "  Iteration 133 finished in 2.33 seconds.\n",
      "\n",
      "--- Iter 134/1139 ---\n",
      "  Train Indices: [1596:2939], Evaluation Indices: [2940:3107]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Best Meta Params: {'colsample_bytree': 1.0, 'eta': 0.03, 'lambda': 1.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 0.8} (Val F1: 0.5141)\n",
      "    Best Threshold: 0.35 (Val F1: 0.5779)\n",
      "  Level 1: Training final Meta-Learner...\n",
      "  Level 1 Final Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.3988, Prc=0.3404, Rec=0.8571, F1=0.4873\n",
      "  Iteration 134 finished in 2.66 seconds.\n",
      "\n",
      "--- Iter 135/1139 ---\n",
      "  Train Indices: [1608:2951], Evaluation Indices: [2952:3119]\n",
      "  Level 0: Generating OOF predictions using 5-Fold CV...\n",
      "  Level 0: Training base models on full training data (1344 rows)...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 609\u001b[0m\n\u001b[0;32m    607\u001b[0m      \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxgb\u001b[39m\u001b[38;5;124m'\u001b[39m: full_fit_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    608\u001b[0m      \u001b[38;5;66;03m# LGBM uses verbose=-1 from init, SVM pipeline is simple\u001b[39;00m\n\u001b[1;32m--> 609\u001b[0m      \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_full\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_full\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfull_fit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    610\u001b[0m      models_full[name] \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m    611\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e_full_fit:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\pipeline.py:427\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    425\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    426\u001b[0m         fit_params_last_step \u001b[38;5;241m=\u001b[39m fit_params_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m--> 427\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_last_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\svm\\_base.py:250\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LibSVM]\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    249\u001b[0m seed \u001b[38;5;241m=\u001b[39m rnd\u001b[38;5;241m.\u001b[39mrandint(np\u001b[38;5;241m.\u001b[39miinfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmax)\n\u001b[1;32m--> 250\u001b[0m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# see comment on the other call to np.iinfo in this file\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape_fit_ \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m (n_samples,)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\svm\\_base.py:329\u001b[0m, in \u001b[0;36mBaseLibSVM._dense_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    315\u001b[0m libsvm\u001b[38;5;241m.\u001b[39mset_verbosity_wrap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[0;32m    317\u001b[0m \u001b[38;5;66;03m# we don't pass **self.get_params() to allow subclasses to\u001b[39;00m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;66;03m# add other parameters to __init__\u001b[39;00m\n\u001b[0;32m    319\u001b[0m (\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_,\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_vectors_,\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_support,\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdual_coef_,\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_,\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probA,\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probB,\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_status_,\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_iter,\n\u001b[1;32m--> 329\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mlibsvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43msvm_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# TODO(1.4): Replace \"_class_weight\" with \"class_weight_\"\u001b[39;49;00m\n\u001b[0;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_class_weight\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mC\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprobability\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdegree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdegree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshrinking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshrinking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoef0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoef0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_from_fit_status()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Combined Script: Load CSV -> Feature Engineering -> Rolling Origin XGB Modeling\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "\n",
    "# Feature Engineering Imports\n",
    "import pandas_ta as ta  # Technical indicators\n",
    "\n",
    "# Modeling Imports\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier, early_stopping, log_evaluation # <--- IMPORT CALLBACKS\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler # Needed for SVM\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import ParameterGrid, StratifiedKFold # StratifiedKFold for stacking\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "from sklearn.pipeline import Pipeline # Optional: useful for SVM with scaling\n",
    "from sklearn.impute import SimpleImputer # Better imputation strategy for pipeline\n",
    "\n",
    "\n",
    "# --- Suppress Warnings ---\n",
    "warnings.filterwarnings('ignore', category=UndefinedMetricWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)\n",
    "warnings.filterwarnings('ignore') # General suppression\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "# Data Loading\n",
    "CSV_FILE_PATH = r'C:\\Users\\mason\\AVP\\BTCUSDrec.csv' # Use raw string for Windows paths\n",
    "SYMBOL_NAME = 'BTCUSD' # Define the symbol represented in the CSV\n",
    "\n",
    "# Feature Selection (VIF Filtered List - 40 Features)\n",
    "SELECTED_FEATURE_NAMES = [\n",
    "    'cmf_20h',                       # Base Indicator\n",
    "    'day_1',                         # Time Dummy\n",
    "    'day_2',                         # Time Dummy\n",
    "    'day_4',                         # Time Dummy\n",
    "    'day_5',                         # Time Dummy\n",
    "    'day_6',                         # Time Dummy\n",
    "    'hour_1',                        # Time Dummy\n",
    "    'hour_10',                       # Time Dummy\n",
    "    'hour_11',                       # Time Dummy\n",
    "    'hour_12',                       # Time Dummy\n",
    "    'hour_13',                       # Time Dummy\n",
    "    'hour_14',                       # Time Dummy\n",
    "    'hour_15',                       # Time Dummy (Added back, was missing from list but likely intended)\n",
    "    'hour_16',                       # Time Dummy\n",
    "    'hour_17',                       # Time Dummy\n",
    "    'hour_18',                       # Time Dummy\n",
    "    'hour_19',                       # Time Dummy\n",
    "    'hour_2',                        # Time Dummy\n",
    "    'hour_20',                       # Time Dummy\n",
    "    'hour_21',                       # Time Dummy\n",
    "    'hour_22',                       # Time Dummy\n",
    "    'hour_23',                       # Time Dummy\n",
    "    'hour_3',                        # Time Dummy\n",
    "    'hour_4',                        # Time Dummy\n",
    "    'hour_5',                        # Time Dummy\n",
    "    'hour_6',                        # Time Dummy\n",
    "    'hour_7',                        # Time Dummy (Added back, was missing from list but likely intended)\n",
    "    'hour_8',                        # Time Dummy\n",
    "    'hour_9',                        # Time Dummy\n",
    "    'lag_12h_volume_return',         # Lag\n",
    "    'lag_168h_price_return',         # Lag\n",
    "    'lag_24h_volume_return',         # Lag\n",
    "    'lag_3h_volume_return',          # Lag\n",
    "    'lag_6h_volume_return',          # Lag\n",
    "    'macd_hist',                     # Base Indicator (Difference)\n",
    "    'rolling_kurt_24h',              # Rolling Statistic\n",
    "    'rolling_skew_24h',              # Rolling Statistic\n",
    "    'rolling_std_3h_sq',             # Transformation\n",
    "    'volume_btc_x_range',            # Interaction\n",
    "    'volume_return_1h',              # Base calculation\n",
    "]\n",
    "\n",
    "\n",
    "# Modeling & Walk-Forward\n",
    "TARGET_THRESHOLD_PCT = 0.05 # Target threshold percentage variable (Adjusted based on last run)\n",
    "\n",
    "# Define separate prediction horizon for target\n",
    "PREDICTION_WINDOW_HOURS = 12 # Predict outcome 12 hours ahead\n",
    "PREDICTION_WINDOW_ROWS = PREDICTION_WINDOW_HOURS\n",
    "\n",
    "# Walk-forward params\n",
    "TRAIN_WINDOW_HOURS = int(24 * 7 * 8) # Training size\n",
    "# Evaluation window size\n",
    "TEST_WINDOW_HOURS = 24 * 14           # Evaluate performance over the next 14 days\n",
    "STEP_HOURS = 12                      # Retrain and predict daily\n",
    "\n",
    "TRAIN_WINDOW_ROWS = TRAIN_WINDOW_HOURS\n",
    "TEST_WINDOW_ROWS = TEST_WINDOW_HOURS # Evaluation window size\n",
    "STEP_ROWS = STEP_HOURS\n",
    "\n",
    "# Stacking Configuration\n",
    "N_STACKING_FOLDS = 5 # Number of folds for generating Level 0 predictions\n",
    "\n",
    "# --- Base Model Static Hyperparameters ---\n",
    "# Use params from your last successful run or defaults\n",
    "# XGBoost Base Model Params\n",
    "XGB_BASE_PARAMS = {\n",
    "    'objective': 'binary:logistic', 'eval_metric': 'logloss',\n",
    "    'eta': 0.05, 'max_depth': 3, 'n_estimators': 85,\n",
    "    'subsample': 0.8, 'colsample_bytree': 0.7, 'min_child_weight': 3,\n",
    "    'gamma': 0.1, 'lambda': 3, 'alpha': 0.3,\n",
    "    'random_state': 42, 'n_jobs': -1, 'tree_method': 'hist',\n",
    "    'use_label_encoder': False,\n",
    "}\n",
    "# LightGBM Base Model Params\n",
    "LGBM_BASE_PARAMS = {\n",
    "    'objective': 'binary', 'metric': 'logloss',\n",
    "    'learning_rate': 0.1, 'n_estimators': 105, 'max_depth': 4,\n",
    "    'num_leaves': 8,\n",
    "    'subsample': 0.8, 'colsample_bytree': 0.7, 'min_child_samples': 5,\n",
    "    'reg_alpha': 0.1, 'reg_lambda': 1.5,\n",
    "    'random_state': 42, 'n_jobs': -1, 'boosting_type': 'gbdt',\n",
    "    'verbose': -1\n",
    "}\n",
    "# SVM Base Model Params\n",
    "SVM_BASE_PARAMS = {\n",
    "    'kernel': 'rbf', # Kept RBF based on last adjustment\n",
    "    'C': 2.4,        # Kept C based on last adjustment\n",
    "    'probability': True,\n",
    "    'max_iter': 5000,\n",
    "    'random_state': 42,\n",
    "    'class_weight': 'balanced'\n",
    "}\n",
    "\n",
    "# --- Meta Learner Configuration ---\n",
    "META_LEARNER_IS_XGB = True\n",
    "META_XGB_PARAM_GRID = {\n",
    "    'max_depth': [2, 3],\n",
    "    'n_estimators': [40, 60],\n",
    "    'eta': [0.03, 0.05],\n",
    "    'lambda': [1.5, 2.5],\n",
    "    'subsample': [0.8, 1.0], # Added tuning for subsample\n",
    "    'colsample_bytree': [0.9, 1.0] # Added tuning for colsample\n",
    "}\n",
    "\n",
    "META_XGB_FIXED_PARAMS = {\n",
    "    'objective': 'binary:logistic', 'eval_metric': 'logloss',\n",
    "    #'subsample': 0.9, 'colsample_bytree': 0.9,\n",
    "    'gamma': 0.0, 'alpha': 0.0,\n",
    "    'random_state': 123, 'n_jobs': -1, 'tree_method': 'hist',\n",
    "    'use_label_encoder': False,\n",
    "    'min_child_weight': 1\n",
    "}\n",
    "\n",
    "# --- Probability Threshold Tuning Configuration ---\n",
    "THRESHOLD_SEARCH_RANGE = np.arange(0.10, 0.90, 0.05)\n",
    "META_VALIDATION_PCT = 0.25\n",
    "\n",
    "# --- Feature Engineering Function (Optimized for VIF-Filtered Features) ---\n",
    "\n",
    "# --- Feature Engineering Function (Optimized for VIF-Filtered Features - CORRECTED v2) ---\n",
    "\n",
    "def calculate_selected_features(df, symbol):\n",
    "    \"\"\"\n",
    "    Calculates only the 40 VIF-filtered features and their necessary prerequisites,\n",
    "    then drops the prerequisites before returning.\n",
    "    \"\"\"\n",
    "    # Use the globally defined SELECTED_FEATURE_NAMES\n",
    "    TARGET_FEATURES_TO_GENERATE = SELECTED_FEATURE_NAMES\n",
    "\n",
    "    print(f\"Starting calculation for {len(TARGET_FEATURES_TO_GENERATE)} VIF-filtered features...\")\n",
    "    start_time = time.time()\n",
    "    if df is None or len(df) < 3: return pd.DataFrame()\n",
    "    df = df.copy()\n",
    "    df['symbol'] = symbol\n",
    "\n",
    "    # --- Timestamp and Index ---\n",
    "    if 'timestamp' not in df.columns: return pd.DataFrame()\n",
    "    try: df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    except Exception as e: return pd.DataFrame()\n",
    "    df = df.sort_values('timestamp').dropna(subset=['timestamp'])\n",
    "    df = df.set_index('timestamp', drop=False)\n",
    "\n",
    "    # --- Volume Columns ---\n",
    "    original_vol_btc_name = 'Volume BTC'\n",
    "    original_vol_usd_name = 'Volume USD'\n",
    "    if original_vol_btc_name in df.columns: df['volume_btc'] = df[original_vol_btc_name]\n",
    "    elif 'volume_btc' in df.columns: df[original_vol_btc_name] = df['volume_btc']\n",
    "    else: df['volume_btc'] = 0; df[original_vol_btc_name] = 0\n",
    "    if original_vol_usd_name in df.columns: pass\n",
    "    elif 'volume_usd' in df.columns: df[original_vol_usd_name] = df['volume_usd']\n",
    "    else: df[original_vol_usd_name] = 0\n",
    "\n",
    "    # --- Basic Checks (OHLC) ---\n",
    "    required_ohlc = ['open', 'high', 'low', 'close']\n",
    "    if not all(col in df.columns for col in required_ohlc): return pd.DataFrame()\n",
    "    for col in required_ohlc: df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    if df[required_ohlc].isnull().any().any(): df = df.dropna(subset=required_ohlc)\n",
    "    if df.empty: return pd.DataFrame()\n",
    "\n",
    "    # --- 1. Calculate ALL Potential Prerequisites Needed for the 40 ---\n",
    "    # This is safer than trying to dynamically figure them out perfectly.\n",
    "    # Calculate a known superset covering components of the 40 features.\n",
    "    print(\"  Calculating necessary prerequisites...\")\n",
    "    min_periods_base = 2\n",
    "    prereqs_calculated = set() # Keep track of what's done\n",
    "\n",
    "    # Basic calculations\n",
    "    try:\n",
    "        df['price_return_1h_temp'] = df['close'].pct_change()\n",
    "        df['volume_return_1h'] = df['volume_btc'].pct_change() # Directly needed\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "             df['price_range_pct_temp'] = (df['high'] - df['low']) / df['low'].replace(0, np.nan)\n",
    "        prereqs_calculated.update(['price_return_1h_temp', 'volume_return_1h', 'price_range_pct_temp'])\n",
    "    except Exception as e: print(f\"Warning: Error in basic calcs: {e}\")\n",
    "\n",
    "    # Lags\n",
    "    try:\n",
    "        for hours in [3, 6, 12, 24]:\n",
    "             df[f'lag_{hours}h_volume_return'] = df['volume_btc'].pct_change(periods=hours)\n",
    "             prereqs_calculated.add(f'lag_{hours}h_volume_return')\n",
    "        for hours in [72, 168]:\n",
    "             df[f'lag_{hours}h_price_return'] = df['close'].pct_change(periods=hours)\n",
    "             prereqs_calculated.add(f'lag_{hours}h_price_return')\n",
    "    except Exception as e: print(f\"Warning: Error in lag calcs: {e}\")\n",
    "\n",
    "    # Rolling Stats\n",
    "    try:\n",
    "        for hours in [3]: df[f'rolling_std_{hours}h_temp'] = df['close'].rolling(window=hours, min_periods=min(hours,min_periods_base)).std() if len(df)>=hours else pd.Series(np.nan, index=df.index)\n",
    "        for hours in [6, 168]: df[f'rolling_std_{hours}h'] = df['close'].rolling(window=hours, min_periods=min(hours,min_periods_base)).std() if len(df)>=hours else pd.Series(np.nan, index=df.index)\n",
    "        for hours in [12, 72]: df[f'rolling_std_{hours}h_temp'] = df['close'].rolling(window=hours, min_periods=min(hours,min_periods_base)).std() if len(df)>=hours else pd.Series(np.nan, index=df.index)\n",
    "        prereqs_calculated.update(['rolling_std_3h_temp', 'rolling_std_6h', 'rolling_std_168h', 'rolling_std_12h_temp', 'rolling_std_72h_temp'])\n",
    "    except Exception as e: print(f\"Warning: Error in rolling std calcs: {e}\")\n",
    "\n",
    "    # Skew/Kurtosis\n",
    "    try:\n",
    "        if 'price_return_1h_temp' in df.columns:\n",
    "            df['rolling_skew_24h'] = df['price_return_1h_temp'].rolling(window=24, min_periods=24).skew() if len(df) >= 24 else pd.Series(np.nan, index=df.index)\n",
    "            df['rolling_kurt_24h'] = df['price_return_1h_temp'].rolling(window=24, min_periods=24).kurt() if len(df) >= 24 else pd.Series(np.nan, index=df.index)\n",
    "            prereqs_calculated.update(['rolling_skew_24h', 'rolling_kurt_24h'])\n",
    "    except Exception as e: print(f\"Warning: Error in skew/kurt calcs: {e}\")\n",
    "\n",
    "    # Volume MAs / Ratios\n",
    "    try:\n",
    "        df['volume_ma_24h_temp'] = df['volume_btc'].rolling(window=24, min_periods=min(24, min_periods_base)).mean() if len(df)>=24 else pd.Series(np.nan, index=df.index)\n",
    "        df['volume_ma_168h'] = df['volume_btc'].rolling(window=168, min_periods=min(168, min_periods_base)).mean() if len(df)>=168 else pd.Series(np.nan, index=df.index)\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            df['volume_div_ma_24h'] = df['volume_btc'] / df['volume_ma_24h_temp'].replace(0, np.nan)\n",
    "        prereqs_calculated.update(['volume_ma_24h_temp', 'volume_ma_168h', 'volume_div_ma_24h'])\n",
    "    except Exception as e: print(f\"Warning: Error in vol MA calcs: {e}\")\n",
    "\n",
    "    # MACD\n",
    "    try:\n",
    "        if len(df) >= 26:\n",
    "            ema_12 = df['close'].ewm(span=12, adjust=False, min_periods=12).mean()\n",
    "            ema_26 = df['close'].ewm(span=26, adjust=False, min_periods=26).mean()\n",
    "            df['macd_temp'] = ema_12 - ema_26\n",
    "        else: df['macd_temp'] = np.nan\n",
    "        if len(df) >= 35 and 'macd_temp' in df.columns and not df['macd_temp'].isnull().all():\n",
    "            df['macd_signal'] = df['macd_temp'].ewm(span=9, adjust=False, min_periods=9).mean()\n",
    "        else: df['macd_signal'] = np.nan\n",
    "        # Calculate hist safely using the calculated components\n",
    "        if 'macd_temp' in df.columns and 'macd_signal' in df.columns:\n",
    "             with warnings.catch_warnings(): # Suppress potential comparison warnings\n",
    "                  warnings.simplefilter(\"ignore\")\n",
    "                  df['macd_hist'] = df['macd_temp'] - df['macd_signal']\n",
    "        else: df['macd_hist'] = np.nan\n",
    "        prereqs_calculated.update(['macd_temp', 'macd_signal', 'macd_hist'])\n",
    "    except Exception as e: print(f\"Warning: Error in MACD calcs: {e}\")\n",
    "\n",
    "    # TA Lib Indicators\n",
    "    try:\n",
    "        ta_df = df.rename(columns={'volume_btc': 'volume'}, errors='ignore')\n",
    "        if all(c in ta_df.columns for c in ['high', 'low', 'close']):\n",
    "            df['cci_20h'] = ta_df.ta.cci(length=20)\n",
    "            if 'volume' in ta_df.columns: df['cmf_20h'] = ta_df.ta.cmf(length=20)\n",
    "            else: df['cmf_20h'] = np.nan\n",
    "            bbands_df = ta_df.ta.bbands(length=20, std=2)\n",
    "            df['bband_width_20h'] = bbands_df.get(f'BBB_20_2.0', np.nan) if bbands_df is not None else np.nan\n",
    "        else: df['cci_20h'], df['cmf_20h'], df['bband_width_20h'] = np.nan, np.nan, np.nan\n",
    "        prereqs_calculated.update(['cci_20h', 'cmf_20h', 'bband_width_20h'])\n",
    "    except Exception as e: print(f\"Warning: Error in TA-Lib calcs: {e}\")\n",
    "\n",
    "    # Other base calculations\n",
    "    try:\n",
    "        range_hl = df['high'] - df['low']\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            df['close_pos_in_range'] = ((df['close'] - df['low']) / range_hl.replace(0, np.nan)).fillna(0.5).replace([np.inf, -np.inf], 0.5)\n",
    "        prereqs_calculated.add('close_pos_in_range')\n",
    "\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "             df['std12_div_std72'] = df['rolling_std_12h_temp'] / df['rolling_std_72h_temp'].replace(0, np.nan) if ('rolling_std_12h_temp' in df.columns and 'rolling_std_72h_temp' in df.columns) else pd.Series(np.nan, index=df.index)\n",
    "        prereqs_calculated.add('std12_div_std72')\n",
    "    except Exception as e: print(f\"Warning: Error in other base calcs: {e}\")\n",
    "\n",
    "    # --- 2. Calculate Final Interaction and Transformation Features ---\n",
    "    print(\"  Calculating final target features...\")\n",
    "    final_feature_dict = {}\n",
    "\n",
    "    # Define helpers again (local scope)\n",
    "    def safe_multiply(col1_name, col2_name):\n",
    "        col1_actual = 'volume_btc' if col1_name == 'Volume BTC' else col1_name\n",
    "        col2_actual = 'volume_btc' if col2_name == 'Volume BTC' else col2_name\n",
    "        if col1_actual in df.columns and col2_actual in df.columns and pd.api.types.is_numeric_dtype(df[col1_actual]) and pd.api.types.is_numeric_dtype(df[col2_actual]):\n",
    "            return df[col1_actual] * df[col2_actual]\n",
    "        return pd.Series(np.nan, index=df.index)\n",
    "    def safe_sq(col_name):\n",
    "         if col_name in df.columns and pd.api.types.is_numeric_dtype(df[col_name]): return df[col_name]**2\n",
    "         return pd.Series(np.nan, index=df.index)\n",
    "    # No log1p needed for the 40 features\n",
    "    # No divide needed for the 40 features (other than std12/std72 which is base)\n",
    "\n",
    "    # Add direct/base features from TARGET_FEATURES_TO_GENERATE\n",
    "    for feat in TARGET_FEATURES_TO_GENERATE:\n",
    "        if '_x_' not in feat and not feat.endswith('_sq'):\n",
    "            if feat in df.columns:\n",
    "                final_feature_dict[feat] = df[feat]\n",
    "            elif feat.startswith('hour_') or feat.startswith('day_'):\n",
    "                 time_type, time_val = feat.split('_')\n",
    "                 time_val = int(time_val)\n",
    "                 if time_type == 'hour': final_feature_dict[feat] = (df.index.hour == time_val).astype(int)\n",
    "                 elif time_type == 'day': final_feature_dict[feat] = (df.index.dayofweek == time_val).astype(int)\n",
    "            #else: print(f\" Debug: Base feature '{feat}' not found/added.\")\n",
    "\n",
    "    # Calculate Interaction/Transformation Features from TARGET_FEATURES_TO_GENERATE\n",
    "    def add_if_requested(name, calculation_func):\n",
    "        if name in TARGET_FEATURES_TO_GENERATE: final_feature_dict[name] = calculation_func()\n",
    "\n",
    "    add_if_requested('cci_20h_x_cmf_20h', lambda: safe_multiply('cci_20h', 'cmf_20h'))\n",
    "    add_if_requested('cci_20h_x_lag_3h_volume_return', lambda: safe_multiply('cci_20h', 'lag_3h_volume_return'))\n",
    "    add_if_requested('cmf_20h_x_rolling_kurt_24h', lambda: safe_multiply('cmf_20h', 'rolling_kurt_24h'))\n",
    "    add_if_requested('cmf_20h_x_rolling_std_168h', lambda: safe_multiply('cmf_20h', 'rolling_std_168h'))\n",
    "    add_if_requested('lag_168h_price_return_x_cmf_20h', lambda: safe_multiply('lag_168h_price_return', 'cmf_20h'))\n",
    "    add_if_requested('lag_168h_price_return_x_rolling_kurt_24h', lambda: safe_multiply('lag_168h_price_return', 'rolling_kurt_24h'))\n",
    "    add_if_requested('lag_168h_price_return_x_volume_div_ma_24h', lambda: safe_multiply('lag_168h_price_return', 'volume_div_ma_24h'))\n",
    "    add_if_requested('lag_24h_volume_return_x_std12_div_std72', lambda: safe_multiply('lag_24h_volume_return', 'std12_div_std72'))\n",
    "    add_if_requested('lag_6h_volume_return_x_rolling_kurt_24h', lambda: safe_multiply('lag_6h_volume_return', 'rolling_kurt_24h'))\n",
    "    add_if_requested('lag_72h_price_return_sq', lambda: safe_sq('lag_72h_price_return'))\n",
    "    add_if_requested('lag_72h_price_return_x_lag_12h_volume_return', lambda: safe_multiply('lag_72h_price_return', 'lag_12h_volume_return'))\n",
    "    add_if_requested('lag_72h_price_return_x_lag_3h_volume_return', lambda: safe_multiply('lag_72h_price_return', 'lag_3h_volume_return'))\n",
    "    add_if_requested('macd_hist_sq', lambda: safe_sq('macd_hist'))\n",
    "    add_if_requested('macd_hist_x_rolling_std_6h', lambda: safe_multiply('macd_hist', 'rolling_std_6h'))\n",
    "    add_if_requested('macd_signal_x_cmf_20h', lambda: safe_multiply('macd_signal', 'cmf_20h'))\n",
    "    add_if_requested('macd_signal_x_rolling_std_6h', lambda: safe_multiply('macd_signal', 'rolling_std_6h'))\n",
    "    add_if_requested('rolling_std_3h_sq', lambda: safe_sq('rolling_std_3h_temp'))\n",
    "    add_if_requested('volume_ma_168h_x_rolling_kurt_24h', lambda: safe_multiply('volume_ma_168h', 'rolling_kurt_24h'))\n",
    "    add_if_requested('volume_ma_168h_x_std12_div_std72', lambda: safe_multiply('volume_ma_168h', 'std12_div_std72'))\n",
    "    # Added volume_btc_x_range calculation\n",
    "    add_if_requested('volume_btc_x_range', lambda: safe_multiply('volume_btc', 'price_range_pct_temp'))\n",
    "\n",
    "\n",
    "    # --- 3. Final Assembly and Cleanup ---\n",
    "    print(\"  Assembling final dataframe...\")\n",
    "    df_final_features = pd.DataFrame(final_feature_dict, index=df.index)\n",
    "\n",
    "    # Combine essential columns with calculated features\n",
    "    essential_cols = ['timestamp', 'symbol', 'open', 'high', 'low', 'close']\n",
    "    essential_cols_present = [col for col in essential_cols if col in df.columns]\n",
    "    df_combined = pd.concat([df[essential_cols_present], df_final_features], axis=1)\n",
    "\n",
    "    # Define final columns to keep: essentials + the target features\n",
    "    cols_to_keep = essential_cols_present + TARGET_FEATURES_TO_GENERATE\n",
    "\n",
    "    # Select ONLY the columns in cols_to_keep that actually exist in df_combined\n",
    "    final_df_structure = df_combined[[col for col in cols_to_keep if col in df_combined.columns]].copy()\n",
    "\n",
    "    # Report any target features that ended up missing\n",
    "    missing_final_cols = set(TARGET_FEATURES_TO_GENERATE) - set(final_df_structure.columns)\n",
    "    if missing_final_cols:\n",
    "        print(f\"  Final Warning: {len(missing_final_cols)} target features \"\n",
    "              f\"could not be generated/found: {missing_final_cols}\")\n",
    "\n",
    "    # Final cleanup\n",
    "    final_df_structure = final_df_structure.reset_index(drop=True)\n",
    "    final_df_structure = final_df_structure.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    end_time = time.time()\n",
    "    actual_feature_count = len([col for col in final_df_structure.columns if col not in essential_cols_present])\n",
    "    print(f\"Selected feature calculation finished. Returning {len(final_df_structure)} rows, \"\n",
    "          f\"{len(final_df_structure.columns)} total columns ({actual_feature_count} features). \"\n",
    "          f\"Took {end_time - start_time:.2f}s.\")\n",
    "\n",
    "    expected_feature_count = len(TARGET_FEATURES_TO_GENERATE)\n",
    "    if actual_feature_count != expected_feature_count:\n",
    "         print(f\"  NOTE: Expected {expected_feature_count} features based on target list, \"\n",
    "               f\"but returning DataFrame with {actual_feature_count} features due to calculation issues or missing prerequisites.\")\n",
    "\n",
    "    return final_df_structure\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print(\"--- 1. Data Loading & Initial Prep ---\")\n",
    "    try:\n",
    "        print(f\"Loading data from: {CSV_FILE_PATH}\")\n",
    "        col_names = ['unix', 'date', 'symbol_csv', 'open', 'high', 'low', 'close', 'Volume BTC', 'Volume USD']\n",
    "        df_raw = pd.read_csv(CSV_FILE_PATH, header=0, names=col_names)\n",
    "        print(f\"Raw data loaded. Shape: {df_raw.shape}\")\n",
    "        df_raw['timestamp'] = pd.to_datetime(df_raw['date'])\n",
    "        # Keep original Volume names for the feature function\n",
    "        df_raw = df_raw.drop(['unix', 'date', 'symbol_csv'], axis=1)\n",
    "        df_raw = df_raw.sort_values('timestamp').reset_index(drop=True)\n",
    "        if df_raw.empty: exit(\"DataFrame empty after loading. Exiting.\")\n",
    "        print(f\"Initial data prep done. Shape: {df_raw.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or processing CSV: {e}\"); traceback.print_exc(); exit()\n",
    "\n",
    "    print(\"\\n--- 2. Feature Engineering (Selected Features) ---\")\n",
    "    feature_calc_start = time.time()\n",
    "    # Assuming calculate_selected_features function is defined above and uses SELECTED_FEATURE_NAMES\n",
    "    df_features = calculate_selected_features(df_raw, symbol=SYMBOL_NAME)\n",
    "    feature_calc_end = time.time()\n",
    "    if df_features.empty: exit(\"Feature calculation failed. Exiting.\")\n",
    "    print(f\"Feature calculation completed in {feature_calc_end - feature_calc_start:.2f} seconds.\")\n",
    "\n",
    "    # Use the features actually present in the dataframe after calculation\n",
    "    # Filter SELECTED_FEATURE_NAMES to only include columns that were actually generated\n",
    "    CURRENT_FEATURE_COLS = [f for f in SELECTED_FEATURE_NAMES if f in df_features.columns]\n",
    "    if len(CURRENT_FEATURE_COLS) == 0:\n",
    "        exit(\"ERROR: No selected features found in the DataFrame after calculation.\")\n",
    "    if len(CURRENT_FEATURE_COLS) < len(SELECTED_FEATURE_NAMES):\n",
    "         print(f\"Warning: Only {len(CURRENT_FEATURE_COLS)} out of {len(SELECTED_FEATURE_NAMES)} requested features were found/generated.\")\n",
    "    print(f\"Using {len(CURRENT_FEATURE_COLS)} features found in DataFrame for modeling.\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- 3. Data Cleaning (Post-Features) ---\")\n",
    "    df_features = df_features.replace([np.inf, -np.inf], np.nan)\n",
    "    nan_check = df_features[CURRENT_FEATURE_COLS].isnull().sum()\n",
    "    total_nans = nan_check.sum()\n",
    "    print(f\"Total NaNs found in feature columns: {total_nans}.\")\n",
    "    # NaNs will be handled by models/pipelines\n",
    "\n",
    "    print(\"\\n--- 4. Modeling Target & Final Prep ---\")\n",
    "    TARGET_COLUMN = 'target'\n",
    "    df = df_features.copy()\n",
    "    df = df.sort_values('timestamp')\n",
    "    if 'close' not in df.columns: exit(\"ERROR: 'close' column missing before target creation.\")\n",
    "\n",
    "    # --- USE PREDICTION_WINDOW_ROWS FOR TARGET ---\n",
    "    print(f\"Creating binary target based on {PREDICTION_WINDOW_HOURS}-hour future return >= {TARGET_THRESHOLD_PCT}%...\")\n",
    "    df['future_price'] = df['close'].shift(-PREDICTION_WINDOW_ROWS) # USE PREDICTION WINDOW\n",
    "    # --- END TARGET MODIFICATION ---\n",
    "\n",
    "    df['price_return_future'] = (df['future_price'] - df['close']) / df['close'].replace(0, np.nan) * 100\n",
    "    df['target'] = (df['price_return_future'] >= TARGET_THRESHOLD_PCT).astype(int)\n",
    "    df = df.drop(['future_price', 'price_return_future'], axis=1)\n",
    "\n",
    "    # Only drop rows where target or 'close' (needed for target calc) is NaN.\n",
    "    initial_rows = len(df)\n",
    "    df = df.dropna(subset=[TARGET_COLUMN, 'close'])\n",
    "    print(f\"Rows after removing NaN targets/close: {len(df)} (Removed {initial_rows - len(df)})\")\n",
    "\n",
    "    if df.empty: exit(\"DataFrame empty after target creation/NaN drop. Exiting.\")\n",
    "    target_counts = df[TARGET_COLUMN].value_counts(normalize=True) * 100\n",
    "    print(\"\\nTarget variable distribution:\")\n",
    "    print(f\"  0 (< {TARGET_THRESHOLD_PCT}% return): {target_counts.get(0, 0):.2f}%\")\n",
    "    print(f\"  1 (>= {TARGET_THRESHOLD_PCT}% return): {target_counts.get(1, 0):.2f}%\")\n",
    "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "    print(f\"Final DataFrame shape for backtesting: {df.shape}\")\n",
    "\n",
    "\n",
    "    # --- 5. Walk-Forward Validation with Stacking ---\n",
    "    print(\"\\n--- 5. Starting Walk-Forward Validation (Stacking Ensemble) ---\")\n",
    "    all_metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
    "    all_best_thresholds = []\n",
    "    meta_feature_names = ['xgb_pred', 'lgbm_pred', 'svm_pred']\n",
    "    meta_feature_importances = {meta_feat: [] for meta_feat in meta_feature_names}\n",
    "    iteration_count = 0\n",
    "\n",
    "    n_rows_total = len(df)\n",
    "    current_train_start_idx = 0\n",
    "    # Estimate depends on the size of the EVALUATION window (TEST_WINDOW_ROWS)\n",
    "    total_iterations_estimate = max(0, (n_rows_total - TRAIN_WINDOW_ROWS - TEST_WINDOW_ROWS) // STEP_ROWS + 1)\n",
    "\n",
    "    # --- UPDATED Print Statement ---\n",
    "    print(f\"Total rows: {n_rows_total}, Train Window: {TRAIN_WINDOW_HOURS}h, Prediction Horizon: {PREDICTION_WINDOW_HOURS}h, Evaluation (Test) Window: {TEST_WINDOW_HOURS}h, Step: {STEP_HOURS}h\")\n",
    "    # --- END Print Statement Update ---\n",
    "\n",
    "    print(f\"Estimated iterations: {total_iterations_estimate}\")\n",
    "    print(f\"Stacking Folds (K): {N_STACKING_FOLDS}\")\n",
    "    print(f\"Meta Learner Grid: {META_XGB_PARAM_GRID}\")\n",
    "    print(f\"Threshold Search Range: {THRESHOLD_SEARCH_RANGE}\")\n",
    "    print(\"-\" * 30)\n",
    "    start_loop_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        # --- Define Window Boundaries using TEST_WINDOW_ROWS for evaluation slice ---\n",
    "        train_end_idx = current_train_start_idx + TRAIN_WINDOW_ROWS\n",
    "        test_start_idx = train_end_idx\n",
    "        test_end_idx = test_start_idx + TEST_WINDOW_ROWS # Size of the evaluation slice\n",
    "        if test_end_idx > n_rows_total:\n",
    "             print(f\"\\nStopping: Evaluation window end ({test_end_idx}) exceeds total rows ({n_rows_total}). Last start index: {current_train_start_idx}\")\n",
    "             break\n",
    "        if current_train_start_idx >= n_rows_total:\n",
    "             print(f\"\\nStopping: Train start index ({current_train_start_idx}) reached end.\")\n",
    "             break\n",
    "\n",
    "        # --- Data Slicing ---\n",
    "        train_df = df.iloc[current_train_start_idx : train_end_idx].copy()\n",
    "        test_df = df.iloc[test_start_idx : test_end_idx].copy() # This slice is for evaluation\n",
    "\n",
    "        # --- Basic Validity Checks ---\n",
    "        min_train_samples = max(50, int(0.1 * TRAIN_WINDOW_ROWS), N_STACKING_FOLDS * 2)\n",
    "        min_test_samples = 5 # Minimum needed in the evaluation slice\n",
    "        if len(train_df) < min_train_samples or len(test_df) < min_test_samples:\n",
    "            print(f\"Skipping iter {iteration_count + 1}: Insufficient data train ({len(train_df)}/{min_train_samples}) or test ({len(test_df)}/{min_test_samples}).\")\n",
    "            current_train_start_idx += STEP_ROWS\n",
    "            continue\n",
    "\n",
    "        # Use CURRENT_FEATURE_COLS determined after feature generation\n",
    "        X_train_full = train_df[CURRENT_FEATURE_COLS]\n",
    "        y_train_full = train_df[TARGET_COLUMN]\n",
    "        X_test = test_df[CURRENT_FEATURE_COLS] # Features for evaluation slice\n",
    "        y_test = test_df[TARGET_COLUMN]       # Ground truth for evaluation slice\n",
    "\n",
    "        if len(y_train_full.unique()) < 2:\n",
    "            print(f\"Skipping iter {iteration_count + 1}: Training data has only one class.\")\n",
    "            current_train_start_idx += STEP_ROWS\n",
    "            continue\n",
    "        if len(y_test.unique()) < 2:\n",
    "             print(f\"Warning iter {iteration_count + 1}: Evaluation test data (size {len(test_df)}) has only one class. Metrics will be affected.\")\n",
    "\n",
    "\n",
    "        # --- Calculate scale_pos_weight ---\n",
    "        neg_count = y_train_full.value_counts().get(0, 0)\n",
    "        pos_count = y_train_full.value_counts().get(1, 0)\n",
    "        scale_pos_weight_val = neg_count / pos_count if pos_count > 0 else 1.0\n",
    "\n",
    "        iter_start_time = time.time()\n",
    "        print(f\"\\n--- Iter {iteration_count + 1}/{total_iterations_estimate} ---\")\n",
    "        print(f\"  Train Indices: [{current_train_start_idx}:{train_end_idx-1}], Evaluation Indices: [{test_start_idx}:{test_end_idx-1}]\")\n",
    "\n",
    "        # --- Level 0: Generate Out-of-Fold (OOF) Predictions ---\n",
    "        print(f\"  Level 0: Generating OOF predictions using {N_STACKING_FOLDS}-Fold CV...\")\n",
    "        skf = StratifiedKFold(n_splits=N_STACKING_FOLDS, shuffle=True, random_state=42 + iteration_count)\n",
    "        oof_xgb = np.full(len(train_df), np.nan)\n",
    "        oof_lgbm = np.full(len(train_df), np.nan)\n",
    "        oof_svm = np.full(len(train_df), np.nan)\n",
    "\n",
    "        # Define Base Models (re-init each iteration)\n",
    "        model_xgb_base = XGBClassifier(**XGB_BASE_PARAMS, scale_pos_weight=scale_pos_weight_val)\n",
    "        model_lgbm_base = LGBMClassifier(**LGBM_BASE_PARAMS, scale_pos_weight=scale_pos_weight_val)\n",
    "        pipeline_svm_base = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('svm', SVC(**SVM_BASE_PARAMS))\n",
    "        ])\n",
    "        models_oof = {'xgb': model_xgb_base, 'lgbm': model_lgbm_base, 'svm': pipeline_svm_base}\n",
    "        oof_arrays = {'xgb': oof_xgb, 'lgbm': oof_lgbm, 'svm': oof_svm}\n",
    "\n",
    "        # K-Fold Loop (Using Attempt 3 logic)\n",
    "        for fold, (train_idx_k, val_idx_k) in enumerate(skf.split(X_train_full, y_train_full)):\n",
    "            X_train_k, y_train_k = X_train_full.iloc[train_idx_k], y_train_full.iloc[train_idx_k]\n",
    "            X_val_k, y_val_k = X_train_full.iloc[val_idx_k], y_train_full.iloc[val_idx_k]\n",
    "\n",
    "            if len(np.unique(y_train_k)) < 2 or len(np.unique(y_val_k)) < 2:\n",
    "                print(f\"    Warning: Fold {fold+1} has single class in train/val. Assigning prior.\")\n",
    "                prior = y_train_full.mean()\n",
    "                for key in oof_arrays: oof_arrays[key][val_idx_k] = prior\n",
    "                continue\n",
    "\n",
    "            for name, model in models_oof.items():\n",
    "                try:\n",
    "                    fit_params_k = {}\n",
    "                    lgbm_eval_set = None\n",
    "                    if name == 'lgbm':\n",
    "                        fit_params_k['callbacks'] = [early_stopping(10, verbose=False), log_evaluation(0)]\n",
    "                        lgbm_eval_set = [(X_val_k, y_val_k)]\n",
    "                        fit_params_k['eval_metric'] = 'logloss' # Explicitly adding metric\n",
    "                    elif name == 'xgb':\n",
    "                        fit_params_k['eval_set'] = [(X_val_k, y_val_k)]\n",
    "                        fit_params_k['early_stopping_rounds'] = 10\n",
    "                        fit_params_k['verbose'] = False\n",
    "\n",
    "                    if name == 'lgbm' and lgbm_eval_set:\n",
    "                        model.fit(X_train_k, y_train_k, eval_set=lgbm_eval_set, **fit_params_k)\n",
    "                    else:\n",
    "                        model.fit(X_train_k, y_train_k, **fit_params_k)\n",
    "\n",
    "                    oof_arrays[name][val_idx_k] = model.predict_proba(X_val_k)[:, 1]\n",
    "                except Exception as e_kfold:\n",
    "                    print(f\"    Error during K-Fold {fold+1} training for {name}: {e_kfold}\")\n",
    "                    prior = y_train_full.mean()\n",
    "                    if val_idx_k is not None and len(val_idx_k) > 0: oof_arrays[name][val_idx_k] = prior\n",
    "\n",
    "        # Check OOF NaNs\n",
    "        if np.isnan(oof_xgb).all() or np.isnan(oof_lgbm).all() or np.isnan(oof_svm).all():\n",
    "             print(\"  ERROR: At least one base model failed in all K-Folds. Skipping iteration.\")\n",
    "             current_train_start_idx += STEP_ROWS\n",
    "             continue\n",
    "\n",
    "        # Create Meta Train Features\n",
    "        X_meta_train = pd.DataFrame({\n",
    "            'xgb_pred': np.nan_to_num(oof_xgb, nan=np.nanmean(oof_xgb)),\n",
    "            'lgbm_pred': np.nan_to_num(oof_lgbm, nan=np.nanmean(oof_lgbm)),\n",
    "            'svm_pred': np.nan_to_num(oof_svm, nan=np.nanmean(oof_svm))\n",
    "        }, index=X_train_full.index)\n",
    "        y_meta_train = y_train_full\n",
    "\n",
    "        # --- Level 0: Train Base Models on Full Training Data ---\n",
    "        print(f\"  Level 0: Training base models on full training data ({len(train_df)} rows)...\")\n",
    "        models_full = {}\n",
    "        all_base_trained = True\n",
    "        for name, model in models_oof.items():\n",
    "             try:\n",
    "                  full_fit_params = {}\n",
    "                  if name == 'xgb': full_fit_params['verbose'] = False\n",
    "                  # LGBM uses verbose=-1 from init, SVM pipeline is simple\n",
    "                  model.fit(X_train_full, y_train_full, **full_fit_params)\n",
    "                  models_full[name] = model\n",
    "             except Exception as e_full_fit:\n",
    "                  print(f\"  ERROR: Failed to train base model '{name}' on full data: {e_full_fit}\")\n",
    "                  all_base_trained = False; break\n",
    "        if not all_base_trained:\n",
    "             print(\"  Skipping iteration due to base model training failure.\")\n",
    "             current_train_start_idx += STEP_ROWS; continue\n",
    "        print(\"  Level 0 Full Training Done.\")\n",
    "\n",
    "        # --- Level 1: Meta Learner Tuning & Threshold Tuning ---\n",
    "        print(\"  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\")\n",
    "        best_meta_params = None\n",
    "        best_meta_score = -np.inf\n",
    "        best_meta_model_for_thresh = None\n",
    "        best_threshold_iter = 0.5\n",
    "        best_thresh_f1_score = -np.inf\n",
    "\n",
    "        meta_val_size = int(len(X_meta_train) * META_VALIDATION_PCT)\n",
    "        if meta_val_size < N_STACKING_FOLDS or (len(X_meta_train) - meta_val_size) < N_STACKING_FOLDS:\n",
    "             print(\"  Warning: Meta dataset too small for validation split. Using defaults.\")\n",
    "             best_meta_params = list(ParameterGrid(META_XGB_PARAM_GRID))[0]\n",
    "             best_threshold_iter = 0.5\n",
    "        else:\n",
    "            X_meta_train_sub = X_meta_train[:-meta_val_size]\n",
    "            y_meta_train_sub = y_meta_train[:-meta_val_size]\n",
    "            X_meta_val = X_meta_train[-meta_val_size:]\n",
    "            y_meta_val = y_meta_train[-meta_val_size:]\n",
    "\n",
    "            if len(y_meta_val.unique()) < 2 or len(y_meta_train_sub.unique()) < 2:\n",
    "                 print(\"  Warning: Meta train/val split has single class. Using defaults.\")\n",
    "                 best_meta_params = list(ParameterGrid(META_XGB_PARAM_GRID))[0]\n",
    "                 best_threshold_iter = 0.5\n",
    "            else:\n",
    "                # Meta Grid Search\n",
    "                for params_meta_cv in ParameterGrid(META_XGB_PARAM_GRID):\n",
    "                    try:\n",
    "                        current_meta_params = {**META_XGB_FIXED_PARAMS, **params_meta_cv}\n",
    "                        model_meta_cv = XGBClassifier(**current_meta_params, scale_pos_weight=scale_pos_weight_val)\n",
    "                        model_meta_cv.fit(X_meta_train_sub, y_meta_train_sub, eval_set=[(X_meta_val, y_meta_val)], early_stopping_rounds=10, verbose=False)\n",
    "                        y_pred_meta_val_cv = model_meta_cv.predict(X_meta_val)\n",
    "                        meta_score = f1_score(y_meta_val, y_pred_meta_val_cv, average='binary', pos_label=1, zero_division=0)\n",
    "                        if meta_score > best_meta_score:\n",
    "                            best_meta_score = meta_score; best_meta_params = params_meta_cv; best_meta_model_for_thresh = model_meta_cv\n",
    "                    except Exception as e_meta_cv:\n",
    "                        print(f\"    Error during Meta CV with params {params_meta_cv}: {e_meta_cv}\")\n",
    "                        if best_meta_params is None: best_meta_params = list(ParameterGrid(META_XGB_PARAM_GRID))[0]\n",
    "                if best_meta_params is None: best_meta_params = list(ParameterGrid(META_XGB_PARAM_GRID))[0]\n",
    "                print(f\"    Best Meta Params: {best_meta_params} (Val F1: {best_meta_score:.4f})\")\n",
    "\n",
    "                # Threshold Tuning\n",
    "                if best_meta_model_for_thresh is not None:\n",
    "                    try:\n",
    "                        y_meta_proba_val = best_meta_model_for_thresh.predict_proba(X_meta_val)[:, 1]\n",
    "                        for t in THRESHOLD_SEARCH_RANGE:\n",
    "                            y_pred_meta_val_t = (y_meta_proba_val >= t).astype(int)\n",
    "                            current_f1 = f1_score(y_meta_val, y_pred_meta_val_t, average='binary', pos_label=1, zero_division=0)\n",
    "                            if current_f1 >= best_thresh_f1_score: best_thresh_f1_score = current_f1; best_threshold_iter = t\n",
    "                        print(f\"    Best Threshold: {best_threshold_iter:.2f} (Val F1: {best_thresh_f1_score:.4f})\")\n",
    "                    except Exception as e_thresh: print(f\"    Error during threshold tuning: {e_thresh}. Using default 0.5.\"); best_threshold_iter = 0.5\n",
    "                else: print(\"    Skipping threshold tuning (no best meta model found). Using default 0.5.\"); best_threshold_iter = 0.5\n",
    "\n",
    "        # --- Level 1: Train Final Meta Learner ---\n",
    "        print(\"  Level 1: Training final Meta-Learner...\")\n",
    "        try:\n",
    "             final_meta_params = {**META_XGB_FIXED_PARAMS, **(best_meta_params or list(ParameterGrid(META_XGB_PARAM_GRID))[0])}\n",
    "             meta_model_final = XGBClassifier(**final_meta_params, scale_pos_weight=scale_pos_weight_val)\n",
    "             meta_model_final.fit(X_meta_train, y_meta_train, verbose=False)\n",
    "             print(\"  Level 1 Final Training Done.\")\n",
    "        except Exception as e_meta_final:\n",
    "             print(f\"  ERROR: Failed to train final meta-learner: {e_meta_final}\")\n",
    "             current_train_start_idx += STEP_ROWS; continue\n",
    "\n",
    "        # --- Prediction Phase ---\n",
    "        print(\"  Prediction: Generating final predictions...\")\n",
    "        try:\n",
    "            # Predict with Level 0 models on the EVALUATION test data slice\n",
    "            pred_xgb_test = models_full['xgb'].predict_proba(X_test)[:, 1]\n",
    "            pred_lgbm_test = models_full['lgbm'].predict_proba(X_test)[:, 1]\n",
    "            pred_svm_test = models_full['svm'].predict_proba(X_test)[:, 1]\n",
    "            # Create Meta Features for Test Data\n",
    "            X_meta_test = pd.DataFrame({'xgb_pred': pred_xgb_test, 'lgbm_pred': pred_lgbm_test, 'svm_pred': pred_svm_test})\n",
    "            # Predict Probabilities with Final Meta Learner\n",
    "            y_proba_test = meta_model_final.predict_proba(X_meta_test)[:, 1]\n",
    "            # Apply Tuned Threshold\n",
    "            y_pred = (y_proba_test >= best_threshold_iter).astype(int)\n",
    "            print(\"  Prediction Done.\")\n",
    "        except Exception as e_pred:\n",
    "             print(f\"  ERROR during prediction phase: {e_pred}\")\n",
    "             for key in all_metrics: all_metrics[key].append(np.nan)\n",
    "             for meta_feat in meta_feature_names: meta_feature_importances[meta_feat].append(np.nan)\n",
    "             all_best_thresholds.append(np.nan)\n",
    "             current_train_start_idx += STEP_ROWS; continue\n",
    "\n",
    "        # --- Evaluation (on the EVALUATION test slice) ---\n",
    "        if len(np.unique(y_test)) < 2:\n",
    "            # Handled above with warning print\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            precision, recall, f1 = 0.0, 0.0, 0.0\n",
    "            if np.unique(y_test)[0] == 1 and np.all(y_pred == 1): precision, recall, f1 = 1.0, 1.0, 1.0\n",
    "        else:\n",
    "             accuracy = accuracy_score(y_test, y_pred)\n",
    "             precision = precision_score(y_test, y_pred, average='binary', pos_label=1, zero_division=0)\n",
    "             recall = recall_score(y_test, y_pred, average='binary', pos_label=1, zero_division=0)\n",
    "             f1 = f1_score(y_test, y_pred, average='binary', pos_label=1, zero_division=0)\n",
    "\n",
    "        all_metrics['accuracy'].append(accuracy)\n",
    "        all_metrics['precision'].append(precision)\n",
    "        all_metrics['recall'].append(recall)\n",
    "        all_metrics['f1'].append(f1)\n",
    "        all_best_thresholds.append(best_threshold_iter)\n",
    "        print(f\"  Evaluation Metrics (Test Window Size: {TEST_WINDOW_HOURS}h): Acc={accuracy:.4f}, Prc={precision:.4f}, Rec={recall:.4f}, F1={f1:.4f}\")\n",
    "\n",
    "        # --- Store Meta-Learner Feature Importances ---\n",
    "        # (Keep importance storing logic as before)\n",
    "        try:\n",
    "            if isinstance(meta_model_final, XGBClassifier):\n",
    "                fold_importances = meta_model_final.get_booster().get_score(importance_type='gain')\n",
    "                booster_feature_names = meta_model_final.get_booster().feature_names\n",
    "                name_map = {booster_name: original_name for booster_name, original_name in zip(booster_feature_names, X_meta_train.columns)}\n",
    "                iter_importances = {key: 0.0 for key in meta_feature_names}\n",
    "                for internal_name, imp_value in fold_importances.items():\n",
    "                     original_name = name_map.get(internal_name)\n",
    "                     if original_name in iter_importances: iter_importances[original_name] = imp_value\n",
    "                for key in meta_feature_names: meta_feature_importances[key].append(iter_importances[key])\n",
    "            else:\n",
    "                 print(\" Meta-learner is not XGBoost, cannot get gain importance easily.\")\n",
    "                 for meta_feat in meta_feature_names: meta_feature_importances[meta_feat].append(np.nan)\n",
    "        except Exception as e_imp:\n",
    "            print(f\"  Warning: Could not get meta-feature importance: {e_imp}\")\n",
    "            for meta_feat in meta_feature_names: meta_feature_importances[meta_feat].append(np.nan)\n",
    "\n",
    "        iteration_count += 1\n",
    "        iter_end_time = time.time()\n",
    "        print(f\"  Iteration {iteration_count} finished in {iter_end_time - iter_start_time:.2f} seconds.\")\n",
    "\n",
    "        # --- Move to Next Window ---\n",
    "        current_train_start_idx += STEP_ROWS\n",
    "\n",
    "\n",
    "    end_loop_time = time.time()\n",
    "    print(\"-\" * 30)\n",
    "    loop_duration_minutes = (end_loop_time - start_loop_time) / 60\n",
    "    print(f\"Walk-Forward Validation (Stacking) finished in {end_loop_time - start_loop_time:.2f} seconds ({loop_duration_minutes:.2f} minutes).\")\n",
    "\n",
    "    # --- 6. Aggregate and Display Results ---\n",
    "    print(\"\\n--- 6. Final Results (Stacking Ensemble) ---\")\n",
    "    if iteration_count > 0 and len(all_metrics['f1']) > 0:\n",
    "        valid_f1 = [m for m in all_metrics['f1'] if not pd.isna(m)]\n",
    "        if valid_f1:\n",
    "            avg_accuracy = np.nanmean(all_metrics['accuracy'])\n",
    "            avg_precision = np.nanmean(all_metrics['precision'])\n",
    "            avg_recall = np.nanmean(all_metrics['recall'])\n",
    "            avg_f1 = np.nanmean(valid_f1)\n",
    "\n",
    "            print(\"\\n--- Average Walk-Forward Validation Results ---\")\n",
    "            print(f\"Total Folds / Successful Iterations Evaluated: {iteration_count}\")\n",
    "            # --- UPDATED Print Statements ---\n",
    "            print(f\"Target Threshold: {TARGET_THRESHOLD_PCT}% increase over {PREDICTION_WINDOW_HOURS} hours (Prediction Horizon)\")\n",
    "            print(f\"Train Window: {TRAIN_WINDOW_HOURS} hours, Evaluation Window: {TEST_WINDOW_HOURS} hours, Step: {STEP_HOURS} hours\")\n",
    "             # --- END Print Statements Update ---\n",
    "            print(f\"Stacking Folds: {N_STACKING_FOLDS}\")\n",
    "            print(f\"Average Accuracy:  {avg_accuracy:.4f}\")\n",
    "            print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "            print(f\"Average Recall:    {avg_recall:.4f}\")\n",
    "            print(f\"Average F1-Score:  {avg_f1:.4f}\")\n",
    "\n",
    "            std_accuracy = np.nanstd(all_metrics['accuracy'])\n",
    "            std_precision = np.nanstd(all_metrics['precision'])\n",
    "            std_recall = np.nanstd(all_metrics['recall'])\n",
    "            std_f1 = np.nanstd(valid_f1)\n",
    "            print(\"\\n--- Standard Deviation of Metrics Across Folds ---\")\n",
    "            print(f\"Std Dev Accuracy:  {std_accuracy:.4f}\")\n",
    "            print(f\"Std Dev Precision: {std_precision:.4f}\")\n",
    "            print(f\"Std Dev Recall:    {std_recall:.4f}\")\n",
    "            print(f\"Std Dev F1-Score:  {std_f1:.4f}\")\n",
    "\n",
    "            avg_threshold = np.nanmean(all_best_thresholds)\n",
    "            std_threshold = np.nanstd(all_best_thresholds)\n",
    "            print(f\"\\nAverage Best Threshold Found: {avg_threshold:.3f} (StdDev: {std_threshold:.3f})\")\n",
    "\n",
    "            print(\"\\n--- Average Meta-Feature Importances (Gain) ---\")\n",
    "            avg_meta_importances = {}\n",
    "            for f, imp_list in meta_feature_importances.items():\n",
    "                 valid_imps = [imp for imp in imp_list if not pd.isna(imp)]\n",
    "                 avg_meta_importances[f] = np.mean(valid_imps) if valid_imps else 0.0\n",
    "            total_importance = sum(avg_meta_importances.values())\n",
    "            if total_importance > 1e-9:\n",
    "                 normalized_importances = {f: (imp / total_importance) * 100 for f, imp in avg_meta_importances.items()}\n",
    "                 sorted_meta_importances = sorted(normalized_importances.items(), key=lambda item: item[1], reverse=True)\n",
    "                 for i, (feature, importance) in enumerate(sorted_meta_importances): print(f\"  {i+1}. {feature}: {importance:.2f}%\")\n",
    "            else: print(\"  Meta-feature importance data could not be calculated or was always zero.\")\n",
    "        else: print(\"\\nNo valid metrics recorded (all folds might have failed evaluation).\")\n",
    "    else: print(\"\\nNo iterations were successfully completed.\")\n",
    "\n",
    "    print(\"\\nScript finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports: Added LGBMClassifier, SVC, StandardScaler, StratifiedKFold, Pipeline.\n",
    "\n",
    "Configuration:\n",
    "\n",
    "Defined N_STACKING_FOLDS.\n",
    "\n",
    "Defined static hyperparameters for base models (XGB_BASE_PARAMS, LGBM_BASE_PARAMS, SVM_BASE_PARAMS). Note the probability=True and class_weight='balanced' for SVC.\n",
    "\n",
    "Defined meta-learner grid (META_XGB_PARAM_GRID) and fixed params (META_XGB_FIXED_PARAMS).\n",
    "\n",
    "Defined META_VALIDATION_PCT.\n",
    "\n",
    "Main Loop Structure: The core logic now happens inside the while True loop:\n",
    "\n",
    "Level 0 OOF: A StratifiedKFold loop iterates through the train_df. In each fold, base models are trained on K-1 parts and predict probabilities on the held-out part. These predictions populate oof_xgb, oof_lgbm, oof_svm. Error handling for single-class folds is included.\n",
    "\n",
    "SVM Handling: SVM requires scaling and imputation. A Pipeline is used, and imputation (fillna(median)) is done just before scaling/fitting within each K-fold and for the full fit to avoid data leakage. Train median is used to impute validation/test sets.\n",
    "\n",
    "Level 0 Full Training: After the K-Fold loop, base models (model_xgb_full, etc.) are trained on the entire train_df.\n",
    "\n",
    "Meta-Feature Creation: X_meta_train is created from the OOF arrays.\n",
    "\n",
    "Level 1 Tuning: X_meta_train is split into sub-train/validation sets. A grid search tunes the meta-XGBoost model (best_meta_params, best_meta_model_for_thresh).\n",
    "\n",
    "Threshold Tuning: Uses best_meta_model_for_thresh and the meta-validation set to find best_threshold_iter.\n",
    "\n",
    "Level 1 Final Training: The final meta-learner (meta_model_final) is trained on all of X_meta_train using best_meta_params.\n",
    "\n",
    "Prediction Pipeline:\n",
    "\n",
    "Base models (*_full) predict on X_test.\n",
    "\n",
    "These predictions form X_meta_test.\n",
    "\n",
    "meta_model_final predicts probabilities on X_meta_test.\n",
    "\n",
    "best_threshold_iter is applied to get final y_pred.\n",
    "\n",
    "Evaluation: Metrics are calculated as before.\n",
    "\n",
    "Importance: Feature importance is now tracked for the meta-learner (features are 'xgb_pred', 'lgbm_pred', 'svm_pred').\n",
    "\n",
    "Important Notes:\n",
    "\n",
    "Computational Cost: This script will be significantly slower than the previous ones due to fitting multiple models multiple times in each iteration.\n",
    "\n",
    "NaN Handling: XGBoost/LightGBM handle NaNs internally (if using hist tree method). SVM requires explicit imputation, which is handled here using median imputation within a Pipeline or just before fitting. Ensure your base models' parameters are set appropriately if you expect NaNs.\n",
    "\n",
    "Hyperparameters: The static hyperparameters for base models are crucial. If they are poorly chosen, the ensemble might not perform well. Consider tuning them separately beforehand if possible.\n",
    "\n",
    "Memory: Storing multiple models and intermediate predictions might increase memory usage per iteration.\n",
    "\n",
    "Complexity: The code is inherently more complex due to the nested nature of stacking.\n",
    "\n",
    "This implementation provides the full stacking workflow with meta-learner tuning and probability threshold optimization within your walk-forward structure. Be prepared for the longer run times!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
