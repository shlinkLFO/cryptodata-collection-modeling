{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Loading ---\n",
      "Loading data from: BTCUSD.csv\n",
      "Selected last 15000 rows.\n",
      "\n",
      "--- Feature Engineering ---\n",
      "Feature engineering complete. Took 0.03 seconds.\n",
      "Columns after features: 51\n",
      "\n",
      "--- Target Definition ---\n",
      "Defining target as 24h future return >= -2.5%...\n",
      "\n",
      "--- Data Preparation ---\n",
      "NaN Handling: Dropped 197 rows.\n",
      "Replacing 8 infinites...\n",
      "Dropped 8 more rows.\n",
      "Final feature matrix shape: (14795, 42)\n",
      "Target vector shape: (14795,)\n",
      "Using 42 features.\n",
      "\n",
      "--- Starting SLIDING Window Backtest with Per-Step HParam Tuning ---\n",
      "!!! WARNING: This will be significantly slower due to GridSearchCV in each step !!!\n",
      "Train Window: 1344 rows, Step: 24 rows, Test Window: 268 rows, Tuning Grid Size: 48\n",
      "\n",
      "--- Step 1 (Predicting for window starting 2023-09-21 02:00:00) ---\n",
      "  Training window: [0:1343]; Testing window: [1344:1611]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 5.45s.\n",
      "  Best Params: {'colsample_bytree': 0.75, 'max_depth': 6, 'n_estimators': 140, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8394\n",
      "  Step 1 finished in 5.55s total.\n",
      "\n",
      "--- Step 2 (Predicting for window starting 2023-09-22 02:00:00) ---\n",
      "  Training window: [24:1367]; Testing window: [1368:1635]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.22s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 105, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8383\n",
      "  Step 2 finished in 2.29s total.\n",
      "\n",
      "--- Step 3 (Predicting for window starting 2023-09-23 02:00:00) ---\n",
      "  Training window: [48:1391]; Testing window: [1392:1659]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.17s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 9, 'n_estimators': 105, 'reg_alpha': 0.14, 'subsample': 0.83}, Best CV F1: 0.8328\n",
      "  Step 3 finished in 2.24s total.\n",
      "\n",
      "--- Step 4 (Predicting for window starting 2023-09-24 02:00:00) ---\n",
      "  Training window: [72:1415]; Testing window: [1416:1683]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.20s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 7, 'n_estimators': 140, 'reg_alpha': 0.26, 'subsample': 0.83}, Best CV F1: 0.8426\n",
      "  Step 4 finished in 2.30s total.\n",
      "\n",
      "--- Step 5 (Predicting for window starting 2023-09-25 02:00:00) ---\n",
      "  Training window: [96:1439]; Testing window: [1440:1707]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.17s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 7, 'n_estimators': 140, 'reg_alpha': 0.26, 'subsample': 0.83}, Best CV F1: 0.8158\n",
      "  Step 5 finished in 2.26s total.\n",
      "\n",
      "--- Step 6 (Predicting for window starting 2023-09-26 02:00:00) ---\n",
      "  Training window: [120:1463]; Testing window: [1464:1731]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.14s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 140, 'reg_alpha': 0.26, 'subsample': 0.92}, Best CV F1: 0.8255\n",
      "  Step 6 finished in 2.21s total.\n",
      "\n",
      "--- Step 7 (Predicting for window starting 2023-09-27 02:00:00) ---\n",
      "  Training window: [144:1487]; Testing window: [1488:1755]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.12s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 105, 'reg_alpha': 0.26, 'subsample': 0.83}, Best CV F1: 0.8471\n",
      "  Step 7 finished in 2.18s total.\n",
      "\n",
      "--- Step 8 (Predicting for window starting 2023-09-28 02:00:00) ---\n",
      "  Training window: [168:1511]; Testing window: [1512:1779]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.12s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 7, 'n_estimators': 140, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8503\n",
      "  Step 8 finished in 2.20s total.\n",
      "\n",
      "--- Step 9 (Predicting for window starting 2023-09-29 02:00:00) ---\n",
      "  Training window: [192:1535]; Testing window: [1536:1803]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.15s.\n",
      "  Best Params: {'colsample_bytree': 0.75, 'max_depth': 7, 'n_estimators': 140, 'reg_alpha': 0.14, 'subsample': 0.83}, Best CV F1: 0.7866\n",
      "  Step 9 finished in 2.23s total.\n",
      "\n",
      "--- Step 10 (Predicting for window starting 2023-09-30 02:00:00) ---\n",
      "  Training window: [216:1559]; Testing window: [1560:1827]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.08s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 7, 'n_estimators': 105, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8409\n",
      "  Step 10 finished in 2.16s total.\n",
      "\n",
      "--- Step 11 (Predicting for window starting 2023-10-01 02:00:00) ---\n",
      "  Training window: [240:1583]; Testing window: [1584:1851]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.10s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 140, 'reg_alpha': 0.14, 'subsample': 0.83}, Best CV F1: 0.8384\n",
      "  Step 11 finished in 2.18s total.\n",
      "\n",
      "--- Step 12 (Predicting for window starting 2023-10-02 02:00:00) ---\n",
      "  Training window: [264:1607]; Testing window: [1608:1875]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.06s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 105, 'reg_alpha': 0.14, 'subsample': 0.83}, Best CV F1: 0.8355\n",
      "  Step 12 finished in 2.12s total.\n",
      "\n",
      "--- Step 13 (Predicting for window starting 2023-10-03 02:00:00) ---\n",
      "  Training window: [288:1631]; Testing window: [1632:1899]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.24s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 105, 'reg_alpha': 0.26, 'subsample': 0.83}, Best CV F1: 0.8221\n",
      "  Step 13 finished in 2.30s total.\n",
      "\n",
      "--- Step 14 (Predicting for window starting 2023-10-04 02:00:00) ---\n",
      "  Training window: [312:1655]; Testing window: [1656:1923]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.23s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 7, 'n_estimators': 140, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8035\n",
      "  Step 14 finished in 2.33s total.\n",
      "\n",
      "--- Step 15 (Predicting for window starting 2023-10-05 02:00:00) ---\n",
      "  Training window: [336:1679]; Testing window: [1680:1947]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.15s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 7, 'n_estimators': 105, 'reg_alpha': 0.14, 'subsample': 0.83}, Best CV F1: 0.7738\n",
      "  Step 15 finished in 2.21s total.\n",
      "\n",
      "--- Step 16 (Predicting for window starting 2023-10-06 02:00:00) ---\n",
      "  Training window: [360:1703]; Testing window: [1704:1971]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.20s.\n",
      "  Best Params: {'colsample_bytree': 0.75, 'max_depth': 6, 'n_estimators': 140, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8079\n",
      "  Step 16 finished in 2.29s total.\n",
      "\n",
      "--- Step 17 (Predicting for window starting 2023-10-07 02:00:00) ---\n",
      "  Training window: [384:1727]; Testing window: [1728:1995]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.20s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 140, 'reg_alpha': 0.14, 'subsample': 0.83}, Best CV F1: 0.8342\n",
      "  Step 17 finished in 2.29s total.\n",
      "\n",
      "--- Step 18 (Predicting for window starting 2023-10-08 02:00:00) ---\n",
      "  Training window: [408:1751]; Testing window: [1752:2019]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.55s.\n",
      "  Best Params: {'colsample_bytree': 0.75, 'max_depth': 6, 'n_estimators': 140, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8092\n",
      "  Step 18 finished in 2.63s total.\n",
      "\n",
      "--- Step 19 (Predicting for window starting 2023-10-09 02:00:00) ---\n",
      "  Training window: [432:1775]; Testing window: [1776:2043]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.21s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 105, 'reg_alpha': 0.26, 'subsample': 0.83}, Best CV F1: 0.8115\n",
      "  Step 19 finished in 2.28s total.\n",
      "\n",
      "--- Step 20 (Predicting for window starting 2023-10-10 02:00:00) ---\n",
      "  Training window: [456:1799]; Testing window: [1800:2067]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.15s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 105, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8471\n",
      "  Step 20 finished in 2.22s total.\n",
      "\n",
      "--- Step 21 (Predicting for window starting 2023-10-11 02:00:00) ---\n",
      "  Training window: [480:1823]; Testing window: [1824:2091]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.21s.\n",
      "  Best Params: {'colsample_bytree': 0.75, 'max_depth': 7, 'n_estimators': 140, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.7990\n",
      "  Step 21 finished in 2.30s total.\n",
      "\n",
      "--- Step 22 (Predicting for window starting 2023-10-12 02:00:00) ---\n",
      "  Training window: [504:1847]; Testing window: [1848:2115]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.14s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 7, 'n_estimators': 105, 'reg_alpha': 0.26, 'subsample': 0.83}, Best CV F1: 0.7735\n",
      "  Step 22 finished in 2.21s total.\n",
      "\n",
      "--- Step 23 (Predicting for window starting 2023-10-13 02:00:00) ---\n",
      "  Training window: [528:1871]; Testing window: [1872:2139]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.03s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 105, 'reg_alpha': 0.26, 'subsample': 0.92}, Best CV F1: 0.6049\n",
      "  Step 23 finished in 2.10s total.\n",
      "\n",
      "--- Step 24 (Predicting for window starting 2023-10-14 02:00:00) ---\n",
      "  Training window: [552:1895]; Testing window: [1896:2163]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.09s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 140, 'reg_alpha': 0.14, 'subsample': 0.83}, Best CV F1: 0.7010\n",
      "  Step 24 finished in 2.17s total.\n",
      "\n",
      "--- Step 25 (Predicting for window starting 2023-10-15 02:00:00) ---\n",
      "  Training window: [576:1919]; Testing window: [1920:2187]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.05s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 105, 'reg_alpha': 0.26, 'subsample': 0.83}, Best CV F1: 0.6003\n",
      "  Step 25 finished in 2.11s total.\n",
      "\n",
      "--- Step 26 (Predicting for window starting 2023-10-16 02:00:00) ---\n",
      "  Training window: [600:1943]; Testing window: [1944:2211]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.02s.\n",
      "  Best Params: {'colsample_bytree': 0.75, 'max_depth': 6, 'n_estimators': 105, 'reg_alpha': 0.14, 'subsample': 0.83}, Best CV F1: 0.6324\n",
      "  Step 26 finished in 2.09s total.\n",
      "\n",
      "--- Step 27 (Predicting for window starting 2023-10-17 02:00:00) ---\n",
      "  Training window: [624:1967]; Testing window: [1968:2235]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.06s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 105, 'reg_alpha': 0.14, 'subsample': 0.83}, Best CV F1: 0.6339\n",
      "  Step 27 finished in 2.13s total.\n",
      "\n",
      "--- Step 28 (Predicting for window starting 2023-10-18 02:00:00) ---\n",
      "  Training window: [648:1991]; Testing window: [1992:2259]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.08s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 7, 'n_estimators': 105, 'reg_alpha': 0.14, 'subsample': 0.83}, Best CV F1: 0.6461\n",
      "  Step 28 finished in 2.15s total.\n",
      "\n",
      "--- Step 29 (Predicting for window starting 2023-10-19 02:00:00) ---\n",
      "  Training window: [672:2015]; Testing window: [2016:2283]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.14s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 105, 'reg_alpha': 0.14, 'subsample': 0.83}, Best CV F1: 0.6746\n",
      "  Step 29 finished in 2.21s total.\n",
      "\n",
      "--- Step 30 (Predicting for window starting 2023-10-20 02:00:00) ---\n",
      "  Training window: [696:2039]; Testing window: [2040:2307]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.07s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 7, 'n_estimators': 140, 'reg_alpha': 0.14, 'subsample': 0.83}, Best CV F1: 0.6737\n",
      "  Step 30 finished in 2.16s total.\n",
      "\n",
      "--- Step 31 (Predicting for window starting 2023-10-21 02:00:00) ---\n",
      "  Training window: [720:2063]; Testing window: [2064:2331]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.09s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 140, 'reg_alpha': 0.26, 'subsample': 0.83}, Best CV F1: 0.6302\n",
      "  Step 31 finished in 2.17s total.\n",
      "\n",
      "--- Step 32 (Predicting for window starting 2023-10-22 02:00:00) ---\n",
      "  Training window: [744:2087]; Testing window: [2088:2355]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.09s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 7, 'n_estimators': 105, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.6477\n",
      "  Step 32 finished in 2.16s total.\n",
      "\n",
      "--- Step 33 (Predicting for window starting 2023-10-23 02:00:00) ---\n",
      "  Training window: [768:2111]; Testing window: [2112:2379]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.09s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 140, 'reg_alpha': 0.26, 'subsample': 0.92}, Best CV F1: 0.6348\n",
      "  Step 33 finished in 2.17s total.\n",
      "\n",
      "--- Step 34 (Predicting for window starting 2023-10-24 02:00:00) ---\n",
      "  Training window: [792:2135]; Testing window: [2136:2403]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.06s.\n",
      "  Best Params: {'colsample_bytree': 0.75, 'max_depth': 6, 'n_estimators': 105, 'reg_alpha': 0.26, 'subsample': 0.92}, Best CV F1: 0.6340\n",
      "  Step 34 finished in 2.13s total.\n",
      "\n",
      "--- Step 35 (Predicting for window starting 2023-10-25 02:00:00) ---\n",
      "  Training window: [816:2159]; Testing window: [2160:2427]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.04s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 140, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.6731\n",
      "  Step 35 finished in 2.12s total.\n",
      "\n",
      "--- Step 36 (Predicting for window starting 2023-10-26 02:00:00) ---\n",
      "  Training window: [840:2183]; Testing window: [2184:2451]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 1.95s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 140, 'reg_alpha': 0.26, 'subsample': 0.83}, Best CV F1: 0.5551\n",
      "  Step 36 finished in 2.02s total.\n",
      "\n",
      "--- Step 37 (Predicting for window starting 2023-10-27 02:00:00) ---\n",
      "  Training window: [864:2207]; Testing window: [2208:2475]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 1.64s.\n",
      "  Best Params: {'colsample_bytree': 0.75, 'max_depth': 6, 'n_estimators': 105, 'reg_alpha': 0.14, 'subsample': 0.83}, Best CV F1: 0.1481\n",
      "  Step 37 finished in 1.70s total.\n",
      "\n",
      "--- Step 38 (Predicting for window starting 2023-10-28 02:00:00) ---\n",
      "  Training window: [888:2231]; Testing window: [2232:2499]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 1.78s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 140, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.2738\n",
      "  Step 38 finished in 1.85s total.\n",
      "\n",
      "--- Step 39 (Predicting for window starting 2023-10-29 02:00:00) ---\n",
      "  Training window: [912:2255]; Testing window: [2256:2523]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 1.75s.\n",
      "  Best Params: {'colsample_bytree': 0.75, 'max_depth': 6, 'n_estimators': 105, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.3056\n",
      "  Step 39 finished in 1.81s total.\n",
      "\n",
      "--- Step 40 (Predicting for window starting 2023-10-30 02:00:00) ---\n",
      "  Training window: [936:2279]; Testing window: [2280:2547]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 1.73s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 105, 'reg_alpha': 0.14, 'subsample': 0.83}, Best CV F1: 0.1333\n",
      "  Step 40 finished in 1.80s total.\n",
      "\n",
      "--- Step 41 (Predicting for window starting 2023-10-31 02:00:00) ---\n",
      "  Training window: [960:2303]; Testing window: [2304:2571]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 1.76s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 105, 'reg_alpha': 0.14, 'subsample': 0.83}, Best CV F1: 0.2407\n",
      "  Step 41 finished in 1.82s total.\n",
      "\n",
      "--- Step 42 (Predicting for window starting 2023-11-01 02:00:00) ---\n",
      "  Training window: [984:2327]; Testing window: [2328:2595]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 1.75s.\n",
      "  Best Params: {'colsample_bytree': 0.75, 'max_depth': 6, 'n_estimators': 105, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.1500\n",
      "  Step 42 finished in 1.82s total.\n",
      "\n",
      "--- Step 43 (Predicting for window starting 2023-11-02 02:00:00) ---\n",
      "  Training window: [1008:2351]; Testing window: [2352:2619]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 1.73s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 105, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.1574\n",
      "  Step 43 finished in 1.79s total.\n",
      "\n",
      "--- Step 44 (Predicting for window starting 2023-11-03 02:00:00) ---\n",
      "  Training window: [1032:2375]; Testing window: [2376:2643]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 1.82s.\n",
      "  Best Params: {'colsample_bytree': 0.75, 'max_depth': 6, 'n_estimators': 105, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.4686\n",
      "  Step 44 finished in 1.89s total.\n",
      "\n",
      "--- Step 45 (Predicting for window starting 2023-11-04 02:00:00) ---\n",
      "  Training window: [1056:2399]; Testing window: [2400:2667]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 1.82s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 105, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.4266\n",
      "  Step 45 finished in 1.88s total.\n",
      "\n",
      "--- Step 46 (Predicting for window starting 2023-11-05 02:00:00) ---\n",
      "  Training window: [1080:2423]; Testing window: [2424:2691]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 1.86s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 140, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.2361\n",
      "  Step 46 finished in 1.95s total.\n",
      "\n",
      "--- Step 47 (Predicting for window starting 2023-11-06 02:00:00) ---\n",
      "  Training window: [1104:2447]; Testing window: [2448:2715]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 1.68s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 140, 'reg_alpha': 0.14, 'subsample': 0.83}, Best CV F1: 0.4315\n",
      "  Step 47 finished in 1.76s total.\n",
      "\n",
      "--- Step 48 (Predicting for window starting 2023-11-07 02:00:00) ---\n",
      "  Training window: [1128:2471]; Testing window: [2472:2739]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 1.69s.\n",
      "  Best Params: {'colsample_bytree': 0.75, 'max_depth': 6, 'n_estimators': 105, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.4611\n",
      "  Step 48 finished in 1.75s total.\n",
      "\n",
      "--- Step 49 (Predicting for window starting 2023-11-08 02:00:00) ---\n",
      "  Training window: [1152:2495]; Testing window: [2496:2763]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 1.64s.\n",
      "  Best Params: {'colsample_bytree': 0.75, 'max_depth': 6, 'n_estimators': 105, 'reg_alpha': 0.26, 'subsample': 0.83}, Best CV F1: 0.5037\n",
      "  Step 49 finished in 1.70s total.\n",
      "\n",
      "--- Step 50 (Predicting for window starting 2023-11-09 02:00:00) ---\n",
      "  Training window: [1176:2519]; Testing window: [2520:2787]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 1.67s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 140, 'reg_alpha': 0.26, 'subsample': 0.83}, Best CV F1: 0.4512\n",
      "  Step 50 finished in 1.74s total.\n",
      "\n",
      "--- Step 51 (Predicting for window starting 2023-11-10 02:00:00) ---\n",
      "  Training window: [1200:2543]; Testing window: [2544:2811]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 1.60s.\n",
      "  Best Params: {'colsample_bytree': 0.75, 'max_depth': 6, 'n_estimators': 140, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.4818\n",
      "  Step 51 finished in 1.68s total.\n",
      "\n",
      "--- Step 52 (Predicting for window starting 2023-11-11 02:00:00) ---\n",
      "  Training window: [1224:2567]; Testing window: [2568:2835]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 1.63s.\n",
      "  Best Params: {'colsample_bytree': 0.75, 'max_depth': 6, 'n_estimators': 140, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.4370\n",
      "  Step 52 finished in 1.70s total.\n",
      "\n",
      "--- Step 53 (Predicting for window starting 2023-11-12 02:00:00) ---\n",
      "  Training window: [1248:2591]; Testing window: [2592:2859]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 1.60s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 105, 'reg_alpha': 0.26, 'subsample': 0.83}, Best CV F1: 0.3205\n",
      "  Step 53 finished in 1.65s total.\n",
      "\n",
      "--- Step 54 (Predicting for window starting 2023-11-13 02:00:00) ---\n",
      "  Training window: [1272:2615]; Testing window: [2616:2883]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 1.61s.\n",
      "  Best Params: {'colsample_bytree': 0.75, 'max_depth': 6, 'n_estimators': 140, 'reg_alpha': 0.26, 'subsample': 0.83}, Best CV F1: 0.3152\n",
      "  Step 54 finished in 1.68s total.\n",
      "\n",
      "--- Step 55 (Predicting for window starting 2023-11-14 02:00:00) ---\n",
      "  Training window: [1296:2639]; Testing window: [2640:2907]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 1.74s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 105, 'reg_alpha': 0.26, 'subsample': 0.92}, Best CV F1: 0.4307\n",
      "  Step 55 finished in 1.80s total.\n",
      "\n",
      "--- Step 56 (Predicting for window starting 2023-11-15 02:00:00) ---\n",
      "  Training window: [1320:2663]; Testing window: [2664:2931]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.00s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 140, 'reg_alpha': 0.26, 'subsample': 0.83}, Best CV F1: 0.6476\n",
      "  Step 56 finished in 2.07s total.\n",
      "\n",
      "--- Step 57 (Predicting for window starting 2023-11-16 02:00:00) ---\n",
      "  Training window: [1344:2687]; Testing window: [2688:2955]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 1.85s.\n",
      "  Best Params: {'colsample_bytree': 0.75, 'max_depth': 6, 'n_estimators': 105, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.6242\n",
      "  Step 57 finished in 1.92s total.\n",
      "\n",
      "--- Step 58 (Predicting for window starting 2023-11-17 02:00:00) ---\n",
      "  Training window: [1368:2711]; Testing window: [2712:2979]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 1.89s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 105, 'reg_alpha': 0.26, 'subsample': 0.92}, Best CV F1: 0.6486\n",
      "  Step 58 finished in 1.96s total.\n",
      "\n",
      "--- Step 59 (Predicting for window starting 2023-11-18 02:00:00) ---\n",
      "  Training window: [1392:2735]; Testing window: [2736:3003]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 1.91s.\n",
      "  Best Params: {'colsample_bytree': 0.75, 'max_depth': 6, 'n_estimators': 105, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.5230\n",
      "  Step 59 finished in 1.98s total.\n",
      "\n",
      "--- Step 60 (Predicting for window starting 2023-11-19 02:00:00) ---\n",
      "  Training window: [1416:2759]; Testing window: [2760:3027]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 1.95s.\n",
      "  Best Params: {'colsample_bytree': 0.75, 'max_depth': 6, 'n_estimators': 105, 'reg_alpha': 0.26, 'subsample': 0.92}, Best CV F1: 0.6266\n",
      "  Step 60 finished in 2.01s total.\n",
      "\n",
      "--- Step 61 (Predicting for window starting 2023-11-20 02:00:00) ---\n",
      "  Training window: [1440:2783]; Testing window: [2784:3051]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 1.97s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 105, 'reg_alpha': 0.26, 'subsample': 0.92}, Best CV F1: 0.6822\n",
      "  Step 61 finished in 2.04s total.\n",
      "\n",
      "--- Step 62 (Predicting for window starting 2023-11-21 02:00:00) ---\n",
      "  Training window: [1464:2807]; Testing window: [2808:3075]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 1.98s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 7, 'n_estimators': 105, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.6266\n",
      "  Step 62 finished in 2.07s total.\n",
      "\n",
      "--- Step 63 (Predicting for window starting 2023-11-22 02:00:00) ---\n",
      "  Training window: [1488:2831]; Testing window: [2832:3099]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.13s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 105, 'reg_alpha': 0.26, 'subsample': 0.92}, Best CV F1: 0.6216\n",
      "  Step 63 finished in 2.20s total.\n",
      "\n",
      "--- Step 64 (Predicting for window starting 2023-11-23 02:00:00) ---\n",
      "  Training window: [1512:2855]; Testing window: [2856:3123]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.10s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 140, 'reg_alpha': 0.26, 'subsample': 0.83}, Best CV F1: 0.5850\n",
      "  Step 64 finished in 2.18s total.\n",
      "\n",
      "--- Step 65 (Predicting for window starting 2023-11-24 02:00:00) ---\n",
      "  Training window: [1536:2879]; Testing window: [2880:3147]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.11s.\n",
      "  Best Params: {'colsample_bytree': 0.75, 'max_depth': 7, 'n_estimators': 140, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.6008\n",
      "  Step 65 finished in 2.19s total.\n",
      "\n",
      "--- Step 66 (Predicting for window starting 2023-11-25 02:00:00) ---\n",
      "  Training window: [1560:2903]; Testing window: [2904:3171]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.10s.\n",
      "  Best Params: {'colsample_bytree': 0.75, 'max_depth': 6, 'n_estimators': 105, 'reg_alpha': 0.26, 'subsample': 0.92}, Best CV F1: 0.5833\n",
      "  Step 66 finished in 2.16s total.\n",
      "\n",
      "--- Step 67 (Predicting for window starting 2023-11-26 02:00:00) ---\n",
      "  Training window: [1584:2927]; Testing window: [2928:3195]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.08s.\n",
      "  Best Params: {'colsample_bytree': 0.75, 'max_depth': 6, 'n_estimators': 105, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.6518\n",
      "  Step 67 finished in 2.17s total.\n",
      "\n",
      "--- Step 68 (Predicting for window starting 2023-11-27 02:00:00) ---\n",
      "  Training window: [1608:2951]; Testing window: [2952:3219]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.19s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 105, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.6979\n",
      "  Step 68 finished in 2.26s total.\n",
      "\n",
      "--- Step 69 (Predicting for window starting 2023-11-28 02:00:00) ---\n",
      "  Training window: [1632:2975]; Testing window: [2976:3243]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 257\u001b[0m\n\u001b[0;32m    251\u001b[0m estimator \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBClassifier(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mXGB_FIXED_PARAMS)\n\u001b[0;32m    252\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[0;32m    253\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator, param_grid\u001b[38;5;241m=\u001b[39mXGB_PARAM_GRID_TUNE, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    254\u001b[0m     cv\u001b[38;5;241m=\u001b[39mStratifiedKFold(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mi),\n\u001b[0;32m    255\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    256\u001b[0m )\n\u001b[1;32m--> 257\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_roll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_roll\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Fit GridSearch\u001b[39;00m\n\u001b[0;32m    258\u001b[0m best_params_step \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_params_\n\u001b[0;32m    259\u001b[0m best_score_step \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_score_\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\model_selection\\_search.py:1422\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1421\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1422\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\model_selection\\_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m     )\n\u001b[1;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# B3_Configurable.py\n",
    "# Simple Predictor with Sliding Window, Per-Step HParam Tuning, and PTT\n",
    "# Configurable parameters at the top. Uses an evaluation window per step.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Modeling Imports\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, ParameterGrid\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "# --- Suppress Warnings ---\n",
    "warnings.filterwarnings('ignore', category=UndefinedMetricWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==============================================================================\n",
    "# --- Configuration ---\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Data ---\n",
    "CSV_FILE_PATH = 'BTCUSD.csv'\n",
    "N_ROWS_INPUT = 15000          # Number of most recent rows to load from CSV\n",
    "\n",
    "# --- Target Definition ---\n",
    "PREDICTION_WINDOW_HOURS = 24  # How many hours ahead to predict\n",
    "TARGET_THRESHOLD_PCT = -2.5   # Target threshold for positive class (>= this value is 1)\n",
    "\n",
    "# --- Backtesting Windowing ---\n",
    "TRAIN_WINDOW_HOURS = 24 * 7 * 8  # Size of the sliding training window (e.g., 8 weeks)\n",
    "STEP_HOURS = 24                  # How often to retrain/predict (e.g., daily)\n",
    "# Test window size as a fraction of the training window\n",
    "TEST_WINDOW_FRACTION = 0.2       # e.g., 0.2 means test window is 20% of train window\n",
    "\n",
    "# --- Model & Tuning ---\n",
    "# Fixed XGBoost parameters (not tuned in grid search)\n",
    "XGB_FIXED_PARAMS = {\n",
    "    'learning_rate': 0.086,\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'use_label_encoder': False, # Deprecated, use False\n",
    "    'random_state': 42,\n",
    "    'reg_lambda': 4.0,         # L2 Regularization (Example value)\n",
    "    'n_jobs': -1               # Use all available CPU cores\n",
    "}\n",
    "\n",
    "# Parameter grid for GridSearchCV (Keep combinations reasonable)\n",
    "XGB_PARAM_GRID_TUNE = {\n",
    "    'max_depth': [6, 7, 9],          # 3 options\n",
    "    'n_estimators': [105, 140],       # 2 options\n",
    "    'subsample': [0.83, 0.92],         # 2 options\n",
    "    'colsample_bytree': [0.68, 0.75],  # 2 options\n",
    "    'reg_alpha': [0.14, 0.26]         # L1 Regularization (2 options)\n",
    "} # Total combinations: 3 * 2 * 2 * 2 * 2 = 48\n",
    "\n",
    "# Probability Threshold Tuning Range\n",
    "PROBABILITY_THRESHOLD_RANGE = (0.10, 0.90) # Start to end (end exclusive)\n",
    "PROBABILITY_THRESHOLD_STEP = 0.05\n",
    "\n",
    "# ==============================================================================\n",
    "# --- Derived Variables (Do not change these directly) ---\n",
    "# ==============================================================================\n",
    "TRAIN_WINDOW_ROWS = TRAIN_WINDOW_HOURS\n",
    "STEP_ROWS = STEP_HOURS\n",
    "PREDICTION_WINDOW_ROWS = PREDICTION_WINDOW_HOURS\n",
    "TEST_WINDOW_ROWS = max(1, int(TEST_WINDOW_FRACTION * TRAIN_WINDOW_ROWS)) # Ensure at least 1 row\n",
    "THRESHOLD_SEARCH_RANGE = np.arange(\n",
    "    PROBABILITY_THRESHOLD_RANGE[0],\n",
    "    PROBABILITY_THRESHOLD_RANGE[1],\n",
    "    PROBABILITY_THRESHOLD_STEP\n",
    ")\n",
    "try:\n",
    "    grid_combinations = len(list(ParameterGrid(XGB_PARAM_GRID_TUNE))) # Calculate grid size\n",
    "except TypeError: # Handle case where grid might be None or empty\n",
    "    grid_combinations = 1\n",
    "\n",
    "# ==============================================================================\n",
    "# --- Script Start ---\n",
    "# ==============================================================================\n",
    "\n",
    "# --- 1. Load Data and Select Last Rows ---\n",
    "print(\"--- Data Loading ---\")\n",
    "print(f\"Loading data from: {CSV_FILE_PATH}\")\n",
    "try:\n",
    "    df_full = pd.read_csv(CSV_FILE_PATH)\n",
    "    df_full = df_full.sort_values(by='unix', ascending=True).reset_index(drop=True)\n",
    "    if 'date' in df_full.columns:\n",
    "        try: df_full['date'] = pd.to_datetime(df_full['date'])\n",
    "        except Exception as e_date: print(f\"Warning: Date parse error: {e_date}\")\n",
    "\n",
    "    if len(df_full) < N_ROWS_INPUT:\n",
    "        print(f\"Warning: Full dataset ({len(df_full)} rows) < {N_ROWS_INPUT}. Using all data.\")\n",
    "        df = df_full.copy()\n",
    "    else:\n",
    "        df = df_full.iloc[-N_ROWS_INPUT:].reset_index(drop=True)\n",
    "        print(f\"Selected last {len(df)} rows.\")\n",
    "except FileNotFoundError: print(f\"Error: {CSV_FILE_PATH} not found.\"); exit()\n",
    "except Exception as e: print(f\"Error loading data: {e}\"); exit()\n",
    "\n",
    "# --- 2. Feature Engineering ---\n",
    "print(\"\\n--- Feature Engineering ---\")\n",
    "start_fe = time.time()\n",
    "base_cols_numeric = ['open', 'high', 'low', 'close', 'Volume BTC', 'Volume USD']\n",
    "for col in base_cols_numeric:\n",
    "    if col in df.columns: df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    else: print(f\"Warning: Missing base column '{col}'\"); df[col] = 0\n",
    "if df[['open', 'high', 'low', 'close']].isnull().any().any():\n",
    "    print(\"Warning: OHLC NaNs found. Dropping rows.\"); df = df.dropna(subset=['open', 'high', 'low', 'close'])\n",
    "if df.empty: exit(\"Error: Empty DF after initial OHLC NaN drop.\")\n",
    "# --- Feature Functions ---\n",
    "def garman_klass_volatility(o, h, l, c, w):\n",
    "    with np.errstate(divide='ignore', invalid='ignore'): log_hl=np.log(h/l.replace(0,np.nan)); log_co=np.log(c/o.replace(0,np.nan))\n",
    "    gk = 0.5*(log_hl**2) - (2*np.log(2)-1)*(log_co**2); gk = gk.fillna(0)\n",
    "    rm = gk.rolling(w, min_periods=max(1,w//2)).mean(); rm = rm.clip(lower=0); return np.sqrt(rm)\n",
    "def parkinson_volatility(h, l, w):\n",
    "    with np.errstate(divide='ignore', invalid='ignore'): log_hl_sq = np.log(h/l.replace(0,np.nan))**2\n",
    "    log_hl_sq = log_hl_sq.fillna(0); rs = log_hl_sq.rolling(w,min_periods=max(1,w//2)).sum()\n",
    "    f = 1/(4*np.log(2)*w) if w>0 else 0; return np.sqrt(f*rs)\n",
    "# --- Feature Calculations (Example subset, add all relevant ones from B2_V2) ---\n",
    "df['price_change_1h_temp'] = df['close'].pct_change()\n",
    "with np.errstate(divide='ignore', invalid='ignore'):\n",
    "    df['price_range_pct'] = (df['high'] - df['low']) / df['close'].replace(0, np.nan) * 100\n",
    "    df['oc_change_pct'] = (df['close'] - df['open']) / df['open'].replace(0, np.nan) * 100\n",
    "df['garman_klass_12h'] = garman_klass_volatility(df['open'],df['high'],df['low'],df['close'],12)\n",
    "df['parkinson_3h'] = parkinson_volatility(df['high'],df['low'],3)\n",
    "min_periods_rolling = 2\n",
    "df['ma_3h'] = df['close'].rolling(3, min_periods=min_periods_rolling).mean()\n",
    "df['rolling_std_3h'] = df['close'].rolling(3, min_periods=min_periods_rolling).std()\n",
    "lag_periods_price = [3, 6, 12, 24, 48, 72, 168]; lag_periods_volume = [3, 6, 12, 24]\n",
    "for lag in lag_periods_price: df[f'lag_{lag}h_price_return'] = df['price_change_1h_temp'].shift(lag) * 100\n",
    "df['volume_return_1h'] = df['Volume BTC'].pct_change() * 100\n",
    "for lag in lag_periods_volume: df[f'lag_{lag}h_volume_return'] = df['volume_return_1h'].shift(lag)\n",
    "ma_periods = [6, 12, 24, 48, 72, 168]; std_periods = [6, 12, 24, 48, 72, 168]\n",
    "for p in ma_periods: df[f'ma_{p}h'] = df['close'].rolling(p, min_periods=max(min_periods_rolling, p//2)).mean()\n",
    "for p in std_periods: df[f'rolling_std_{p}h'] = df['price_change_1h_temp'].rolling(p, min_periods=max(min_periods_rolling, p//2)).std() * 100\n",
    "df['prev_close']=df['close'].shift(1); df['hml']=df['high']-df['low']; df['hmpc']=np.abs(df['high']-df['prev_close']); df['lmpc']=np.abs(df['low']-df['prev_close'])\n",
    "df['tr']=df[['hml','hmpc','lmpc']].max(axis=1)\n",
    "atr_periods = [14, 24, 48]\n",
    "for p in atr_periods: df[f'atr_{p}h'] = df['tr'].rolling(p, min_periods=max(1,p//2)).mean()\n",
    "df = df.drop(columns=['prev_close', 'hml', 'hmpc', 'lmpc', 'tr'])\n",
    "epsilon = 1e-9\n",
    "for p in [24, 48, 168]:\n",
    "    mc=f'ma_{p}h'; df[f'close_div_ma_{p}h'] = df['close']/(df[mc]+epsilon) if mc in df else np.nan\n",
    "if 'ma_12h' in df and 'ma_48h' in df: df['ma12_div_ma48'] = df['ma_12h']/(df['ma_48h']+epsilon)\n",
    "else: df['ma12_div_ma48']=np.nan\n",
    "if 'ma_24h' in df and 'ma_168h' in df: df['ma24_div_ma168'] = df['ma_24h']/(df['ma_168h']+epsilon)\n",
    "else: df['ma24_div_ma168']=np.nan\n",
    "if 'rolling_std_12h' in df and 'rolling_std_72h' in df: df['std12_div_std72'] = df['rolling_std_12h']/(df['rolling_std_72h']+epsilon)\n",
    "else: df['std12_div_std72']=np.nan\n",
    "if 'price_range_pct' in df: df['volume_btc_x_range'] = df['Volume BTC'] * df['price_range_pct']\n",
    "else: df['volume_btc_x_range']=np.nan\n",
    "if 'rolling_std_3h' in df: df['rolling_std_3h_sq'] = df['rolling_std_3h']**2\n",
    "else: df['rolling_std_3h_sq']=np.nan\n",
    "if 'price_change_1h_temp' in df: df['price_return_1h_sq'] = df['price_change_1h_temp']**2 * 10000\n",
    "else: df['price_return_1h_sq']=np.nan\n",
    "if 'rolling_std_12h' in df: df['rolling_std_12h_sqrt'] = np.sqrt(df['rolling_std_12h'].clip(lower=0)+epsilon)\n",
    "else: df['rolling_std_12h_sqrt']=np.nan\n",
    "cols_to_drop_intermediate = ['price_change_1h_temp', 'volume_return_1h']\n",
    "df = df.drop(columns=[col for col in cols_to_drop_intermediate if col in df.columns])\n",
    "# Add 'symbol' column (if not already present from CSV header)\n",
    "if 'symbol' not in df.columns: df['symbol'] = 'BTCUSD' # Example\n",
    "print(f\"Feature engineering complete. Took {time.time() - start_fe:.2f} seconds.\")\n",
    "print(f\"Columns after features: {df.shape[1]}\")\n",
    "\n",
    "# --- 3. Define Target Variable ---\n",
    "print(\"\\n--- Target Definition ---\")\n",
    "print(f\"Defining target as {PREDICTION_WINDOW_HOURS}h future return >= {TARGET_THRESHOLD_PCT}%...\")\n",
    "target_col = f'target_return_{PREDICTION_WINDOW_HOURS}h' # Dynamic target column name\n",
    "df[target_col] = df['close'].shift(-PREDICTION_WINDOW_ROWS).sub(df['close']).div(df['close'].replace(0, np.nan)).mul(100)\n",
    "\n",
    "# --- 4. Prepare Data for Modeling ---\n",
    "print(\"\\n--- Data Preparation ---\")\n",
    "cols_to_keep_final = ['unix', 'date', target_col, 'symbol']\n",
    "potential_feature_cols = [col for col in df.columns if col not in cols_to_keep_final]\n",
    "numeric_feature_cols = df[potential_feature_cols].select_dtypes(include=np.number).columns.tolist()\n",
    "final_feature_cols = [col for col in numeric_feature_cols if col not in base_cols_numeric]\n",
    "cols_to_select = final_feature_cols + [col for col in cols_to_keep_final if col in df.columns]\n",
    "df_model_ready = df[cols_to_select].copy()\n",
    "\n",
    "initial_rows = len(df_model_ready); df_model_ready = df_model_ready.dropna(); final_rows = len(df_model_ready)\n",
    "print(f\"NaN Handling: Dropped {initial_rows - final_rows} rows.\")\n",
    "\n",
    "numeric_cols_final = df_model_ready[final_feature_cols].select_dtypes(include=np.number).columns.tolist()\n",
    "inf_mask = np.isinf(df_model_ready[numeric_cols_final]); inf_count = inf_mask.sum().sum()\n",
    "if inf_count > 0:\n",
    "    print(f\"Replacing {inf_count} infinites...\"); df_model_ready.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    rows_b4 = len(df_model_ready); df_model_ready = df_model_ready.dropna(); print(f\"Dropped {rows_b4 - len(df_model_ready)} more rows.\")\n",
    "if df_model_ready.empty: exit(\"Error: DataFrame empty after NaN/Inf handling.\")\n",
    "\n",
    "X = df_model_ready[final_feature_cols]\n",
    "y_binary = (df_model_ready[target_col] <= TARGET_THRESHOLD_PCT).astype(int)                                                                    ### ADJUST < or >\n",
    "\n",
    "if 'date' in df_model_ready.columns and pd.api.types.is_datetime64_any_dtype(df_model_ready['date']): timestamps = df_model_ready['date']\n",
    "elif 'unix' in df_model_ready.columns: timestamps = pd.to_datetime(df_model_ready['unix'], unit='ms')\n",
    "else: print(\"Warning: No date/unix. Using index.\"); timestamps = pd.Series(df_model_ready.index)\n",
    "\n",
    "print(f\"Final feature matrix shape: {X.shape}\"); print(f\"Target vector shape: {y_binary.shape}\"); print(f\"Using {len(final_feature_cols)} features.\")\n",
    "\n",
    "# --- 5. SLIDING Window Backtesting with Per-Step HParam Tuning ---\n",
    "print(\"\\n--- Starting SLIDING Window Backtest with Per-Step HParam Tuning ---\")\n",
    "print(\"!!! WARNING: This will be significantly slower due to GridSearchCV in each step !!!\")\n",
    "if len(X) < TRAIN_WINDOW_ROWS + STEP_ROWS:\n",
    "    print(f\"Error: Not enough data ({len(X)}) for train window ({TRAIN_WINDOW_ROWS}) + step ({STEP_ROWS}).\"); exit()\n",
    "\n",
    "all_predictions_proba = []; all_actual = []; backtest_timestamps = []\n",
    "all_best_params = [] # Store best params for each step\n",
    "num_steps = 0\n",
    "start_index_loop = TRAIN_WINDOW_ROWS; end_index_loop = len(X) - TEST_WINDOW_ROWS + 1 # Adjust end index for test window\n",
    "\n",
    "print(f\"Train Window: {TRAIN_WINDOW_ROWS} rows, Step: {STEP_ROWS} rows, Test Window: {TEST_WINDOW_ROWS} rows, Tuning Grid Size: {grid_combinations}\")\n",
    "loop_start_time = time.time()\n",
    "\n",
    "for i in range(start_index_loop, end_index_loop, STEP_ROWS):\n",
    "    step_start_time = time.time()\n",
    "    train_idx_start = i - TRAIN_WINDOW_ROWS\n",
    "    train_idx_end = i\n",
    "    test_idx_start = i\n",
    "    test_idx_end = i + TEST_WINDOW_ROWS # Define end of test window\n",
    "\n",
    "    if test_idx_end > len(X): # Ensure test window doesn't exceed data\n",
    "        print(f\"Stopping loop: Test window end ({test_idx_end}) exceeds available data ({len(X)}).\")\n",
    "        break\n",
    "\n",
    "    # Get train and test sets\n",
    "    X_train_roll = X.iloc[train_idx_start : train_idx_end]\n",
    "    y_train_roll = y_binary.iloc[train_idx_start : train_idx_end]\n",
    "    X_test_roll = X.iloc[test_idx_start : test_idx_end]  # Test on the window\n",
    "    y_test_roll_actual_series = y_binary.iloc[test_idx_start : test_idx_end] # Actual labels for the window\n",
    "    current_timestamp = timestamps.iloc[test_idx_start] # Timestamp for the start of the test window\n",
    "\n",
    "    if X_train_roll.empty or len(np.unique(y_train_roll)) < 2:\n",
    "        print(f\"Warning: Skipping step starting at index {i}. Invalid training data.\"); continue\n",
    "\n",
    "    print(f\"\\n--- Step {num_steps + 1} (Predicting for window starting {current_timestamp}) ---\")\n",
    "    print(f\"  Training window: [{train_idx_start}:{train_idx_end-1}]; Testing window: [{test_idx_start}:{test_idx_end-1}]\")\n",
    "\n",
    "    # --- Hyperparameter Tuning ---\n",
    "    print(f\"  Running GridSearchCV (cv=3, scoring='f1')...\")\n",
    "    grid_search_start_time = time.time()\n",
    "    try:\n",
    "        estimator = xgb.XGBClassifier(**XGB_FIXED_PARAMS)\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=estimator, param_grid=XGB_PARAM_GRID_TUNE, scoring='f1',\n",
    "            cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=i),\n",
    "            n_jobs=-1, verbose=0\n",
    "        )\n",
    "        grid_search.fit(X_train_roll, y_train_roll) # Fit GridSearch\n",
    "        best_params_step = grid_search.best_params_\n",
    "        best_score_step = grid_search.best_score_\n",
    "        print(f\"  GridSearchCV finished in {time.time() - grid_search_start_time:.2f}s.\")\n",
    "        print(f\"  Best Params: {best_params_step}, Best CV F1: {best_score_step:.4f}\")\n",
    "        all_best_params.append(best_params_step) # Store best params\n",
    "\n",
    "        # --- Fit final model for the step ---\n",
    "        final_model_params = {**XGB_FIXED_PARAMS, **best_params_step}\n",
    "        model_roll = xgb.XGBClassifier(**final_model_params)\n",
    "        model_roll.fit(X_train_roll, y_train_roll, verbose=False) # Fit final model\n",
    "\n",
    "        # --- Predict probabilities for the entire test window ---\n",
    "        prob_roll_window = model_roll.predict_proba(X_test_roll)[:, 1] # Get probs for all points in test window\n",
    "\n",
    "        # --- Store results ---\n",
    "        # Store ALL probabilities and actuals for the test window for later PTT evaluation\n",
    "        all_predictions_proba.extend(prob_roll_window)\n",
    "        all_actual.extend(y_test_roll_actual_series.tolist())\n",
    "        backtest_timestamps.extend(timestamps.iloc[test_idx_start : test_idx_end].tolist()) # Store all timestamps\n",
    "        num_steps += 1\n",
    "\n",
    "    except Exception as e_step:\n",
    "        print(f\"!! Error during GridSearch/Fit/Predict at step starting {i}: {e_step}\"); traceback.print_exc(); continue\n",
    "\n",
    "    step_end_time = time.time()\n",
    "    # Note: num_steps counts completed training/prediction cycles\n",
    "    print(f\"  Step {num_steps} finished in {step_end_time - step_start_time:.2f}s total.\")\n",
    "\n",
    "loop_end_time = time.time()\n",
    "print(f\"\\nBacktesting loop finished. Completed {num_steps} steps (each predicting {TEST_WINDOW_ROWS} points) in {(loop_end_time - loop_start_time)/60:.2f} minutes.\")\n",
    "\n",
    "# --- 6. Evaluate Backtesting Results with PTT ---\n",
    "# Now PTT is applied across ALL individual predictions made during the backtest\n",
    "if num_steps > 0 and len(all_predictions_proba) == len(all_actual):\n",
    "    print(\"\\n--- Evaluating Results with Probability Threshold Tuning (on all points) ---\")\n",
    "    print(f\"Threshold search range: {THRESHOLD_SEARCH_RANGE}\")\n",
    "    best_threshold = 0.5; best_f1_thresh = -1.0\n",
    "    results_per_threshold = {}\n",
    "    probabilities_np = np.array(all_predictions_proba); actual_np = np.array(all_actual)\n",
    "\n",
    "    for t in THRESHOLD_SEARCH_RANGE:\n",
    "        predictions_thresh = (probabilities_np >= t).astype(int)\n",
    "        # Handle edge cases for metrics calculation (same logic as before)\n",
    "        if np.sum(actual_np) == 0 and np.sum(predictions_thresh) == 0: acc_t, pre_t, rec_t, f1_t = 1.0, 1.0, 1.0, 1.0\n",
    "        elif np.sum(actual_np) > 0 and np.sum(predictions_thresh) == 0: acc_t = accuracy_score(actual_np, predictions_thresh); pre_t, rec_t, f1_t = 0.0, 0.0, 0.0\n",
    "        elif np.sum(actual_np) == 0 and np.sum(predictions_thresh) > 0: acc_t = accuracy_score(actual_np, predictions_thresh); pre_t, rec_t, f1_t = 0.0, 0.0, 0.0\n",
    "        else:\n",
    "             acc_t = accuracy_score(actual_np, predictions_thresh)\n",
    "             pre_t = precision_score(actual_np, predictions_thresh, zero_division=0)\n",
    "             rec_t = recall_score(actual_np, predictions_thresh, zero_division=0)\n",
    "             f1_t = f1_score(actual_np, predictions_thresh, zero_division=0)\n",
    "        results_per_threshold[round(t, 2)] = {'f1': f1_t, 'acc': acc_t, 'pre': pre_t, 'rec': rec_t}\n",
    "        if f1_t >= best_f1_thresh: best_f1_thresh = f1_t; best_threshold = t\n",
    "\n",
    "    print(f\"\\nBest Threshold found: {best_threshold:.2f} (Yielding F1 Score: {best_f1_thresh:.4f})\")\n",
    "\n",
    "    final_predictions_optimized = (probabilities_np >= best_threshold).astype(int)\n",
    "    final_accuracy = accuracy_score(actual_np, final_predictions_optimized)\n",
    "    final_precision = precision_score(actual_np, final_predictions_optimized, zero_division=0)\n",
    "    final_recall = recall_score(actual_np, final_predictions_optimized, zero_division=0)\n",
    "    final_f1 = f1_score(actual_np, final_predictions_optimized, zero_division=0)\n",
    "\n",
    "    print(\"\\n--- Final Performance Metrics (Optimized Threshold on all points) ---\")\n",
    "    print(f\"Target: {PREDICTION_WINDOW_HOURS}h return >= {TARGET_THRESHOLD_PCT}%\")\n",
    "    print(f\"Windowing: Train={TRAIN_WINDOW_ROWS} rows, Step={STEP_ROWS} rows, Test Window={TEST_WINDOW_ROWS} rows\")\n",
    "    print(f\"Total Individual Predictions Evaluated: {len(actual_np)}\")\n",
    "    print(f\"Overall Accuracy:  {final_accuracy:.4f}\")\n",
    "    print(f\"Overall Precision: {final_precision:.4f}\")\n",
    "    print(f\"Overall Recall:    {final_recall:.4f}\")\n",
    "    print(f\"Overall F1 Score:  {final_f1:.4f}\")\n",
    "\n",
    "    if 0.5 in results_per_threshold:\n",
    "        res_def = results_per_threshold[0.5]\n",
    "        print(f\"(Compare: Default 0.5 Thresh -> F1:{res_def['f1']:.4f})\")\n",
    "\n",
    "    # --- 7. Plot Cumulative Accuracy ---\n",
    "    print(\"\\nPlotting cumulative accuracy (optimized threshold)...\")\n",
    "    # Ensure backtest_timestamps matches the length of predictions/actuals\n",
    "    if len(backtest_timestamps) != len(actual_np):\n",
    "         print(f\"Warning: Timestamp length ({len(backtest_timestamps)}) mismatch with prediction length ({len(actual_np)}). Skipping plot.\")\n",
    "    else:\n",
    "        # Calculate cumulative accuracy based on OPTIMIZED predictions\n",
    "        cumulative_accuracy_list_optimized = (np.cumsum(final_predictions_optimized == actual_np) / np.arange(1, len(actual_np) + 1))\n",
    "        try:\n",
    "            plt.figure(figsize=(14, 7))\n",
    "            plt.plot(backtest_timestamps, cumulative_accuracy_list_optimized, marker='.', linestyle='-', markersize=2, label='Cumulative Accuracy (Optimized)') # Smaller markersize\n",
    "            rolling_window_plot = min(max(len(actual_np) // 10, 50), 500) # Adjust rolling window size\n",
    "            if len(actual_np) > rolling_window_plot:\n",
    "                 # Create a DataFrame for easier rolling calculation with time index\n",
    "                 results_df = pd.DataFrame({'actual': actual_np, 'pred': final_predictions_optimized}, index=pd.to_datetime(backtest_timestamps))\n",
    "                 results_df['correct'] = (results_df['actual'] == results_df['pred']).astype(int)\n",
    "                 rolling_acc = results_df['correct'].rolling(window=f'{rolling_window_plot}H').mean() # Use time window if possible, else row window\n",
    "                 # rolling_acc = pd.Series(cumulative_accuracy_list_optimized).rolling(window=rolling_window_plot).mean() # Alternative row-based rolling\n",
    "\n",
    "                 # Plot rolling accuracy, ensuring index alignment\n",
    "                 plt.plot(rolling_acc.index, rolling_acc, linestyle='--', color='red', label=f'Rolling Acc ({rolling_window_plot} points/hours)')\n",
    "\n",
    "            plt.title(f'B3 Backtest (Train:{TRAIN_WINDOW_ROWS}, Step:{STEP_ROWS}, Test:{TEST_WINDOW_ROWS}, Tuned) - Best Thresh: {best_threshold:.2f}')\n",
    "            plt.xlabel('Timestamp'); plt.ylabel('Accuracy')\n",
    "            min_y_plot = max(0.0, np.min(cumulative_accuracy_list_optimized) - 0.05 if len(cumulative_accuracy_list_optimized)>0 else 0.4)\n",
    "            max_y_plot = min(1.0, np.max(cumulative_accuracy_list_optimized) + 0.05 if len(cumulative_accuracy_list_optimized)>0 else 0.8)\n",
    "            plt.ylim(min_y_plot, max_y_plot)\n",
    "            plt.grid(True, linestyle='--', alpha=0.6); plt.legend(); plt.xticks(rotation=30, ha='right'); plt.tight_layout(); plt.show()\n",
    "        except Exception as e_plot: print(f\"Error plotting: {e_plot}\")\n",
    "\n",
    "else:\n",
    "    print(\"No predictions were made/stored, cannot evaluate or plot.\")\n",
    "\n",
    "print(\"\\nScript B3_Configurable.py finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['open', 'high', 'low', 'close', 'Volume BTC', 'Volume USD',\n",
       "       'price_range_pct', 'oc_change_pct', 'garman_klass_12h', 'parkinson_3h',\n",
       "       'ma_3h', 'rolling_std_3h', 'lag_3h_price_return', 'lag_6h_price_return',\n",
       "       'lag_12h_price_return', 'lag_24h_price_return', 'lag_48h_price_return',\n",
       "       'lag_72h_price_return', 'lag_168h_price_return', 'volume_return_1h',\n",
       "       'lag_3h_volume_return', 'lag_6h_volume_return', 'lag_12h_volume_return',\n",
       "       'lag_24h_volume_return', 'ma_6h', 'ma_12h', 'ma_24h', 'ma_48h',\n",
       "       'ma_72h', 'ma_168h', 'rolling_std_6h', 'rolling_std_12h',\n",
       "       'rolling_std_24h', 'rolling_std_48h', 'rolling_std_72h',\n",
       "       'rolling_std_168h', 'atr_14h', 'atr_24h', 'atr_48h', 'close_div_ma_24h',\n",
       "       'close_div_ma_48h', 'close_div_ma_168h', 'ma12_div_ma48',\n",
       "       'ma24_div_ma168', 'std12_div_std72', 'volume_btc_x_range',\n",
       "       'rolling_std_3h_sq', 'price_return_1h_sq', 'rolling_std_12h_sqrt'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What Now?\n",
    "\n",
    "This is great progress! It tells you that predicting 12-hour direction is a much more promising path with your data and feature types.\n",
    "\n",
    "Stick with the Simpler Structure (for now): Keep the single model (XGBoost) and the expanding window backtest for now.\n",
    "\n",
    "Optimize This Setup:\n",
    "\n",
    "Apply VIF: Now that you have a working model structure and a seemingly viable target, apply VIF filtering (e.g., threshold 5 or even your strict 1.69) to the features generated in this simpler script. Does reducing collinearity now improve the already decent results?\n",
    "\n",
    "Tune Hyperparameters: Tune the XGBoost parameters (n_estimators, max_depth, learning_rate, reg_alpha, reg_lambda, subsample, colsample_bytree, min_child_weight) using a method like Optuna or RandomizedSearchCV within the rolling backtest loop (similar to how the meta-learner was tuned, but now for the single main model).\n",
    "\n",
    "Experiment with Target Horizon: Is 12 hours optimal for the >0% target? Try 8 hours, 24 hours.\n",
    "\n",
    "Experiment with Training Window: Does the expanding window work best, or would a large sliding window perform better for this target?\n",
    "\n",
    "You've found a much better baseline. Now optimize it systematically!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
