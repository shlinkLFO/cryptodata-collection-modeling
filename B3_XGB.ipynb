{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Loading ---\n",
      "Loading data from: BTCUSD.csv\n",
      "Selected last 15000 rows.\n",
      "\n",
      "--- Feature Engineering ---\n",
      "Feature engineering complete. Took 0.02 seconds.\n",
      "Columns after features: 51\n",
      "\n",
      "--- Target Definition ---\n",
      "Defining target as 24h future return >= 0.12%...\n",
      "\n",
      "--- Data Preparation ---\n",
      "NaN Handling: Dropped 197 rows.\n",
      "Replacing 8 infinites...\n",
      "Dropped 8 more rows after Inf handling.\n",
      "Final feature matrix shape: (14795, 42)\n",
      "Target vector shape: (14795,)\n",
      "Using 42 features.\n",
      "\n",
      "--- Starting SLIDING Window Backtest with Per-Step HParam Tuning ---\n",
      "!!! WARNING: This will be significantly slower due to GridSearchCV in each step !!!\n",
      "Sliding Window: 1008 rows, Step: 48 rows, Tuning Grid Size: 64 combinations\n",
      "\n",
      "--- Step 1 (Predicting for 2023-09-07 02:00:00) ---\n",
      "  Training window: [0:1007]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 5.95s.\n",
      "  Best Params: {'colsample_bytree': 0.7, 'max_depth': 6, 'n_estimators': 90, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.7952\n",
      "  Step 1 finished in 6.01s total.\n",
      "\n",
      "--- Step 2 (Predicting for 2023-09-09 02:00:00) ---\n",
      "  Training window: [48:1055]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.96s.\n",
      "  Best Params: {'colsample_bytree': 0.7, 'max_depth': 7, 'n_estimators': 90, 'reg_alpha': 0.3, 'subsample': 0.78}, Best CV F1: 0.8219\n",
      "  Step 2 finished in 3.05s total.\n",
      "\n",
      "--- Step 3 (Predicting for 2023-09-11 02:00:00) ---\n",
      "  Training window: [96:1103]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.99s.\n",
      "  Best Params: {'colsample_bytree': 0.7, 'max_depth': 4, 'n_estimators': 135, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8255\n",
      "  Step 3 finished in 3.07s total.\n",
      "\n",
      "--- Step 4 (Predicting for 2023-09-13 02:00:00) ---\n",
      "  Training window: [144:1151]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.06s.\n",
      "  Best Params: {'colsample_bytree': 0.9, 'max_depth': 6, 'n_estimators': 90, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8179\n",
      "  Step 4 finished in 3.13s total.\n",
      "\n",
      "--- Step 5 (Predicting for 2023-09-15 02:00:00) ---\n",
      "  Training window: [192:1199]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.01s.\n",
      "  Best Params: {'colsample_bytree': 0.9, 'max_depth': 7, 'n_estimators': 135, 'reg_alpha': 0.3, 'subsample': 0.92}, Best CV F1: 0.8694\n",
      "  Step 5 finished in 3.11s total.\n",
      "\n",
      "--- Step 6 (Predicting for 2023-09-17 02:00:00) ---\n",
      "  Training window: [240:1247]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.05s.\n",
      "  Best Params: {'colsample_bytree': 0.9, 'max_depth': 6, 'n_estimators': 90, 'reg_alpha': 0.3, 'subsample': 0.92}, Best CV F1: 0.8697\n",
      "  Step 6 finished in 3.11s total.\n",
      "\n",
      "--- Step 7 (Predicting for 2023-09-19 02:00:00) ---\n",
      "  Training window: [288:1295]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.14s.\n",
      "  Best Params: {'colsample_bytree': 0.9, 'max_depth': 7, 'n_estimators': 90, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8746\n",
      "  Step 7 finished in 3.21s total.\n",
      "\n",
      "--- Step 8 (Predicting for 2023-09-21 02:00:00) ---\n",
      "  Training window: [336:1343]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.00s.\n",
      "  Best Params: {'colsample_bytree': 0.9, 'max_depth': 5, 'n_estimators': 90, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8692\n",
      "  Step 8 finished in 3.05s total.\n",
      "\n",
      "--- Step 9 (Predicting for 2023-09-23 02:00:00) ---\n",
      "  Training window: [384:1391]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.07s.\n",
      "  Best Params: {'colsample_bytree': 0.9, 'max_depth': 7, 'n_estimators': 135, 'reg_alpha': 0.14, 'subsample': 0.78}, Best CV F1: 0.8560\n",
      "  Step 9 finished in 3.19s total.\n",
      "\n",
      "--- Step 10 (Predicting for 2023-09-25 02:00:00) ---\n",
      "  Training window: [432:1439]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.00s.\n",
      "  Best Params: {'colsample_bytree': 0.7, 'max_depth': 6, 'n_estimators': 135, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8522\n",
      "  Step 10 finished in 3.08s total.\n",
      "\n",
      "--- Step 11 (Predicting for 2023-09-27 02:00:00) ---\n",
      "  Training window: [480:1487]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.07s.\n",
      "  Best Params: {'colsample_bytree': 0.7, 'max_depth': 4, 'n_estimators': 135, 'reg_alpha': 0.3, 'subsample': 0.92}, Best CV F1: 0.8693\n",
      "  Step 11 finished in 3.16s total.\n",
      "\n",
      "--- Step 12 (Predicting for 2023-09-29 02:00:00) ---\n",
      "  Training window: [528:1535]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.13s.\n",
      "  Best Params: {'colsample_bytree': 0.7, 'max_depth': 6, 'n_estimators': 135, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8642\n",
      "  Step 12 finished in 3.22s total.\n",
      "\n",
      "--- Step 13 (Predicting for 2023-10-01 02:00:00) ---\n",
      "  Training window: [576:1583]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.03s.\n",
      "  Best Params: {'colsample_bytree': 0.7, 'max_depth': 6, 'n_estimators': 135, 'reg_alpha': 0.3, 'subsample': 0.92}, Best CV F1: 0.8977\n",
      "  Step 13 finished in 3.11s total.\n",
      "\n",
      "--- Step 14 (Predicting for 2023-10-03 02:00:00) ---\n",
      "  Training window: [624:1631]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.06s.\n",
      "  Best Params: {'colsample_bytree': 0.7, 'max_depth': 6, 'n_estimators': 135, 'reg_alpha': 0.3, 'subsample': 0.92}, Best CV F1: 0.8987\n",
      "  Step 14 finished in 3.16s total.\n",
      "\n",
      "--- Step 15 (Predicting for 2023-10-05 02:00:00) ---\n",
      "  Training window: [672:1679]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.02s.\n",
      "  Best Params: {'colsample_bytree': 0.7, 'max_depth': 6, 'n_estimators': 90, 'reg_alpha': 0.3, 'subsample': 0.78}, Best CV F1: 0.8935\n",
      "  Step 15 finished in 3.08s total.\n",
      "\n",
      "--- Step 16 (Predicting for 2023-10-07 02:00:00) ---\n",
      "  Training window: [720:1727]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.08s.\n",
      "  Best Params: {'colsample_bytree': 0.7, 'max_depth': 6, 'n_estimators': 135, 'reg_alpha': 0.3, 'subsample': 0.78}, Best CV F1: 0.9021\n",
      "  Step 16 finished in 3.18s total.\n",
      "\n",
      "--- Step 17 (Predicting for 2023-10-09 02:00:00) ---\n",
      "  Training window: [768:1775]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.08s.\n",
      "  Best Params: {'colsample_bytree': 0.7, 'max_depth': 6, 'n_estimators': 135, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.9084\n",
      "  Step 17 finished in 3.16s total.\n",
      "\n",
      "--- Step 18 (Predicting for 2023-10-11 02:00:00) ---\n",
      "  Training window: [816:1823]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.95s.\n",
      "  Best Params: {'colsample_bytree': 0.9, 'max_depth': 6, 'n_estimators': 135, 'reg_alpha': 0.14, 'subsample': 0.78}, Best CV F1: 0.8991\n",
      "  Step 18 finished in 3.04s total.\n",
      "\n",
      "--- Step 19 (Predicting for 2023-10-13 02:00:00) ---\n",
      "  Training window: [864:1871]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.10s.\n",
      "  Best Params: {'colsample_bytree': 0.7, 'max_depth': 7, 'n_estimators': 135, 'reg_alpha': 0.3, 'subsample': 0.78}, Best CV F1: 0.9052\n",
      "  Step 19 finished in 3.21s total.\n",
      "\n",
      "--- Step 20 (Predicting for 2023-10-15 02:00:00) ---\n",
      "  Training window: [912:1919]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.04s.\n",
      "  Best Params: {'colsample_bytree': 0.7, 'max_depth': 6, 'n_estimators': 135, 'reg_alpha': 0.3, 'subsample': 0.78}, Best CV F1: 0.8921\n",
      "  Step 20 finished in 3.12s total.\n",
      "\n",
      "--- Step 21 (Predicting for 2023-10-17 02:00:00) ---\n",
      "  Training window: [960:1967]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.02s.\n",
      "  Best Params: {'colsample_bytree': 0.9, 'max_depth': 6, 'n_estimators': 135, 'reg_alpha': 0.3, 'subsample': 0.92}, Best CV F1: 0.9099\n",
      "  Step 21 finished in 3.10s total.\n",
      "\n",
      "--- Step 22 (Predicting for 2023-10-19 02:00:00) ---\n",
      "  Training window: [1008:2015]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.00s.\n",
      "  Best Params: {'colsample_bytree': 0.7, 'max_depth': 4, 'n_estimators': 135, 'reg_alpha': 0.3, 'subsample': 0.92}, Best CV F1: 0.9157\n",
      "  Step 22 finished in 3.06s total.\n",
      "\n",
      "--- Step 23 (Predicting for 2023-10-21 02:00:00) ---\n",
      "  Training window: [1056:2063]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.08s.\n",
      "  Best Params: {'colsample_bytree': 0.7, 'max_depth': 5, 'n_estimators': 90, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.9315\n",
      "  Step 23 finished in 3.14s total.\n",
      "\n",
      "--- Step 24 (Predicting for 2023-10-23 02:00:00) ---\n",
      "  Training window: [1104:2111]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.20s.\n",
      "  Best Params: {'colsample_bytree': 0.7, 'max_depth': 4, 'n_estimators': 135, 'reg_alpha': 0.3, 'subsample': 0.92}, Best CV F1: 0.9243\n",
      "  Step 24 finished in 3.28s total.\n",
      "\n",
      "--- Step 25 (Predicting for 2023-10-25 02:00:00) ---\n",
      "  Training window: [1152:2159]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 252\u001b[0m\n\u001b[0;32m    246\u001b[0m estimator \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBClassifier(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mXGB_FIXED_PARAMS)\n\u001b[0;32m    247\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[0;32m    248\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator, param_grid\u001b[38;5;241m=\u001b[39mXGB_PARAM_GRID_TUNE, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    249\u001b[0m     cv\u001b[38;5;241m=\u001b[39mStratifiedKFold(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mi), \u001b[38;5;66;03m# Seed CV split for some reproducibility per step\u001b[39;00m\n\u001b[0;32m    250\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;66;03m# Can increase verbose level for debugging GS\u001b[39;00m\n\u001b[0;32m    251\u001b[0m )\n\u001b[1;32m--> 252\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_roll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_roll\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    253\u001b[0m best_params_step \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_params_\n\u001b[0;32m    254\u001b[0m best_score_step \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_score_\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\model_selection\\_search.py:1422\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1421\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1422\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\model_selection\\_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m     )\n\u001b[1;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# B3_Configurable.py\n",
    "# Simple Predictor with Sliding Window, Per-Step HParam Tuning, and PTT\n",
    "# Refactored for easier configuration.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Modeling Imports\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, ParameterGrid\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "# --- Suppress Warnings ---\n",
    "warnings.filterwarnings('ignore', category=UndefinedMetricWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==============================================================================\n",
    "# --- Configuration ---\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Data ---\n",
    "CSV_FILE_PATH = 'BTCUSD.csv'\n",
    "N_ROWS_INPUT = 15000          # Number of most recent rows to load from CSV\n",
    "\n",
    "# --- Target Definition ---\n",
    "PREDICTION_WINDOW_HOURS = 24  # How many hours ahead to predict\n",
    "TARGET_THRESHOLD_PCT = 0.12    # Threshold for positive class (e.g., 0.0 means >= 0% increase)\n",
    "\n",
    "# --- Backtesting Windowing ---\n",
    "TRAIN_WINDOW_HOURS = 24 * 7 * 6     # Size of the sliding training window (in hours/rows)\n",
    "STEP_HOURS = 48               # How often to retrain/predict (in hours/rows)\n",
    "\n",
    "# --- Model & Tuning ---\n",
    "# Fixed XGBoost parameters (not tuned in grid search)\n",
    "XGB_FIXED_PARAMS = {\n",
    "    'learning_rate': 0.1,\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'use_label_encoder': False, # Deprecated, use False\n",
    "    'random_state': 42,\n",
    "    #'reg_alpha': 0.1,          # L1 Regularization\n",
    "    'reg_lambda': 2.0,         # L2 Regularization\n",
    "    'n_jobs': -1               # Use all available CPU cores\n",
    "}\n",
    "\n",
    "# Parameter grid for GridSearchCV (Keep combinations reasonable, e.g., < ~64)\n",
    "XGB_PARAM_GRID_TUNE = {\n",
    "    'max_depth': [5, 6, 7],             # 2 options\n",
    "    'n_estimators': [95, 130],  # 3 options\n",
    "    'subsample': [0.78, 0.92],         # 2 options\n",
    "    'colsample_bytree': [0.7, 0.9],  # 2 options\n",
    "    'reg_alpha': [0.14, 0.30], # 2 options\n",
    "} # Total combinations: 2 * 3 * 2 * 2 * 2 = 48\n",
    "\n",
    "# Probability Threshold Tuning Range\n",
    "PROBABILITY_THRESHOLD_RANGE = (0.10, 0.90) # Start and End (exclusive for end)\n",
    "PROBABILITY_THRESHOLD_STEP = 0.05\n",
    "\n",
    "# ==============================================================================\n",
    "# --- Derived Variables (Do not change these directly) ---\n",
    "# ==============================================================================\n",
    "TRAIN_WINDOW_ROWS = TRAIN_WINDOW_HOURS\n",
    "STEP_ROWS = STEP_HOURS\n",
    "PREDICTION_WINDOW_ROWS = PREDICTION_WINDOW_HOURS\n",
    "THRESHOLD_SEARCH_RANGE = np.arange(\n",
    "    PROBABILITY_THRESHOLD_RANGE[0],\n",
    "    PROBABILITY_THRESHOLD_RANGE[1],\n",
    "    PROBABILITY_THRESHOLD_STEP\n",
    ")\n",
    "grid_combinations = len(list(ParameterGrid(XGB_PARAM_GRID_TUNE))) # Calculate grid size\n",
    "\n",
    "# ==============================================================================\n",
    "# --- Script Start ---\n",
    "# ==============================================================================\n",
    "\n",
    "# --- 1. Load Data and Select Last Rows ---\n",
    "print(\"--- Data Loading ---\")\n",
    "print(f\"Loading data from: {CSV_FILE_PATH}\")\n",
    "try:\n",
    "    df_full = pd.read_csv(CSV_FILE_PATH)\n",
    "    df_full = df_full.sort_values(by='unix', ascending=True).reset_index(drop=True)\n",
    "    if 'date' in df_full.columns:\n",
    "        try: df_full['date'] = pd.to_datetime(df_full['date'])\n",
    "        except Exception as e_date: print(f\"Warning: Date parse error: {e_date}\")\n",
    "\n",
    "    if len(df_full) < N_ROWS_INPUT:\n",
    "        print(f\"Warning: Full dataset ({len(df_full)} rows) < {N_ROWS_INPUT}. Using all data.\")\n",
    "        df = df_full.copy()\n",
    "    else:\n",
    "        df = df_full.iloc[-N_ROWS_INPUT:].reset_index(drop=True)\n",
    "        print(f\"Selected last {len(df)} rows.\")\n",
    "except FileNotFoundError: print(f\"Error: {CSV_FILE_PATH} not found.\"); exit()\n",
    "except Exception as e: print(f\"Error loading data: {e}\"); exit()\n",
    "\n",
    "# --- 2. Feature Engineering ---\n",
    "print(\"\\n--- Feature Engineering ---\")\n",
    "start_fe = time.time()\n",
    "base_cols_numeric = ['open', 'high', 'low', 'close', 'Volume BTC', 'Volume USD']\n",
    "for col in base_cols_numeric:\n",
    "    if col in df.columns: df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    else: print(f\"Warning: Missing base column '{col}'\"); df[col] = 0\n",
    "if df[['open', 'high', 'low', 'close']].isnull().any().any():\n",
    "    print(\"Warning: OHLC NaNs found. Dropping rows.\"); df = df.dropna(subset=['open', 'high', 'low', 'close'])\n",
    "if df.empty: exit(\"Error: Empty DF after initial OHLC NaN drop.\")\n",
    "# --- Feature Functions ---\n",
    "def garman_klass_volatility(o, h, l, c, w):\n",
    "    with np.errstate(divide='ignore', invalid='ignore'): log_hl=np.log(h/l.replace(0,np.nan)); log_co=np.log(c/o.replace(0,np.nan))\n",
    "    gk = 0.5*(log_hl**2) - (2*np.log(2)-1)*(log_co**2); gk = gk.fillna(0)\n",
    "    rm = gk.rolling(w, min_periods=max(1,w//2)).mean(); rm = rm.clip(lower=0); return np.sqrt(rm)\n",
    "def parkinson_volatility(h, l, w):\n",
    "    with np.errstate(divide='ignore', invalid='ignore'): log_hl_sq = np.log(h/l.replace(0,np.nan))**2\n",
    "    log_hl_sq = log_hl_sq.fillna(0); rs = log_hl_sq.rolling(w,min_periods=max(1,w//2)).sum()\n",
    "    f = 1/(4*np.log(2)*w) if w>0 else 0; return np.sqrt(f*rs)\n",
    "# --- Feature Calculations ---\n",
    "df['price_change_1h_temp'] = df['close'].pct_change()\n",
    "with np.errstate(divide='ignore', invalid='ignore'):\n",
    "    df['price_range_pct'] = (df['high'] - df['low']) / df['close'].replace(0, np.nan) * 100\n",
    "    df['oc_change_pct'] = (df['close'] - df['open']) / df['open'].replace(0, np.nan) * 100\n",
    "df['garman_klass_12h'] = garman_klass_volatility(df['open'],df['high'],df['low'],df['close'],12)\n",
    "df['parkinson_3h'] = parkinson_volatility(df['high'],df['low'],3)\n",
    "min_periods_rolling = 2\n",
    "df['ma_3h'] = df['close'].rolling(3, min_periods=min_periods_rolling).mean()\n",
    "df['rolling_std_3h'] = df['close'].rolling(3, min_periods=min_periods_rolling).std()\n",
    "lag_periods_price = [3, 6, 12, 24, 48, 72, 168]; lag_periods_volume = [3, 6, 12, 24]\n",
    "for lag in lag_periods_price: df[f'lag_{lag}h_price_return'] = df['price_change_1h_temp'].shift(lag) * 100\n",
    "df['volume_return_1h'] = df['Volume BTC'].pct_change() * 100\n",
    "for lag in lag_periods_volume: df[f'lag_{lag}h_volume_return'] = df['volume_return_1h'].shift(lag)\n",
    "ma_periods = [6, 12, 24, 48, 72, 168]; std_periods = [6, 12, 24, 48, 72, 168]\n",
    "for p in ma_periods: df[f'ma_{p}h'] = df['close'].rolling(p, min_periods=max(min_periods_rolling, p//2)).mean()\n",
    "for p in std_periods: df[f'rolling_std_{p}h'] = df['price_change_1h_temp'].rolling(p, min_periods=max(min_periods_rolling, p//2)).std() * 100\n",
    "df['prev_close']=df['close'].shift(1); df['hml']=df['high']-df['low']; df['hmpc']=np.abs(df['high']-df['prev_close']); df['lmpc']=np.abs(df['low']-df['prev_close'])\n",
    "df['tr']=df[['hml','hmpc','lmpc']].max(axis=1)\n",
    "atr_periods = [14, 24, 48]\n",
    "for p in atr_periods: df[f'atr_{p}h'] = df['tr'].rolling(p, min_periods=max(1,p//2)).mean()\n",
    "df = df.drop(columns=['prev_close', 'hml', 'hmpc', 'lmpc', 'tr'])\n",
    "epsilon = 1e-9\n",
    "for p in [24, 48, 168]:\n",
    "    mc=f'ma_{p}h'; df[f'close_div_ma_{p}h'] = df['close']/(df[mc]+epsilon) if mc in df else np.nan\n",
    "if 'ma_12h' in df and 'ma_48h' in df: df['ma12_div_ma48'] = df['ma_12h']/(df['ma_48h']+epsilon)\n",
    "else: df['ma12_div_ma48']=np.nan\n",
    "if 'ma_24h' in df and 'ma_168h' in df: df['ma24_div_ma168'] = df['ma_24h']/(df['ma_168h']+epsilon)\n",
    "else: df['ma24_div_ma168']=np.nan\n",
    "if 'rolling_std_12h' in df and 'rolling_std_72h' in df: df['std12_div_std72'] = df['rolling_std_12h']/(df['rolling_std_72h']+epsilon)\n",
    "else: df['std12_div_std72']=np.nan\n",
    "if 'price_range_pct' in df: df['volume_btc_x_range'] = df['Volume BTC'] * df['price_range_pct']\n",
    "else: df['volume_btc_x_range']=np.nan\n",
    "if 'rolling_std_3h' in df: df['rolling_std_3h_sq'] = df['rolling_std_3h']**2\n",
    "else: df['rolling_std_3h_sq']=np.nan\n",
    "if 'price_change_1h_temp' in df: df['price_return_1h_sq'] = df['price_change_1h_temp']**2 * 10000\n",
    "else: df['price_return_1h_sq']=np.nan\n",
    "if 'rolling_std_12h' in df: df['rolling_std_12h_sqrt'] = np.sqrt(df['rolling_std_12h'].clip(lower=0)+epsilon)\n",
    "else: df['rolling_std_12h_sqrt']=np.nan\n",
    "cols_to_drop_intermediate = ['price_change_1h_temp', 'volume_return_1h']\n",
    "df = df.drop(columns=[col for col in cols_to_drop_intermediate if col in df.columns])\n",
    "# df['symbol'] = 'BTCUSD' # Only add if needed for some reason OUTSIDE modeling features\n",
    "print(f\"Feature engineering complete. Took {time.time() - start_fe:.2f} seconds.\")\n",
    "print(f\"Columns after features: {df.shape[1]}\")\n",
    "\n",
    "# --- 3. Define Target Variable ---\n",
    "print(\"\\n--- Target Definition ---\")\n",
    "print(f\"Defining target as {PREDICTION_WINDOW_HOURS}h future return >= {TARGET_THRESHOLD_PCT}%...\")\n",
    "target_col = f'target_return_{PREDICTION_WINDOW_HOURS}h' # Dynamic target column name\n",
    "df[target_col] = df['close'].shift(-PREDICTION_WINDOW_ROWS).sub(df['close']).div(df['close'].replace(0, np.nan)).mul(100)\n",
    "\n",
    "# --- 4. Prepare Data for Modeling ---\n",
    "print(\"\\n--- Data Preparation ---\")\n",
    "cols_to_keep_final = ['unix', 'date', target_col, 'symbol'] # Columns to keep, but EXCLUDE from X\n",
    "potential_feature_cols = [col for col in df.columns if col not in cols_to_keep_final]\n",
    "numeric_feature_cols = df[potential_feature_cols].select_dtypes(include=np.number).columns.tolist()\n",
    "# Ensure base OHLCV are not included unless desired\n",
    "final_feature_cols = [col for col in numeric_feature_cols if col not in base_cols_numeric]\n",
    "\n",
    "# Keep only defined features and necessary identifiers/target\n",
    "cols_to_select = final_feature_cols + [col for col in cols_to_keep_final if col in df.columns] # Only keep existing identifier/target cols\n",
    "df_model_ready = df[cols_to_select].copy()\n",
    "\n",
    "# Handle NaNs (Drop rows with ANY NaN in features OR target)\n",
    "initial_rows = len(df_model_ready); df_model_ready = df_model_ready.dropna(); final_rows = len(df_model_ready)\n",
    "print(f\"NaN Handling: Dropped {initial_rows - final_rows} rows.\")\n",
    "\n",
    "# Handle Infinites\n",
    "numeric_cols_final = df_model_ready[final_feature_cols].select_dtypes(include=np.number).columns.tolist()\n",
    "inf_mask = np.isinf(df_model_ready[numeric_cols_final]); inf_count = inf_mask.sum().sum()\n",
    "if inf_count > 0:\n",
    "    print(f\"Replacing {inf_count} infinites...\"); df_model_ready.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    rows_b4 = len(df_model_ready); df_model_ready = df_model_ready.dropna(); print(f\"Dropped {rows_b4 - len(df_model_ready)} more rows after Inf handling.\")\n",
    "if df_model_ready.empty: exit(\"Error: DataFrame empty after NaN/Inf handling.\")\n",
    "\n",
    "# Define final Features (X) and Binary Target (y)\n",
    "X = df_model_ready[final_feature_cols]\n",
    "y_binary = (df_model_ready[target_col] >= TARGET_THRESHOLD_PCT).astype(int)\n",
    "\n",
    "# Get timestamps\n",
    "if 'date' in df_model_ready.columns and pd.api.types.is_datetime64_any_dtype(df_model_ready['date']): timestamps = df_model_ready['date']\n",
    "elif 'unix' in df_model_ready.columns: timestamps = pd.to_datetime(df_model_ready['unix'], unit='ms')\n",
    "else: print(\"Warning: No date/unix. Using index.\"); timestamps = pd.Series(df_model_ready.index)\n",
    "\n",
    "print(f\"Final feature matrix shape: {X.shape}\"); print(f\"Target vector shape: {y_binary.shape}\"); print(f\"Using {len(final_feature_cols)} features.\")\n",
    "\n",
    "# --- 5. SLIDING Window Backtesting with Per-Step HParam Tuning ---\n",
    "print(\"\\n--- Starting SLIDING Window Backtest with Per-Step HParam Tuning ---\")\n",
    "print(\"!!! WARNING: This will be significantly slower due to GridSearchCV in each step !!!\")\n",
    "\n",
    "# Use derived _ROWS variables\n",
    "if len(X) < TRAIN_WINDOW_ROWS + STEP_ROWS:\n",
    "     print(f\"Error: Not enough data ({len(X)}) for train window ({TRAIN_WINDOW_ROWS}) + step ({STEP_ROWS}).\"); exit()\n",
    "\n",
    "# Store results\n",
    "all_predictions_proba = []; all_actual = []; backtest_timestamps = []\n",
    "num_steps = 0\n",
    "start_index_loop = TRAIN_WINDOW_ROWS; end_index_loop = len(X)\n",
    "\n",
    "print(f\"Sliding Window: {TRAIN_WINDOW_ROWS} rows, Step: {STEP_ROWS} rows, Tuning Grid Size: {grid_combinations} combinations\")\n",
    "loop_start_time = time.time()\n",
    "\n",
    "for i in range(start_index_loop, end_index_loop, STEP_ROWS):\n",
    "    step_start_time = time.time()\n",
    "    train_idx_start = i - TRAIN_WINDOW_ROWS; train_idx_end = i; test_idx = i\n",
    "    if test_idx >= len(X): break\n",
    "    X_train_roll = X.iloc[train_idx_start : train_idx_end]\n",
    "    y_train_roll = y_binary.iloc[train_idx_start : train_idx_end]\n",
    "    X_test_roll = X.iloc[test_idx : test_idx + 1]\n",
    "    y_test_roll_actual = y_binary.iloc[test_idx]\n",
    "    current_timestamp = timestamps.iloc[test_idx]\n",
    "\n",
    "    if X_train_roll.empty or len(np.unique(y_train_roll)) < 2:\n",
    "        print(f\"Warning: Skipping index {test_idx}. Invalid training data.\"); continue\n",
    "\n",
    "    print(f\"\\n--- Step {num_steps + 1} (Predicting for {current_timestamp}) ---\")\n",
    "    print(f\"  Training window: [{train_idx_start}:{train_idx_end-1}]\")\n",
    "\n",
    "    # --- Hyperparameter Tuning ---\n",
    "    print(f\"  Running GridSearchCV (cv=3, scoring='f1')...\")\n",
    "    grid_search_start_time = time.time()\n",
    "    try:\n",
    "        estimator = xgb.XGBClassifier(**XGB_FIXED_PARAMS)\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=estimator, param_grid=XGB_PARAM_GRID_TUNE, scoring='f1',\n",
    "            cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=i), # Seed CV split for some reproducibility per step\n",
    "            n_jobs=-1, verbose=0 # Can increase verbose level for debugging GS\n",
    "        )\n",
    "        grid_search.fit(X_train_roll, y_train_roll)\n",
    "        best_params_step = grid_search.best_params_\n",
    "        best_score_step = grid_search.best_score_\n",
    "        print(f\"  GridSearchCV finished in {time.time() - grid_search_start_time:.2f}s.\")\n",
    "        print(f\"  Best Params: {best_params_step}, Best CV F1: {best_score_step:.4f}\")\n",
    "\n",
    "        # --- Fit final model for the step ---\n",
    "        final_model_params = {**XGB_FIXED_PARAMS, **best_params_step}\n",
    "        model_roll = xgb.XGBClassifier(**final_model_params)\n",
    "        model_roll.fit(X_train_roll, y_train_roll, verbose=False)\n",
    "\n",
    "        # --- Predict probability ---\n",
    "        prob_roll = model_roll.predict_proba(X_test_roll)[0, 1]\n",
    "\n",
    "        # --- Store results ---\n",
    "        all_predictions_proba.append(prob_roll)\n",
    "        all_actual.append(y_test_roll_actual)\n",
    "        backtest_timestamps.append(current_timestamp)\n",
    "        num_steps += 1\n",
    "\n",
    "    except Exception as e_step:\n",
    "        print(f\"!! Error during GridSearch/Fit/Predict at index {test_idx}: {e_step}\"); traceback.print_exc(); continue\n",
    "\n",
    "    step_end_time = time.time()\n",
    "    print(f\"  Step {num_steps} finished in {step_end_time - step_start_time:.2f}s total.\")\n",
    "\n",
    "loop_end_time = time.time()\n",
    "print(f\"\\nBacktesting loop finished. Completed {num_steps} steps in {(loop_end_time - loop_start_time)/60:.2f} minutes.\")\n",
    "\n",
    "# --- 6. Evaluate Backtesting Results with PTT ---\n",
    "if num_steps > 0 and len(all_predictions_proba) == len(all_actual):\n",
    "    print(\"\\n--- Evaluating Results with Probability Threshold Tuning ---\")\n",
    "    print(f\"Threshold search range: {THRESHOLD_SEARCH_RANGE}\")\n",
    "    best_threshold = 0.5; best_f1_thresh = -1.0\n",
    "    results_per_threshold = {}\n",
    "    probabilities_np = np.array(all_predictions_proba); actual_np = np.array(all_actual)\n",
    "\n",
    "    for t in THRESHOLD_SEARCH_RANGE:\n",
    "        predictions_thresh = (probabilities_np >= t).astype(int)\n",
    "        # Handle edge cases for metrics calculation\n",
    "        if np.sum(actual_np) == 0 and np.sum(predictions_thresh) == 0:\n",
    "            acc_t, pre_t, rec_t, f1_t = 1.0, 1.0, 1.0, 1.0 # Perfect prediction of negatives\n",
    "        elif np.sum(actual_np) > 0 and np.sum(predictions_thresh) == 0:\n",
    "             acc_t = accuracy_score(actual_np, predictions_thresh)\n",
    "             pre_t, rec_t, f1_t = 0.0, 0.0, 0.0 # No predicted positives\n",
    "        elif np.sum(actual_np) == 0 and np.sum(predictions_thresh) > 0:\n",
    "             acc_t = accuracy_score(actual_np, predictions_thresh)\n",
    "             pre_t, rec_t, f1_t = 0.0, 0.0, 0.0 # Predicted positives but none exist\n",
    "        else: # Normal case or TP+FN > 0 and TP+FP > 0\n",
    "             acc_t = accuracy_score(actual_np, predictions_thresh)\n",
    "             pre_t = precision_score(actual_np, predictions_thresh, zero_division=0)\n",
    "             rec_t = recall_score(actual_np, predictions_thresh, zero_division=0)\n",
    "             f1_t = f1_score(actual_np, predictions_thresh, zero_division=0)\n",
    "\n",
    "        results_per_threshold[round(t, 2)] = {'f1': f1_t, 'acc': acc_t, 'pre': pre_t, 'rec': rec_t}\n",
    "        if f1_t >= best_f1_thresh: # Prefer higher threshold if F1 is equal\n",
    "            best_f1_thresh = f1_t; best_threshold = t\n",
    "\n",
    "    print(f\"\\nBest Threshold found: {best_threshold:.2f} (Yielding F1 Score: {best_f1_thresh:.4f})\")\n",
    "\n",
    "    final_predictions_optimized = (probabilities_np >= best_threshold).astype(int)\n",
    "    final_accuracy = accuracy_score(actual_np, final_predictions_optimized)\n",
    "    final_precision = precision_score(actual_np, final_predictions_optimized, zero_division=0)\n",
    "    final_recall = recall_score(actual_np, final_predictions_optimized, zero_division=0)\n",
    "    final_f1 = f1_score(actual_np, final_predictions_optimized, zero_division=0)\n",
    "\n",
    "    print(\"\\n--- Final Performance Metrics (Using Optimized Threshold) ---\")\n",
    "    print(f\"Target: {PREDICTION_WINDOW_HOURS}h return >= {TARGET_THRESHOLD_PCT}%\")\n",
    "    print(f\"Windowing: Train={TRAIN_WINDOW_ROWS} rows, Step={STEP_ROWS} rows\")\n",
    "    print(f\"Overall Accuracy:  {final_accuracy:.4f}\")\n",
    "    print(f\"Overall Precision: {final_precision:.4f}\")\n",
    "    print(f\"Overall Recall:    {final_recall:.4f}\")\n",
    "    print(f\"Overall F1 Score:  {final_f1:.4f}\")\n",
    "\n",
    "    if 0.5 in results_per_threshold:\n",
    "        res_def = results_per_threshold[0.5]\n",
    "        print(f\"(Compare: Default 0.5 Thresh -> F1:{res_def['f1']:.4f})\")\n",
    "\n",
    "    # --- 7. Plot Cumulative Accuracy ---\n",
    "    print(\"\\nPlotting cumulative accuracy (optimized threshold)...\")\n",
    "    cumulative_accuracy_list_optimized = (np.cumsum(final_predictions_optimized == actual_np) / np.arange(1, num_steps + 1))\n",
    "    try:\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        plt.plot(backtest_timestamps, cumulative_accuracy_list_optimized, marker='.', linestyle='-', markersize=3, label='Cumulative Accuracy (Optimized)')\n",
    "        rolling_window_plot = min(max(num_steps // 10, 50), 250) # Adaptive rolling window for plot, with min/max\n",
    "        if num_steps > rolling_window_plot:\n",
    "             rolling_acc = pd.Series(cumulative_accuracy_list_optimized).rolling(window=rolling_window_plot).mean()\n",
    "             plt.plot(backtest_timestamps[rolling_window_plot-1:], rolling_acc.dropna(), linestyle='--', color='red', label=f'Rolling Acc ({rolling_window_plot} steps)') # Align plot\n",
    "        plt.title(f'B3 Backtest (Train:{TRAIN_WINDOW_ROWS}, Step:{STEP_ROWS}, Tuned) - Best Thresh: {best_threshold:.2f}')\n",
    "        plt.xlabel('Timestamp'); plt.ylabel('Accuracy')\n",
    "        min_y_plot = max(0.0, np.min(cumulative_accuracy_list_optimized) - 0.05 if len(cumulative_accuracy_list_optimized)>0 else 0.4)\n",
    "        max_y_plot = min(1.0, np.max(cumulative_accuracy_list_optimized) + 0.05 if len(cumulative_accuracy_list_optimized)>0 else 0.8)\n",
    "        plt.ylim(min_y_plot, max_y_plot)\n",
    "        plt.grid(True, linestyle='--', alpha=0.6); plt.legend(); plt.xticks(rotation=30, ha='right'); plt.tight_layout(); plt.show()\n",
    "    except Exception as e_plot: print(f\"Error plotting: {e_plot}\")\n",
    "\n",
    "else:\n",
    "    print(\"No predictions were made/stored, cannot evaluate or plot.\")\n",
    "\n",
    "print(\"\\nScript B3_Configurable.py finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['open', 'high', 'low', 'close', 'Volume BTC', 'Volume USD',\n",
       "       'price_range_pct', 'oc_change_pct', 'garman_klass_12h', 'parkinson_3h',\n",
       "       'ma_3h', 'rolling_std_3h', 'lag_3h_price_return', 'lag_6h_price_return',\n",
       "       'lag_12h_price_return', 'lag_24h_price_return', 'lag_48h_price_return',\n",
       "       'lag_72h_price_return', 'lag_168h_price_return', 'volume_return_1h',\n",
       "       'lag_3h_volume_return', 'lag_6h_volume_return', 'lag_12h_volume_return',\n",
       "       'lag_24h_volume_return', 'ma_6h', 'ma_12h', 'ma_24h', 'ma_48h',\n",
       "       'ma_72h', 'ma_168h', 'rolling_std_6h', 'rolling_std_12h',\n",
       "       'rolling_std_24h', 'rolling_std_48h', 'rolling_std_72h',\n",
       "       'rolling_std_168h', 'atr_14h', 'atr_24h', 'atr_48h', 'close_div_ma_24h',\n",
       "       'close_div_ma_48h', 'close_div_ma_168h', 'ma12_div_ma48',\n",
       "       'ma24_div_ma168', 'std12_div_std72', 'volume_btc_x_range',\n",
       "       'rolling_std_3h_sq', 'price_return_1h_sq', 'rolling_std_12h_sqrt'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What Now?\n",
    "\n",
    "This is great progress! It tells you that predicting 12-hour direction is a much more promising path with your data and feature types.\n",
    "\n",
    "Stick with the Simpler Structure (for now): Keep the single model (XGBoost) and the expanding window backtest for now.\n",
    "\n",
    "Optimize This Setup:\n",
    "\n",
    "Apply VIF: Now that you have a working model structure and a seemingly viable target, apply VIF filtering (e.g., threshold 5 or even your strict 1.69) to the features generated in this simpler script. Does reducing collinearity now improve the already decent results?\n",
    "\n",
    "Tune Hyperparameters: Tune the XGBoost parameters (n_estimators, max_depth, learning_rate, reg_alpha, reg_lambda, subsample, colsample_bytree, min_child_weight) using a method like Optuna or RandomizedSearchCV within the rolling backtest loop (similar to how the meta-learner was tuned, but now for the single main model).\n",
    "\n",
    "Experiment with Target Horizon: Is 12 hours optimal for the >0% target? Try 8 hours, 24 hours.\n",
    "\n",
    "Experiment with Training Window: Does the expanding window work best, or would a large sliding window perform better for this target?\n",
    "\n",
    "You've found a much better baseline. Now optimize it systematically!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
