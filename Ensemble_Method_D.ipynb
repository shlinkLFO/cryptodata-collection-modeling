{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Ensemble_Method_D (SVM Meta-Learner) ---\n",
      "--- 1. Data Loading & Initial Prep ---\n",
      "Loading data from: C:\\Users\\mason\\AVP\\BTCUSDrec.csv\n",
      "Raw data loaded. Shape: (15177, 9)\n",
      "Initial data prep done. Shape: (15177, 7)\n",
      "\n",
      "--- 2. Feature Engineering ---\n",
      "Starting calculation for 49 target columns (incl. base)...\n",
      "  Calculating features...\n",
      "    Calculating ATR, Garman-Klass, Parkinson features...\n",
      "  Assembling final dataframe...\n",
      "Selected feature calculation finished. Returning 15177 rows, 51 columns (43 features). Took 0.04s.\n",
      "Feature calculation completed in 0.04 seconds.\n",
      "Using 43 features for modeling.\n",
      "\n",
      "--- 3. Data Cleaning ---\n",
      "Total NaNs in 43 numeric features: 1034.\n",
      "\n",
      "--- 4. Target & Final Prep ---\n",
      "Creating target: 3h return >= 0.8%...\n",
      "Rows after NaN target/close drop: 15174 (Removed 3)\n",
      "Note: 183 rows have NaNs in features. Models/Imputer handle.\n",
      "\n",
      "Target distribution:\n",
      "  0 (< 0.8%): 87.98%\n",
      "  1 (>= 0.8%): 12.02%\n",
      "Final DataFrame shape: (15174, 52)\n",
      "\n",
      "--- 5. Starting Walk-Forward Validation (Stacking - SVM Meta) ---\n",
      "Total rows: 15174, Train: 840h (840 rows), Eval: 168h (168 rows), Step: 48h (48 rows)\n",
      "Estimated iterations: 296\n",
      "Using 43 features.\n",
      "Stacking Folds (K): 7\n",
      "Meta Learner: SVM (RBF), Tuning over: {'C': [0.1, 3, 5], 'gamma': ['scale', 'auto', 0.1]}\n",
      "Threshold Search Range: [0.1  0.15 0.2  0.25 0.3  0.35 0.4  0.45 0.5  0.55 0.6  0.65 0.7  0.75\n",
      " 0.8  0.85]\n",
      "------------------------------\n",
      "\n",
      "--- Iter 1/296 ---\n",
      "  Train Indices: [0:839], Eval Indices: [840:1007]\n",
      "  Train Target Dist: {0.0: 0.9666666666666667, 1.0: 0.03333333333333333}\n",
      "  Test Target Dist: {0.0: 0.9464285714285714, 1.0: 0.05357142857142857}\n",
      "  Using scale_pos_weight for XGB/LGBM: 29.0000\n",
      "  Grid searching base models...\n",
      "    Best XGB Params: {'max_depth': 4, 'n_estimators': 65, 'eta': 0.05, 'lambda': 1.5} (CV F1: 0.315)\n",
      "    Best LGBM Params: {'max_depth': 3, 'n_estimators': 75, 'learning_rate': 0.08, 'subsample': 0.75} (CV F1: 0.377)\n",
      "    Best SVM Params: {'C': 1.6, 'gamma': 'scale'} (CV F1: 0.242)\n",
      "  Level 0: Generating OOF predictions (7-Fold CV)...\n",
      "  Level 0 OOF Done. Meta Train Shape: (840, 3)\n",
      "  Level 0: Training base models on full train data (840 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (SVM) and Probability Threshold...\n",
      "    Tuning meta learner over 9 SVM param combinations...\n",
      "    Best Meta SVM Params Found: {'C': 0.1, 'gamma': 0.1} (Validation F1: 0.3125)\n",
      "    Tuning threshold over range [0.1  0.15 0.2  0.25 0.3  0.35 0.4  0.45 0.5  0.55 0.6  0.65 0.7  0.75\n",
      " 0.8  0.85]...\n",
      "    Best Threshold Found: 0.20 (Validation F1: 0.4444)\n",
      "  Level 1: Training final Meta-Learner (SVM)...\n",
      "  Level 1 Final Meta Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation (Test Window 168h): Acc=0.9286, Prc=0.0000, Rec=0.0000, F1=0.0000\n",
      "  Iteration 1 finished in 3.79 seconds.\n",
      "--------------------\n",
      "\n",
      "--- Iter 2/296 ---\n",
      "  Train Indices: [48:887], Eval Indices: [888:1055]\n",
      "  Train Target Dist: {0.0: 0.9714285714285714, 1.0: 0.02857142857142857}\n",
      "  Test Target Dist: {0.0: 0.9464285714285714, 1.0: 0.05357142857142857}\n",
      "  Using scale_pos_weight for XGB/LGBM: 34.0000\n",
      "  Grid searching base models...\n",
      "    Best XGB Params: {'max_depth': 4, 'n_estimators': 65, 'eta': 0.05, 'lambda': 1.5} (CV F1: 0.280)\n",
      "    Best LGBM Params: {'max_depth': 3, 'n_estimators': 75, 'learning_rate': 0.08, 'subsample': 0.75} (CV F1: 0.303)\n",
      "    Best SVM Params: {'C': 3.2, 'gamma': 'scale'} (CV F1: 0.166)\n",
      "  Level 0: Generating OOF predictions (7-Fold CV)...\n",
      "  Level 0 OOF Done. Meta Train Shape: (840, 3)\n",
      "  Level 0: Training base models on full train data (840 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (SVM) and Probability Threshold...\n",
      "    Tuning meta learner over 9 SVM param combinations...\n",
      "    Best Meta SVM Params Found: {'C': 5, 'gamma': 0.1} (Validation F1: 0.0000)\n",
      "    Tuning threshold over range [0.1  0.15 0.2  0.25 0.3  0.35 0.4  0.45 0.5  0.55 0.6  0.65 0.7  0.75\n",
      " 0.8  0.85]...\n",
      "    Best Threshold Found: 0.85 (Validation F1: 0.0000)\n",
      "  Level 1: Training final Meta-Learner (SVM)...\n",
      "  Level 1 Final Meta Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation (Test Window 168h): Acc=0.9464, Prc=0.0000, Rec=0.0000, F1=0.0000\n",
      "  Iteration 2 finished in 3.65 seconds.\n",
      "--------------------\n",
      "\n",
      "--- Iter 3/296 ---\n",
      "  Train Indices: [96:935], Eval Indices: [936:1103]\n",
      "  Train Target Dist: {0.0: 0.969047619047619, 1.0: 0.030952380952380953}\n",
      "  Test Target Dist: {0.0: 0.9583333333333334, 1.0: 0.041666666666666664}\n",
      "  Using scale_pos_weight for XGB/LGBM: 31.3077\n",
      "  Grid searching base models...\n",
      "    Best XGB Params: {'max_depth': 4, 'n_estimators': 65, 'eta': 0.05, 'lambda': 1.5} (CV F1: 0.213)\n",
      "    Best LGBM Params: {'max_depth': 3, 'n_estimators': 45, 'learning_rate': 0.08, 'subsample': 0.75} (CV F1: 0.217)\n",
      "    Best SVM Params: {'C': 3.2, 'gamma': 'scale'} (CV F1: 0.162)\n",
      "  Level 0: Generating OOF predictions (7-Fold CV)...\n",
      "  Level 0 OOF Done. Meta Train Shape: (840, 3)\n",
      "  Level 0: Training base models on full train data (840 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (SVM) and Probability Threshold...\n",
      "    Tuning meta learner over 9 SVM param combinations...\n",
      "    Best Meta SVM Params Found: {'C': 0.1, 'gamma': 0.1} (Validation F1: 0.3636)\n",
      "    Tuning threshold over range [0.1  0.15 0.2  0.25 0.3  0.35 0.4  0.45 0.5  0.55 0.6  0.65 0.7  0.75\n",
      " 0.8  0.85]...\n",
      "    Best Threshold Found: 0.10 (Validation F1: 0.4444)\n",
      "  Level 1: Training final Meta-Learner (SVM)...\n",
      "  Level 1 Final Meta Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation (Test Window 168h): Acc=0.9583, Prc=0.0000, Rec=0.0000, F1=0.0000\n",
      "  Iteration 3 finished in 3.59 seconds.\n",
      "--------------------\n",
      "\n",
      "--- Iter 4/296 ---\n",
      "  Train Indices: [144:983], Eval Indices: [984:1151]\n",
      "  Train Target Dist: {0.0: 0.9702380952380952, 1.0: 0.02976190476190476}\n",
      "  Test Target Dist: {0.0: 0.9285714285714286, 1.0: 0.07142857142857142}\n",
      "  Using scale_pos_weight for XGB/LGBM: 32.6000\n",
      "  Grid searching base models...\n",
      "    Best XGB Params: {'max_depth': 4, 'n_estimators': 35, 'eta': 0.03, 'lambda': 1.5} (CV F1: 0.254)\n",
      "    Best LGBM Params: {'max_depth': 3, 'n_estimators': 75, 'learning_rate': 0.08, 'subsample': 0.75} (CV F1: 0.207)\n",
      "    Best SVM Params: {'C': 3.2, 'gamma': 'scale'} (CV F1: 0.150)\n",
      "  Level 0: Generating OOF predictions (7-Fold CV)...\n",
      "  Level 0 OOF Done. Meta Train Shape: (840, 3)\n",
      "  Level 0: Training base models on full train data (840 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (SVM) and Probability Threshold...\n",
      "    Tuning meta learner over 9 SVM param combinations...\n",
      "    Best Meta SVM Params Found: {'C': 5, 'gamma': 'auto'} (Validation F1: 0.1905)\n",
      "    Tuning threshold over range [0.1  0.15 0.2  0.25 0.3  0.35 0.4  0.45 0.5  0.55 0.6  0.65 0.7  0.75\n",
      " 0.8  0.85]...\n",
      "    Best Threshold Found: 0.85 (Validation F1: 0.0000)\n",
      "  Level 1: Training final Meta-Learner (SVM)...\n",
      "  Level 1 Final Meta Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation (Test Window 168h): Acc=0.9286, Prc=0.0000, Rec=0.0000, F1=0.0000\n",
      "  Iteration 4 finished in 3.36 seconds.\n",
      "--------------------\n",
      "\n",
      "--- Iter 5/296 ---\n",
      "  Train Indices: [192:1031], Eval Indices: [1032:1199]\n",
      "  Train Target Dist: {0.0: 0.9642857142857143, 1.0: 0.03571428571428571}\n",
      "  Test Target Dist: {0.0: 0.9702380952380952, 1.0: 0.02976190476190476}\n",
      "  Using scale_pos_weight for XGB/LGBM: 27.0000\n",
      "  Grid searching base models...\n",
      "    Best XGB Params: {'max_depth': 4, 'n_estimators': 65, 'eta': 0.05, 'lambda': 2.5} (CV F1: 0.317)\n",
      "    Best LGBM Params: {'max_depth': 3, 'n_estimators': 75, 'learning_rate': 0.08, 'subsample': 0.75} (CV F1: 0.359)\n",
      "    Best SVM Params: {'C': 3.2, 'gamma': 'scale'} (CV F1: 0.234)\n",
      "  Level 0: Generating OOF predictions (7-Fold CV)...\n",
      "  Level 0 OOF Done. Meta Train Shape: (840, 3)\n",
      "  Level 0: Training base models on full train data (840 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (SVM) and Probability Threshold...\n",
      "    Tuning meta learner over 9 SVM param combinations...\n",
      "    Best Meta SVM Params Found: {'C': 5, 'gamma': 'auto'} (Validation F1: 0.4000)\n",
      "    Tuning threshold over range [0.1  0.15 0.2  0.25 0.3  0.35 0.4  0.45 0.5  0.55 0.6  0.65 0.7  0.75\n",
      " 0.8  0.85]...\n",
      "    Best Threshold Found: 0.10 (Validation F1: 0.4444)\n",
      "  Level 1: Training final Meta-Learner (SVM)...\n",
      "  Level 1 Final Meta Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation (Test Window 168h): Acc=0.8333, Prc=0.0741, Rec=0.4000, F1=0.1250\n",
      "  Iteration 5 finished in 3.63 seconds.\n",
      "--------------------\n",
      "\n",
      "--- Iter 6/296 ---\n",
      "  Train Indices: [240:1079], Eval Indices: [1080:1247]\n",
      "  Train Target Dist: {0.0: 0.9642857142857143, 1.0: 0.03571428571428571}\n",
      "  Test Target Dist: {0.0: 0.9642857142857143, 1.0: 0.03571428571428571}\n",
      "  Using scale_pos_weight for XGB/LGBM: 27.0000\n",
      "  Grid searching base models...\n",
      "    Best XGB Params: {'max_depth': 4, 'n_estimators': 65, 'eta': 0.03, 'lambda': 2.5} (CV F1: 0.378)\n",
      "    Best LGBM Params: {'max_depth': 3, 'n_estimators': 75, 'learning_rate': 0.08, 'subsample': 0.75} (CV F1: 0.398)\n",
      "    Best SVM Params: {'C': 1.6, 'gamma': 'scale'} (CV F1: 0.234)\n",
      "  Level 0: Generating OOF predictions (7-Fold CV)...\n",
      "  Level 0 OOF Done. Meta Train Shape: (840, 3)\n",
      "  Level 0: Training base models on full train data (840 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (SVM) and Probability Threshold...\n",
      "    Tuning meta learner over 9 SVM param combinations...\n",
      "    Best Meta SVM Params Found: {'C': 0.1, 'gamma': 'auto'} (Validation F1: 0.4706)\n",
      "    Tuning threshold over range [0.1  0.15 0.2  0.25 0.3  0.35 0.4  0.45 0.5  0.55 0.6  0.65 0.7  0.75\n",
      " 0.8  0.85]...\n",
      "    Best Threshold Found: 0.10 (Validation F1: 0.3478)\n",
      "  Level 1: Training final Meta-Learner (SVM)...\n",
      "  Level 1 Final Meta Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation (Test Window 168h): Acc=0.9286, Prc=0.1250, Rec=0.1667, F1=0.1429\n",
      "  Iteration 6 finished in 3.53 seconds.\n",
      "--------------------\n",
      "\n",
      "--- Iter 7/296 ---\n",
      "  Train Indices: [288:1127], Eval Indices: [1128:1295]\n",
      "  Train Target Dist: {0.0: 0.9654761904761905, 1.0: 0.034523809523809526}\n",
      "  Test Target Dist: {0.0: 0.9642857142857143, 1.0: 0.03571428571428571}\n",
      "  Using scale_pos_weight for XGB/LGBM: 27.9655\n",
      "  Grid searching base models...\n",
      "    Best XGB Params: {'max_depth': 4, 'n_estimators': 65, 'eta': 0.05, 'lambda': 1.5} (CV F1: 0.428)\n",
      "    Best LGBM Params: {'max_depth': 3, 'n_estimators': 75, 'learning_rate': 0.08, 'subsample': 0.75} (CV F1: 0.470)\n",
      "    Best SVM Params: {'C': 3.2, 'gamma': 'scale'} (CV F1: 0.255)\n",
      "  Level 0: Generating OOF predictions (7-Fold CV)...\n",
      "  Level 0 OOF Done. Meta Train Shape: (840, 3)\n",
      "  Level 0: Training base models on full train data (840 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (SVM) and Probability Threshold...\n",
      "    Tuning meta learner over 9 SVM param combinations...\n",
      "    Best Meta SVM Params Found: {'C': 0.1, 'gamma': 0.1} (Validation F1: 0.6667)\n",
      "    Tuning threshold over range [0.1  0.15 0.2  0.25 0.3  0.35 0.4  0.45 0.5  0.55 0.6  0.65 0.7  0.75\n",
      " 0.8  0.85]...\n",
      "    Best Threshold Found: 0.20 (Validation F1: 0.6000)\n",
      "  Level 1: Training final Meta-Learner (SVM)...\n",
      "  Level 1 Final Meta Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation (Test Window 168h): Acc=0.9524, Prc=0.0000, Rec=0.0000, F1=0.0000\n",
      "  Iteration 7 finished in 3.39 seconds.\n",
      "--------------------\n",
      "\n",
      "--- Iter 8/296 ---\n",
      "  Train Indices: [336:1175], Eval Indices: [1176:1343]\n",
      "  Train Target Dist: {0.0: 0.9595238095238096, 1.0: 0.04047619047619048}\n",
      "  Test Target Dist: {0.0: 0.9880952380952381, 1.0: 0.011904761904761904}\n",
      "  Using scale_pos_weight for XGB/LGBM: 23.7059\n",
      "  Grid searching base models...\n",
      "    Best XGB Params: {'max_depth': 4, 'n_estimators': 65, 'eta': 0.05, 'lambda': 2.5} (CV F1: 0.446)\n",
      "    Best LGBM Params: {'max_depth': 3, 'n_estimators': 75, 'learning_rate': 0.08, 'subsample': 0.75} (CV F1: 0.512)\n",
      "    Best SVM Params: {'C': 3.2, 'gamma': 'scale'} (CV F1: 0.258)\n",
      "  Level 0: Generating OOF predictions (7-Fold CV)...\n",
      "  Level 0 OOF Done. Meta Train Shape: (840, 3)\n",
      "  Level 0: Training base models on full train data (840 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (SVM) and Probability Threshold...\n",
      "    Tuning meta learner over 9 SVM param combinations...\n",
      "    Best Meta SVM Params Found: {'C': 5, 'gamma': 0.1} (Validation F1: 0.5000)\n",
      "    Tuning threshold over range [0.1  0.15 0.2  0.25 0.3  0.35 0.4  0.45 0.5  0.55 0.6  0.65 0.7  0.75\n",
      " 0.8  0.85]...\n",
      "    Best Threshold Found: 0.10 (Validation F1: 0.5217)\n",
      "  Level 1: Training final Meta-Learner (SVM)...\n",
      "  Level 1 Final Meta Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation (Test Window 168h): Acc=0.8036, Prc=0.0303, Rec=0.5000, F1=0.0571\n",
      "  Iteration 8 finished in 3.56 seconds.\n",
      "--------------------\n",
      "\n",
      "--- Iter 9/296 ---\n",
      "  Train Indices: [384:1223], Eval Indices: [1224:1391]\n",
      "  Train Target Dist: {0.0: 0.9619047619047619, 1.0: 0.0380952380952381}\n",
      "  Test Target Dist: {0.0: 0.9702380952380952, 1.0: 0.02976190476190476}\n",
      "  Using scale_pos_weight for XGB/LGBM: 25.2500\n",
      "  Grid searching base models...\n",
      "    Best XGB Params: {'max_depth': 4, 'n_estimators': 65, 'eta': 0.05, 'lambda': 2.5} (CV F1: 0.363)\n",
      "    Best LGBM Params: {'max_depth': 3, 'n_estimators': 75, 'learning_rate': 0.08, 'subsample': 0.75} (CV F1: 0.388)\n",
      "    Best SVM Params: {'C': 3.2, 'gamma': 'scale'} (CV F1: 0.271)\n",
      "  Level 0: Generating OOF predictions (7-Fold CV)...\n",
      "  Level 0 OOF Done. Meta Train Shape: (840, 3)\n",
      "  Level 0: Training base models on full train data (840 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (SVM) and Probability Threshold...\n",
      "    Tuning meta learner over 9 SVM param combinations...\n",
      "    Best Meta SVM Params Found: {'C': 5, 'gamma': 'auto'} (Validation F1: 0.4000)\n",
      "    Tuning threshold over range [0.1  0.15 0.2  0.25 0.3  0.35 0.4  0.45 0.5  0.55 0.6  0.65 0.7  0.75\n",
      " 0.8  0.85]...\n",
      "    Best Threshold Found: 0.15 (Validation F1: 0.5000)\n",
      "  Level 1: Training final Meta-Learner (SVM)...\n",
      "  Level 1 Final Meta Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation (Test Window 168h): Acc=0.8690, Prc=0.0000, Rec=0.0000, F1=0.0000\n",
      "  Iteration 9 finished in 3.43 seconds.\n",
      "--------------------\n",
      "\n",
      "--- Iter 10/296 ---\n",
      "  Train Indices: [432:1271], Eval Indices: [1272:1439]\n",
      "  Train Target Dist: {0.0: 0.9607142857142857, 1.0: 0.039285714285714285}\n",
      "  Test Target Dist: {0.0: 0.9761904761904762, 1.0: 0.023809523809523808}\n",
      "  Using scale_pos_weight for XGB/LGBM: 24.4545\n",
      "  Grid searching base models...\n",
      "    Best XGB Params: {'max_depth': 4, 'n_estimators': 65, 'eta': 0.05, 'lambda': 1.5} (CV F1: 0.255)\n",
      "    Best LGBM Params: {'max_depth': 3, 'n_estimators': 75, 'learning_rate': 0.08, 'subsample': 0.75} (CV F1: 0.311)\n",
      "    Best SVM Params: {'C': 3.2, 'gamma': 'scale'} (CV F1: 0.276)\n",
      "  Level 0: Generating OOF predictions (7-Fold CV)...\n",
      "  Level 0 OOF Done. Meta Train Shape: (840, 3)\n",
      "  Level 0: Training base models on full train data (840 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (SVM) and Probability Threshold...\n",
      "    Tuning meta learner over 9 SVM param combinations...\n",
      "    Best Meta SVM Params Found: {'C': 0.1, 'gamma': 0.1} (Validation F1: 0.3529)\n",
      "    Tuning threshold over range [0.1  0.15 0.2  0.25 0.3  0.35 0.4  0.45 0.5  0.55 0.6  0.65 0.7  0.75\n",
      " 0.8  0.85]...\n",
      "    Best Threshold Found: 0.20 (Validation F1: 0.4444)\n",
      "  Level 1: Training final Meta-Learner (SVM)...\n",
      "  Level 1 Final Meta Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation (Test Window 168h): Acc=0.9702, Prc=0.0000, Rec=0.0000, F1=0.0000\n",
      "  Iteration 10 finished in 3.39 seconds.\n",
      "--------------------\n",
      "\n",
      "--- Iter 11/296 ---\n",
      "  Train Indices: [480:1319], Eval Indices: [1320:1487]\n",
      "  Train Target Dist: {0.0: 0.9642857142857143, 1.0: 0.03571428571428571}\n",
      "  Test Target Dist: {0.0: 0.9404761904761905, 1.0: 0.05952380952380952}\n",
      "  Using scale_pos_weight for XGB/LGBM: 27.0000\n",
      "  Grid searching base models...\n",
      "    Best XGB Params: {'max_depth': 4, 'n_estimators': 65, 'eta': 0.05, 'lambda': 1.5} (CV F1: 0.339)\n",
      "    Best LGBM Params: {'max_depth': 3, 'n_estimators': 75, 'learning_rate': 0.08, 'subsample': 0.75} (CV F1: 0.464)\n",
      "    Best SVM Params: {'C': 3.2, 'gamma': 'scale'} (CV F1: 0.361)\n",
      "  Level 0: Generating OOF predictions (7-Fold CV)...\n",
      "  Level 0 OOF Done. Meta Train Shape: (840, 3)\n",
      "  Level 0: Training base models on full train data (840 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (SVM) and Probability Threshold...\n",
      "    Tuning meta learner over 9 SVM param combinations...\n",
      "    Best Meta SVM Params Found: {'C': 3, 'gamma': 0.1} (Validation F1: 0.3333)\n",
      "    Tuning threshold over range [0.1  0.15 0.2  0.25 0.3  0.35 0.4  0.45 0.5  0.55 0.6  0.65 0.7  0.75\n",
      " 0.8  0.85]...\n",
      "    Best Threshold Found: 0.10 (Validation F1: 0.2667)\n",
      "  Level 1: Training final Meta-Learner (SVM)...\n",
      "  Level 1 Final Meta Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation (Test Window 168h): Acc=0.7857, Prc=0.0357, Rec=0.1000, F1=0.0526\n",
      "  Iteration 11 finished in 3.56 seconds.\n",
      "--------------------\n",
      "\n",
      "--- Iter 12/296 ---\n",
      "  Train Indices: [528:1367], Eval Indices: [1368:1535]\n",
      "  Train Target Dist: {0.0: 0.9702380952380952, 1.0: 0.02976190476190476}\n",
      "  Test Target Dist: {0.0: 0.9107142857142857, 1.0: 0.08928571428571429}\n",
      "  Using scale_pos_weight for XGB/LGBM: 32.6000\n",
      "  Grid searching base models...\n",
      "    Best XGB Params: {'max_depth': 4, 'n_estimators': 65, 'eta': 0.03, 'lambda': 1.5} (CV F1: 0.263)\n",
      "    Best LGBM Params: {'max_depth': 2, 'n_estimators': 75, 'learning_rate': 0.08, 'subsample': 0.75} (CV F1: 0.253)\n",
      "    Best SVM Params: {'C': 3.2, 'gamma': 'scale'} (CV F1: 0.375)\n",
      "  Level 0: Generating OOF predictions (7-Fold CV)...\n",
      "  Level 0 OOF Done. Meta Train Shape: (840, 3)\n",
      "  Level 0: Training base models on full train data (840 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (SVM) and Probability Threshold...\n",
      "    Tuning meta learner over 9 SVM param combinations...\n",
      "    Best Meta SVM Params Found: {'C': 5, 'gamma': 'scale'} (Validation F1: 0.1667)\n",
      "    Tuning threshold over range [0.1  0.15 0.2  0.25 0.3  0.35 0.4  0.45 0.5  0.55 0.6  0.65 0.7  0.75\n",
      " 0.8  0.85]...\n",
      "    Best Threshold Found: 0.85 (Validation F1: 0.0000)\n",
      "  Level 1: Training final Meta-Learner (SVM)...\n",
      "  Level 1 Final Meta Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation (Test Window 168h): Acc=0.9107, Prc=0.0000, Rec=0.0000, F1=0.0000\n",
      "  Iteration 12 finished in 3.37 seconds.\n",
      "--------------------\n",
      "\n",
      "--- Iter 13/296 ---\n",
      "  Train Indices: [576:1415], Eval Indices: [1416:1583]\n",
      "  Train Target Dist: {0.0: 0.9666666666666667, 1.0: 0.03333333333333333}\n",
      "  Test Target Dist: {0.0: 0.9226190476190477, 1.0: 0.07738095238095238}\n",
      "  Using scale_pos_weight for XGB/LGBM: 29.0000\n",
      "  Grid searching base models...\n",
      "    Best XGB Params: {'max_depth': 4, 'n_estimators': 65, 'eta': 0.05, 'lambda': 1.5} (CV F1: 0.244)\n",
      "    Best LGBM Params: {'max_depth': 2, 'n_estimators': 75, 'learning_rate': 0.08, 'subsample': 0.75} (CV F1: 0.259)\n",
      "    Best SVM Params: {'C': 3.2, 'gamma': 'scale'} (CV F1: 0.313)\n",
      "  Level 0: Generating OOF predictions (7-Fold CV)...\n",
      "  Level 0 OOF Done. Meta Train Shape: (840, 3)\n",
      "  Level 0: Training base models on full train data (840 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (SVM) and Probability Threshold...\n",
      "    Tuning meta learner over 9 SVM param combinations...\n",
      "    Best Meta SVM Params Found: {'C': 3, 'gamma': 'auto'} (Validation F1: 0.3448)\n",
      "    Tuning threshold over range [0.1  0.15 0.2  0.25 0.3  0.35 0.4  0.45 0.5  0.55 0.6  0.65 0.7  0.75\n",
      " 0.8  0.85]...\n",
      "    Best Threshold Found: 0.10 (Validation F1: 0.2857)\n",
      "  Level 1: Training final Meta-Learner (SVM)...\n",
      "  Level 1 Final Meta Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation (Test Window 168h): Acc=0.8512, Prc=0.0714, Rec=0.0769, F1=0.0741\n",
      "  Iteration 13 finished in 3.28 seconds.\n",
      "--------------------\n",
      "\n",
      "--- Iter 14/296 ---\n",
      "  Train Indices: [624:1463], Eval Indices: [1464:1631]\n",
      "  Train Target Dist: {0.0: 0.9678571428571429, 1.0: 0.03214285714285714}\n",
      "  Test Target Dist: {0.0: 0.8988095238095238, 1.0: 0.10119047619047619}\n",
      "  Using scale_pos_weight for XGB/LGBM: 30.1111\n",
      "  Grid searching base models...\n",
      "    Best XGB Params: {'max_depth': 4, 'n_estimators': 35, 'eta': 0.03, 'lambda': 1.5} (CV F1: 0.299)\n",
      "    Best LGBM Params: {'max_depth': 3, 'n_estimators': 75, 'learning_rate': 0.08, 'subsample': 0.75} (CV F1: 0.320)\n",
      "    Best SVM Params: {'C': 3.2, 'gamma': 'scale'} (CV F1: 0.303)\n",
      "  Level 0: Generating OOF predictions (7-Fold CV)...\n",
      "  Level 0 OOF Done. Meta Train Shape: (840, 3)\n",
      "  Level 0: Training base models on full train data (840 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (SVM) and Probability Threshold...\n",
      "    Tuning meta learner over 9 SVM param combinations...\n",
      "    Best Meta SVM Params Found: {'C': 0.1, 'gamma': 0.1} (Validation F1: 0.1935)\n",
      "    Tuning threshold over range [0.1  0.15 0.2  0.25 0.3  0.35 0.4  0.45 0.5  0.55 0.6  0.65 0.7  0.75\n",
      " 0.8  0.85]...\n",
      "    Best Threshold Found: 0.15 (Validation F1: 0.2105)\n",
      "  Level 1: Training final Meta-Learner (SVM)...\n",
      "  Level 1 Final Meta Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation (Test Window 168h): Acc=0.8393, Prc=0.0833, Rec=0.0588, F1=0.0690\n",
      "  Iteration 14 finished in 3.27 seconds.\n",
      "--------------------\n",
      "\n",
      "--- Iter 15/296 ---\n",
      "  Train Indices: [672:1511], Eval Indices: [1512:1679]\n",
      "  Train Target Dist: {0.0: 0.9654761904761905, 1.0: 0.034523809523809526}\n",
      "  Test Target Dist: {0.0: 0.9166666666666666, 1.0: 0.08333333333333333}\n",
      "  Using scale_pos_weight for XGB/LGBM: 27.9655\n",
      "  Grid searching base models...\n",
      "    Best XGB Params: {'max_depth': 4, 'n_estimators': 35, 'eta': 0.05, 'lambda': 1.5} (CV F1: 0.397)\n",
      "    Best LGBM Params: {'max_depth': 2, 'n_estimators': 75, 'learning_rate': 0.08, 'subsample': 0.75} (CV F1: 0.395)\n",
      "    Best SVM Params: {'C': 3.2, 'gamma': 'scale'} (CV F1: 0.338)\n",
      "  Level 0: Generating OOF predictions (7-Fold CV)...\n",
      "  Level 0 OOF Done. Meta Train Shape: (840, 3)\n",
      "  Level 0: Training base models on full train data (840 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (SVM) and Probability Threshold...\n",
      "    Tuning meta learner over 9 SVM param combinations...\n",
      "    Best Meta SVM Params Found: {'C': 0.1, 'gamma': 'scale'} (Validation F1: 0.2182)\n",
      "    Tuning threshold over range [0.1  0.15 0.2  0.25 0.3  0.35 0.4  0.45 0.5  0.55 0.6  0.65 0.7  0.75\n",
      " 0.8  0.85]...\n",
      "    Best Threshold Found: 0.10 (Validation F1: 0.2326)\n",
      "  Level 1: Training final Meta-Learner (SVM)...\n",
      "  Level 1 Final Meta Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation (Test Window 168h): Acc=0.9048, Prc=0.0000, Rec=0.0000, F1=0.0000\n",
      "  Iteration 15 finished in 3.06 seconds.\n",
      "--------------------\n",
      "\n",
      "--- Iter 16/296 ---\n",
      "  Train Indices: [720:1559], Eval Indices: [1560:1727]\n",
      "  Train Target Dist: {0.0: 0.9607142857142857, 1.0: 0.039285714285714285}\n",
      "  Test Target Dist: {0.0: 0.9404761904761905, 1.0: 0.05952380952380952}\n",
      "  Using scale_pos_weight for XGB/LGBM: 24.4545\n",
      "  Grid searching base models...\n",
      "    Best XGB Params: {'max_depth': 4, 'n_estimators': 65, 'eta': 0.05, 'lambda': 2.5} (CV F1: 0.403)\n",
      "    Best LGBM Params: {'max_depth': 3, 'n_estimators': 75, 'learning_rate': 0.04, 'subsample': 0.75} (CV F1: 0.322)\n",
      "    Best SVM Params: {'C': 3.2, 'gamma': 'scale'} (CV F1: 0.309)\n",
      "  Level 0: Generating OOF predictions (7-Fold CV)...\n",
      "  Level 0 OOF Done. Meta Train Shape: (840, 3)\n",
      "  Level 0: Training base models on full train data (840 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (SVM) and Probability Threshold...\n",
      "    Tuning meta learner over 9 SVM param combinations...\n",
      "    Best Meta SVM Params Found: {'C': 0.1, 'gamma': 0.1} (Validation F1: 0.2737)\n",
      "    Tuning threshold over range [0.1  0.15 0.2  0.25 0.3  0.35 0.4  0.45 0.5  0.55 0.6  0.65 0.7  0.75\n",
      " 0.8  0.85]...\n",
      "    Best Threshold Found: 0.25 (Validation F1: 0.3529)\n",
      "  Level 1: Training final Meta-Learner (SVM)...\n",
      "  Level 1 Final Meta Training Done.\n",
      "  Prediction: Generating final predictions...\n",
      "  Prediction Done.\n",
      "  Evaluation (Test Window 168h): Acc=0.9405, Prc=0.5000, Rec=0.2000, F1=0.2857\n",
      "  Iteration 16 finished in 3.59 seconds.\n",
      "--------------------\n",
      "\n",
      "--- Iter 17/296 ---\n",
      "  Train Indices: [768:1607], Eval Indices: [1608:1775]\n",
      "  Train Target Dist: {0.0: 0.9595238095238096, 1.0: 0.04047619047619048}\n",
      "  Test Target Dist: {0.0: 0.9464285714285714, 1.0: 0.05357142857142857}\n",
      "  Using scale_pos_weight for XGB/LGBM: 23.7059\n",
      "  Grid searching base models...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 332\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Grid searching base models...\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# Base model search\u001b[39;00m\n\u001b[0;32m    331\u001b[0m best_xgb_params, xgb_score \u001b[38;5;241m=\u001b[39m grid_search_base_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxgb\u001b[39m\u001b[38;5;124m'\u001b[39m, BASE_XGB_PARAM_GRID, X_train_full, y_train_full, scale_pos_weight_val)\n\u001b[1;32m--> 332\u001b[0m best_lgbm_params, lgbm_score \u001b[38;5;241m=\u001b[39m \u001b[43mgrid_search_base_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlgbm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBASE_LGBM_PARAM_GRID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_full\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_full\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_pos_weight_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    333\u001b[0m best_svm_params, svm_score \u001b[38;5;241m=\u001b[39m grid_search_base_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvm\u001b[39m\u001b[38;5;124m'\u001b[39m, BASE_SVM_PARAM_GRID, X_train_full, y_train_full, scale_pos_weight_val)\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    Best XGB Params: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_xgb_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (CV F1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mxgb_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m); \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    Best LGBM Params: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_lgbm_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (CV F1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlgbm_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m); \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    Best SVM Params: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_svm_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (CV F1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msvm_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 244\u001b[0m, in \u001b[0;36mgrid_search_base_model\u001b[1;34m(model_type, base_param_grid, X, y, scale_pos_weight_val)\u001b[0m\n\u001b[0;32m    242\u001b[0m          model \u001b[38;5;241m=\u001b[39m Pipeline([(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimputer\u001b[39m\u001b[38;5;124m'\u001b[39m, SimpleImputer(strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmedian\u001b[39m\u001b[38;5;124m'\u001b[39m)), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaler\u001b[39m\u001b[38;5;124m'\u001b[39m, StandardScaler()), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvm\u001b[39m\u001b[38;5;124m'\u001b[39m, SVC(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfixed_params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msvm_grid_params))])\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 244\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_inner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_inner\u001b[49m\u001b[43m)\u001b[49m; y_pred_inner \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_val_inner)\n\u001b[0;32m    245\u001b[0m     score \u001b[38;5;241m=\u001b[39m f1_score(y_val_inner, y_pred_inner, zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m); scores\u001b[38;5;241m.\u001b[39mappend(score)\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e: scores\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\lightgbm\\sklearn.py:1560\u001b[0m, in \u001b[0;36mLGBMClassifier.fit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m   1557\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1558\u001b[0m             valid_sets\u001b[38;5;241m.\u001b[39mappend((valid_x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_le\u001b[38;5;241m.\u001b[39mtransform(valid_y)))\n\u001b[1;32m-> 1560\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1561\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1562\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1563\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1565\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1566\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1567\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1568\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_class_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_class_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1569\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_init_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1570\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1572\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1573\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1574\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1575\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1576\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\lightgbm\\sklearn.py:1049\u001b[0m, in \u001b[0;36mLGBMModel.fit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m   1046\u001b[0m evals_result: _EvalResultDict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m   1047\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mappend(record_evaluation(evals_result))\n\u001b[1;32m-> 1049\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metrics_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1058\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;66;03m# This populates the property self.n_features_, the number of features in the fitted model,\u001b[39;00m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;66;03m# and so should only be set after fitting.\u001b[39;00m\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;66;03m# The related property self._n_features_in, which populates self.n_features_in_,\u001b[39;00m\n\u001b[0;32m   1064\u001b[0m \u001b[38;5;66;03m# is set BEFORE fitting.\u001b[39;00m\n\u001b[0;32m   1065\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster\u001b[38;5;241m.\u001b[39mnum_feature()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\lightgbm\\engine.py:322\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_before_iter:\n\u001b[0;32m    311\u001b[0m     cb(\n\u001b[0;32m    312\u001b[0m         callback\u001b[38;5;241m.\u001b[39mCallbackEnv(\n\u001b[0;32m    313\u001b[0m             model\u001b[38;5;241m=\u001b[39mbooster,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    319\u001b[0m         )\n\u001b[0;32m    320\u001b[0m     )\n\u001b[1;32m--> 322\u001b[0m \u001b[43mbooster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    324\u001b[0m evaluation_result_list: List[_LGBM_BoosterEvalMethodResultType] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    325\u001b[0m \u001b[38;5;66;03m# check evaluation result.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\lightgbm\\basic.py:4155\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, train_set, fobj)\u001b[0m\n\u001b[0;32m   4152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_objective_to_none:\n\u001b[0;32m   4153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot update due to null objective function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   4154\u001b[0m _safe_call(\n\u001b[1;32m-> 4155\u001b[0m     \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLGBM_BoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4156\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_finished\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4158\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4159\u001b[0m )\n\u001b[0;32m   4160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__is_predicted_cur_iter \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__num_dataset)]\n\u001b[0;32m   4161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m is_finished\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Ensemble_Method_D.py\n",
    "# Combined Script: Load CSV -> Feature Engineering -> Rolling Origin Stacking\n",
    "# Uses feature set from Simple_Predictor_B\n",
    "# Meta-Learner: SVM with RBF Kernel\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "\n",
    "# Modeling Imports\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "from sklearn.svm import SVC # Base learner AND Meta learner\n",
    "from sklearn.preprocessing import StandardScaler # For SVM base and meta\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import ParameterGrid, StratifiedKFold\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# --- Suppress Warnings ---\n",
    "warnings.filterwarnings('ignore', category=UndefinedMetricWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "# Data Loading\n",
    "CSV_FILE_PATH = r'C:\\Users\\mason\\AVP\\BTCUSDrec.csv'\n",
    "SYMBOL_NAME = 'BTCUSD'\n",
    "\n",
    "# Feature Selection (From Simple_Predictor_B)\n",
    "SELECTED_FEATURE_NAMES = [\n",
    "    'open', 'high', 'low', 'close', 'Volume BTC', 'Volume USD',\n",
    "    'price_range_pct', 'oc_change_pct', 'garman_klass_12h', 'parkinson_3h',\n",
    "    'ma_3h', 'rolling_std_3h', 'lag_3h_price_return', 'lag_6h_price_return',\n",
    "    'lag_12h_price_return', 'lag_24h_price_return', 'lag_48h_price_return',\n",
    "    'lag_72h_price_return', 'lag_168h_price_return', 'volume_return_1h',\n",
    "    'lag_3h_volume_return', 'lag_6h_volume_return', 'lag_12h_volume_return',\n",
    "    'lag_24h_volume_return', 'ma_6h', 'ma_12h', 'ma_24h', 'ma_48h',\n",
    "    'ma_72h', 'ma_168h', 'rolling_std_6h', 'rolling_std_12h',\n",
    "    'rolling_std_24h', 'rolling_std_48h', 'rolling_std_72h',\n",
    "    'rolling_std_168h', 'atr_14h', 'atr_24h', 'atr_48h', 'close_div_ma_24h',\n",
    "    'close_div_ma_48h', 'close_div_ma_168h', 'ma12_div_ma48',\n",
    "    'ma24_div_ma168', 'std12_div_std72', 'volume_btc_x_range',\n",
    "    'rolling_std_3h_sq', 'price_return_1h_sq', 'rolling_std_12h_sqrt'\n",
    "]\n",
    "MODEL_FEATURE_COLS = [f for f in SELECTED_FEATURE_NAMES if f not in ['open', 'high', 'low', 'close', 'Volume BTC', 'Volume USD']]\n",
    "\n",
    "# Modeling & Walk-Forward\n",
    "TARGET_THRESHOLD_PCT = 0.8\n",
    "PREDICTION_WINDOW_HOURS = 3\n",
    "PREDICTION_WINDOW_ROWS = PREDICTION_WINDOW_HOURS\n",
    "\n",
    "# Walk-forward params (Same as C)\n",
    "TRAIN_WINDOW_HOURS = int(24 * 7 * 5)\n",
    "TEST_WINDOW_HOURS = 24 * 7\n",
    "STEP_HOURS = 48\n",
    "\n",
    "TRAIN_WINDOW_ROWS = TRAIN_WINDOW_HOURS\n",
    "TEST_WINDOW_ROWS = TEST_WINDOW_HOURS\n",
    "STEP_ROWS = STEP_HOURS\n",
    "\n",
    "# Stacking Configuration\n",
    "N_STACKING_FOLDS = 7\n",
    "\n",
    "# --- Base Model Parameter Grids (For per-iteration tuning) ---\n",
    "BASE_XGB_PARAM_GRID = {\n",
    "    'max_depth': [2, 4], 'n_estimators': [35, 65],\n",
    "    'eta': [0.03, 0.05], 'lambda': [1.5, 2.5]\n",
    "}\n",
    "BASE_LGBM_PARAM_GRID = {\n",
    "    'max_depth': [2, 3], 'n_estimators': [45, 75],\n",
    "    'learning_rate': [0.04, 0.08], 'subsample': [0.75, 0.9]\n",
    "}\n",
    "BASE_SVM_PARAM_GRID = { # Base SVM grid remains the same\n",
    "    'C': [1.6, 3.2], 'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# --- Base Model Static/Fixed Hyperparameters ---\n",
    "XGB_BASE_PARAMS = {\n",
    "    'objective': 'binary:logistic', 'eval_metric': 'logloss',\n",
    "    'subsample': 0.8, 'colsample_bytree': 0.7, 'min_child_weight': 2,\n",
    "    'gamma': 0.1, 'alpha': 0.1, 'random_state': 42, 'n_jobs': -1,\n",
    "    'tree_method': 'hist', 'use_label_encoder': False,\n",
    "}\n",
    "LGBM_BASE_PARAMS = {\n",
    "    'objective': 'binary', 'metric': 'logloss', 'num_leaves': 8,\n",
    "    'colsample_bytree': 0.7, 'min_child_samples': 5, 'reg_alpha': 0.1,\n",
    "    'reg_lambda': 1.5, 'random_state': 42, 'n_jobs': -1,\n",
    "    'boosting_type': 'gbdt', 'verbose': -1\n",
    "}\n",
    "SVM_BASE_PARAMS = { # For base SVM\n",
    "    'kernel': 'rbf', 'probability': True, 'max_iter': 5000,\n",
    "    'random_state': 42, 'class_weight': 'balanced'\n",
    "}\n",
    "\n",
    "# --- Meta Learner Configuration (SVM with RBF Kernel) ---\n",
    "META_SVM_PARAM_GRID = {\n",
    "    'C': [0.1, 3, 5],          # Regularization strength\n",
    "    'gamma': ['scale', 'auto', 0.1] # Kernel coefficient ('scale'/'auto' are often good starting points)\n",
    "}\n",
    "META_SVM_FIXED_PARAMS = {\n",
    "    'kernel': 'rbf',\n",
    "    'probability': True,          # MUST be True for threshold tuning\n",
    "    'class_weight': 'balanced', # Important for potentially imbalanced meta-features\n",
    "    'max_iter': 5000,            # Set a reasonable limit\n",
    "    'random_state': 123,\n",
    "}\n",
    "\n",
    "# --- Probability Threshold Tuning Configuration ---\n",
    "THRESHOLD_SEARCH_RANGE = np.arange(0.10, 0.90, 0.05)\n",
    "META_VALIDATION_PCT = 0.25\n",
    "\n",
    "# --- Feature Engineering Functions (Copied from Ensemble_Method_C) ---\n",
    "def garman_klass_volatility(open_, high, low, close, window):\n",
    "    log_hl = np.log(high / low)\n",
    "    log_co = np.log(close / open_)\n",
    "    gk = 0.5 * (log_hl ** 2) - (2 * np.log(2) - 1) * (log_co ** 2)\n",
    "    gk = gk.fillna(0)\n",
    "    rolling_mean = gk.rolling(window=window, min_periods=max(1, window // 2)).mean()\n",
    "    rolling_mean = rolling_mean.clip(lower=0)\n",
    "    return np.sqrt(rolling_mean)\n",
    "\n",
    "def parkinson_volatility(high, low, window):\n",
    "    log_hl_sq = np.log(high / low) ** 2\n",
    "    log_hl_sq = log_hl_sq.fillna(0)\n",
    "    rolling_sum = log_hl_sq.rolling(window=window, min_periods=max(1, window // 2)).sum()\n",
    "    factor = 1 / (4 * np.log(2) * window)\n",
    "    return np.sqrt(factor * rolling_sum)\n",
    "\n",
    "def calculate_selected_features(df, symbol):\n",
    "    print(f\"Starting calculation for {len(SELECTED_FEATURE_NAMES)} target columns (incl. base)...\")\n",
    "    start_time = time.time()\n",
    "    if df is None or len(df) < 3: return pd.DataFrame()\n",
    "    df = df.copy(); df['symbol'] = symbol\n",
    "    if 'timestamp' not in df.columns: print(\"Error: 'timestamp' column not found.\"); return pd.DataFrame()\n",
    "    try: df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    except Exception as e: print(f\"Error converting timestamp: {e}\"); return pd.DataFrame()\n",
    "    df = df.sort_values('timestamp').dropna(subset=['timestamp'])\n",
    "    df = df.set_index('timestamp', drop=False)\n",
    "    original_vol_btc_name = 'Volume BTC'; original_vol_usd_name = 'Volume USD'\n",
    "    if original_vol_btc_name not in df.columns: df[original_vol_btc_name] = 0\n",
    "    if original_vol_usd_name not in df.columns: df[original_vol_usd_name] = 0\n",
    "    df[original_vol_btc_name] = pd.to_numeric(df[original_vol_btc_name], errors='coerce').fillna(0)\n",
    "    df[original_vol_usd_name] = pd.to_numeric(df[original_vol_usd_name], errors='coerce').fillna(0)\n",
    "    required_ohlc = ['open', 'high', 'low', 'close']\n",
    "    if not all(col in df.columns for col in required_ohlc): print(f\"Error: Missing OHLC columns.\"); return pd.DataFrame()\n",
    "    for col in required_ohlc: df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    if df[required_ohlc].isnull().any().any(): print(\"Warning: NaNs in OHLC, dropping rows.\"); df = df.dropna(subset=required_ohlc)\n",
    "    if df.empty: print(\"DataFrame empty after OHLC checks.\"); return pd.DataFrame()\n",
    "    print(\"  Calculating features...\")\n",
    "    min_periods_rolling = 2\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        df['price_range_pct'] = (df['high'] - df['low']) / df['close'].replace(0, np.nan)\n",
    "        df['oc_change_pct'] = (df['close'] - df['open']) / df['open'].replace(0, np.nan)\n",
    "        df['price_return_1h_temp'] = df['close'].pct_change()\n",
    "        df['volume_return_1h'] = df[original_vol_btc_name].pct_change()\n",
    "    lag_price_hours = [3, 6, 12, 24, 48, 72, 168]; lag_volume_hours = [3, 6, 12, 24]\n",
    "    for hours in lag_price_hours: df[f'lag_{hours}h_price_return'] = df['close'].pct_change(periods=hours)\n",
    "    for hours in lag_volume_hours: df[f'lag_{hours}h_volume_return'] = df[original_vol_btc_name].pct_change(periods=hours)\n",
    "    ma_hours = [3, 6, 12, 24, 48, 72, 168]\n",
    "    for hours in ma_hours: df[f'ma_{hours}h'] = df['close'].rolling(window=hours, min_periods=max(min_periods_rolling, hours // 2)).mean()\n",
    "    std_hours = [3, 6, 12, 24, 48, 72, 168]\n",
    "    if 'price_return_1h_temp' in df.columns:\n",
    "        for hours in std_hours: df[f'rolling_std_{hours}h'] = df['price_return_1h_temp'].rolling(window=hours, min_periods=max(min_periods_rolling, hours // 2)).std() * 100\n",
    "    else:\n",
    "        for hours in std_hours: df[f'rolling_std_{hours}h'] = np.nan\n",
    "    print(\"    Calculating ATR, Garman-Klass, Parkinson features...\")\n",
    "    df['prev_close'] = df['close'].shift(1); df['high_minus_low'] = df['high'] - df['low']\n",
    "    df['high_minus_prev_close'] = np.abs(df['high'] - df['prev_close']); df['low_minus_prev_close'] = np.abs(df['low'] - df['prev_close'])\n",
    "    df['true_range'] = df[['high_minus_low', 'high_minus_prev_close', 'low_minus_prev_close']].max(axis=1)\n",
    "    for p in [14, 24, 48]: df[f'atr_{p}h'] = df['true_range'].rolling(window=p, min_periods=max(1, p // 2)).mean()\n",
    "    df = df.drop(columns=['prev_close', 'high_minus_low', 'high_minus_prev_close', 'low_minus_prev_close', 'true_range'])\n",
    "    df['garman_klass_12h'] = garman_klass_volatility(df['open'], df['high'], df['low'], df['close'], window=12)\n",
    "    df['parkinson_3h'] = parkinson_volatility(df['high'], df['low'], window=3)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        for hours in [24, 48, 168]:\n",
    "            ma_col = f'ma_{hours}h'\n",
    "            if ma_col in df.columns: df[f'close_div_ma_{hours}h'] = df['close'] / df[ma_col].replace(0, np.nan)\n",
    "            else: df[f'close_div_ma_{hours}h'] = np.nan\n",
    "        if 'ma_12h' in df.columns and 'ma_48h' in df.columns: df['ma12_div_ma48'] = df['ma_12h'] / df['ma_48h'].replace(0, np.nan)\n",
    "        else: df['ma12_div_ma48'] = np.nan\n",
    "        if 'ma_24h' in df.columns and 'ma_168h' in df.columns: df['ma24_div_ma168'] = df['ma_24h'] / df['ma_168h'].replace(0, np.nan)\n",
    "        else: df['ma24_div_ma168'] = np.nan\n",
    "        if 'rolling_std_12h' in df.columns and 'rolling_std_72h' in df.columns: df['std12_div_std72'] = df['rolling_std_12h'] / df['rolling_std_72h'].replace(0, np.nan)\n",
    "        else: df['std12_div_std72'] = np.nan\n",
    "        if original_vol_btc_name in df.columns and 'price_range_pct' in df.columns: df['volume_btc_x_range'] = df[original_vol_btc_name] * df['price_range_pct']\n",
    "        else: df['volume_btc_x_range'] = np.nan\n",
    "    if 'rolling_std_3h' in df.columns: df['rolling_std_3h_sq'] = df['rolling_std_3h'] ** 2\n",
    "    else: df['rolling_std_3h_sq'] = np.nan\n",
    "    if 'price_return_1h_temp' in df.columns: df['price_return_1h_sq'] = (df['price_return_1h_temp'] ** 2) * 10000\n",
    "    else: df['price_return_1h_sq'] = np.nan\n",
    "    if 'rolling_std_12h' in df.columns:\n",
    "        epsilon = 1e-9; df['rolling_std_12h_sqrt'] = np.sqrt(df['rolling_std_12h'].clip(lower=0) + epsilon)\n",
    "    else: df['rolling_std_12h_sqrt'] = np.nan\n",
    "    if 'price_return_1h_temp' in df.columns: df = df.drop(columns=['price_return_1h_temp'])\n",
    "    print(\"  Assembling final dataframe...\")\n",
    "    final_cols_present = [col for col in SELECTED_FEATURE_NAMES if col in df.columns]\n",
    "    df_final = df[final_cols_present + ['timestamp', 'symbol']].copy()\n",
    "    missing_final_cols = set(SELECTED_FEATURE_NAMES) - set(df_final.columns)\n",
    "    if missing_final_cols: print(f\"  Final Warning: {len(missing_final_cols)} target columns missing: {missing_final_cols}\")\n",
    "    df_final = df_final.reset_index(drop=True)\n",
    "    df_final = df_final.replace([np.inf, -np.inf], np.nan)\n",
    "    end_time = time.time()\n",
    "    actual_feature_count = len([col for col in df_final.columns if col not in ['timestamp', 'symbol', 'open', 'high', 'low', 'close', 'Volume BTC', 'Volume USD']])\n",
    "    print(f\"Selected feature calculation finished. Returning {len(df_final)} rows, {len(df_final.columns)} columns ({actual_feature_count} features). Took {end_time - start_time:.2f}s.\")\n",
    "    return df_final\n",
    "\n",
    "# --- Helper: Grid Search for Base Models (Copied from Ensemble_Method_C) ---\n",
    "def grid_search_base_model(model_type, base_param_grid, X, y, scale_pos_weight_val):\n",
    "    best_score = -np.inf; best_params = None\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    fixed_params = {}; full_param_list = []\n",
    "    if model_type == 'xgb': fixed_params = XGB_BASE_PARAMS\n",
    "    elif model_type == 'lgbm': fixed_params = LGBM_BASE_PARAMS\n",
    "    elif model_type == 'svm': fixed_params = SVM_BASE_PARAMS\n",
    "    for grid_p in ParameterGrid(base_param_grid): p = fixed_params.copy(); p.update(grid_p); full_param_list.append(p)\n",
    "    if not full_param_list: full_param_list.append(fixed_params)\n",
    "    for params in full_param_list:\n",
    "        scores = []\n",
    "        if len(np.unique(y)) < 2: print(f\"    Skipping CV for {model_type}: Target single class.\"); return list(ParameterGrid(base_param_grid))[0] if base_param_grid else {}, 0.0\n",
    "        for train_idx, val_idx in cv.split(X, y):\n",
    "            y_val_inner = y.iloc[val_idx]\n",
    "            if len(np.unique(y_val_inner)) < 2: scores.append(0); continue\n",
    "            X_train_inner, y_train_inner = X.iloc[train_idx], y.iloc[train_idx]; X_val_inner = X.iloc[val_idx]\n",
    "            try:\n",
    "                if model_type == 'xgb':\n",
    "                    params_xgb = {k: v for k, v in params.items() if k not in ['C', 'gamma', 'kernel', 'probability', 'max_iter', 'class_weight']}\n",
    "                    model = XGBClassifier(**params_xgb, scale_pos_weight=scale_pos_weight_val)\n",
    "                elif model_type == 'lgbm':\n",
    "                     params_lgbm = {k: v for k, v in params.items() if k not in ['C', 'gamma', 'kernel', 'probability', 'max_iter', 'class_weight']}\n",
    "                     model = LGBMClassifier(**params_lgbm, scale_pos_weight=scale_pos_weight_val)\n",
    "                elif model_type == 'svm':\n",
    "                     svm_grid_params = {k: params[k] for k in base_param_grid if k in params}\n",
    "                     model = Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler()), ('svm', SVC(**fixed_params, **svm_grid_params))])\n",
    "                else: continue\n",
    "                model.fit(X_train_inner, y_train_inner); y_pred_inner = model.predict(X_val_inner)\n",
    "                score = f1_score(y_val_inner, y_pred_inner, zero_division=0); scores.append(score)\n",
    "            except Exception as e: scores.append(0)\n",
    "        mean_score = np.mean(scores) if scores else 0\n",
    "        if mean_score > best_score: best_score = mean_score; best_params = {k: params[k] for k in base_param_grid if k in params}\n",
    "    if best_params is None and base_param_grid: best_params = list(ParameterGrid(base_param_grid))[0]\n",
    "    return best_params, best_score\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"--- Ensemble_Method_D (SVM Meta-Learner) ---\")\n",
    "    print(\"--- 1. Data Loading & Initial Prep ---\")\n",
    "    try: # Basic Data Loading\n",
    "        print(f\"Loading data from: {CSV_FILE_PATH}\"); col_names = ['unix', 'date', 'symbol_csv', 'open', 'high', 'low', 'close', 'Volume BTC', 'Volume USD']\n",
    "        df_raw = pd.read_csv(CSV_FILE_PATH, header=0, names=col_names); print(f\"Raw data loaded. Shape: {df_raw.shape}\")\n",
    "        df_raw['timestamp'] = pd.to_datetime(df_raw['date']); df_raw = df_raw.drop(['unix', 'date', 'symbol_csv'], axis=1)\n",
    "        df_raw = df_raw.sort_values('timestamp').reset_index(drop=True); print(f\"Initial data prep done. Shape: {df_raw.shape}\")\n",
    "        if df_raw.empty: exit(\"DataFrame empty after loading.\")\n",
    "    except Exception as e: print(f\"Error loading/processing CSV: {e}\"); traceback.print_exc(); exit()\n",
    "\n",
    "    print(\"\\n--- 2. Feature Engineering ---\")\n",
    "    feature_calc_start = time.time(); df_features = calculate_selected_features(df_raw, symbol=SYMBOL_NAME); feature_calc_end = time.time()\n",
    "    if df_features.empty: exit(\"Feature calculation failed.\")\n",
    "    print(f\"Feature calculation completed in {feature_calc_end - feature_calc_start:.2f} seconds.\")\n",
    "    CURRENT_FEATURE_COLS = [f for f in MODEL_FEATURE_COLS if f in df_features.columns]\n",
    "    if not CURRENT_FEATURE_COLS: exit(\"ERROR: No modeling features found after calculation.\")\n",
    "    if len(CURRENT_FEATURE_COLS) < len(MODEL_FEATURE_COLS): print(f\"Warning: Only {len(CURRENT_FEATURE_COLS)}/{len(MODEL_FEATURE_COLS)} modeling features generated.\")\n",
    "    print(f\"Using {len(CURRENT_FEATURE_COLS)} features for modeling.\")\n",
    "\n",
    "    print(\"\\n--- 3. Data Cleaning ---\")\n",
    "    numeric_feature_cols = df_features[CURRENT_FEATURE_COLS].select_dtypes(include=np.number).columns.tolist()\n",
    "    df_features[numeric_feature_cols] = df_features[numeric_feature_cols].replace([np.inf, -np.inf], np.nan)\n",
    "    nan_check = df_features[numeric_feature_cols].isnull().sum(); total_nans = nan_check.sum()\n",
    "    print(f\"Total NaNs in {len(numeric_feature_cols)} numeric features: {total_nans}.\")\n",
    "\n",
    "    print(\"\\n--- 4. Target & Final Prep ---\")\n",
    "    TARGET_COLUMN = 'target'; df = df_features.copy(); df = df.sort_values('timestamp')\n",
    "    if 'close' not in df.columns: exit(\"ERROR: 'close' column missing.\")\n",
    "    print(f\"Creating target: {PREDICTION_WINDOW_HOURS}h return >= {TARGET_THRESHOLD_PCT}%...\")\n",
    "    df['future_price'] = df['close'].shift(-PREDICTION_WINDOW_ROWS)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'): df['price_return_future'] = (df['future_price'] - df['close']) / df['close'].replace(0, np.nan) * 100\n",
    "    df[TARGET_COLUMN] = np.where(df['price_return_future'] >= TARGET_THRESHOLD_PCT, 1, 0)\n",
    "    df.loc[df['price_return_future'].isnull(), TARGET_COLUMN] = np.nan; df = df.drop(['future_price', 'price_return_future'], axis=1)\n",
    "    initial_rows = len(df); essential_check_cols = ['close', TARGET_COLUMN]; df = df.dropna(subset=essential_check_cols)\n",
    "    print(f\"Rows after NaN target/close drop: {len(df)} (Removed {initial_rows - len(df)})\")\n",
    "    rows_lost_features = len(df) - len(df.dropna(subset=CURRENT_FEATURE_COLS))\n",
    "    if rows_lost_features > 0: print(f\"Note: {rows_lost_features} rows have NaNs in features. Models/Imputer handle.\")\n",
    "    if df.empty: exit(\"DataFrame empty after target/NaN drop.\")\n",
    "    target_counts = df[TARGET_COLUMN].value_counts(normalize=True) * 100\n",
    "    print(\"\\nTarget distribution:\"); print(f\"  0 (< {TARGET_THRESHOLD_PCT}%): {target_counts.get(0, 0):.2f}%\"); print(f\"  1 (>= {TARGET_THRESHOLD_PCT}%): {target_counts.get(1, 0):.2f}%\")\n",
    "    df = df.sort_values('timestamp').reset_index(drop=True); print(f\"Final DataFrame shape: {df.shape}\")\n",
    "\n",
    "    # --- 5. Walk-Forward Validation ---\n",
    "    print(\"\\n--- 5. Starting Walk-Forward Validation (Stacking - SVM Meta) ---\")\n",
    "    all_metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}; all_best_thresholds = []\n",
    "    # Meta-feature importance is not straightforward for RBF SVM, omitting detailed tracking\n",
    "    iteration_count = 0; n_rows_total = len(df); current_train_start_idx = 0\n",
    "    total_iterations_estimate = max(0, (n_rows_total - TRAIN_WINDOW_ROWS - TEST_WINDOW_ROWS) // STEP_ROWS + 1) if STEP_ROWS > 0 else 0\n",
    "\n",
    "    print(f\"Total rows: {n_rows_total}, Train: {TRAIN_WINDOW_HOURS}h ({TRAIN_WINDOW_ROWS} rows), Eval: {TEST_WINDOW_HOURS}h ({TEST_WINDOW_ROWS} rows), Step: {STEP_HOURS}h ({STEP_ROWS} rows)\")\n",
    "    print(f\"Estimated iterations: {total_iterations_estimate}\"); print(f\"Using {len(CURRENT_FEATURE_COLS)} features.\")\n",
    "    print(f\"Stacking Folds (K): {N_STACKING_FOLDS}\"); print(f\"Meta Learner: SVM (RBF), Tuning over: {META_SVM_PARAM_GRID}\")\n",
    "    print(f\"Threshold Search Range: {THRESHOLD_SEARCH_RANGE}\"); print(\"-\" * 30)\n",
    "    start_loop_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        train_end_idx = current_train_start_idx + TRAIN_WINDOW_ROWS; test_start_idx = train_end_idx; test_end_idx = test_start_idx + TEST_WINDOW_ROWS\n",
    "        if test_end_idx > n_rows_total: print(f\"\\nStopping: Eval window end ({test_end_idx}) > total rows ({n_rows_total}).\"); break\n",
    "        if current_train_start_idx >= n_rows_total: print(f\"\\nStopping: Train start index ({current_train_start_idx}) reached end.\"); break\n",
    "\n",
    "        train_df = df.iloc[current_train_start_idx : train_end_idx].copy(); test_df = df.iloc[test_start_idx : test_end_idx].copy()\n",
    "        min_train_samples = max(50, int(0.1 * TRAIN_WINDOW_ROWS), N_STACKING_FOLDS * 5); min_test_samples = 10\n",
    "        if len(train_df) < min_train_samples or len(test_df) < min_test_samples:\n",
    "            print(f\"Skipping iter {iteration_count + 1}: Insufficient data train ({len(train_df)}/{min_train_samples}) or test ({len(test_df)}/{min_test_samples}).\"); current_train_start_idx += STEP_ROWS; continue\n",
    "        X_train_full = train_df[CURRENT_FEATURE_COLS]; y_train_full = train_df[TARGET_COLUMN]\n",
    "        X_test = test_df[CURRENT_FEATURE_COLS]; y_test = test_df[TARGET_COLUMN]\n",
    "        if len(y_train_full.unique()) < 2: print(f\"Skipping iter {iteration_count + 1}: Train data single class.\"); current_train_start_idx += STEP_ROWS; continue\n",
    "        if len(y_test.unique()) < 2: print(f\"Warning iter {iteration_count + 1}: Eval test data single class.\")\n",
    "        neg_count = y_train_full.value_counts().get(0, 0); pos_count = y_train_full.value_counts().get(1, 0)\n",
    "        scale_pos_weight_val = neg_count / pos_count if pos_count > 0 else 1.0\n",
    "\n",
    "        iter_start_time = time.time(); print(f\"\\n--- Iter {iteration_count + 1}/{total_iterations_estimate} ---\")\n",
    "        print(f\"  Train Indices: [{current_train_start_idx}:{train_end_idx-1}], Eval Indices: [{test_start_idx}:{test_end_idx-1}]\")\n",
    "        print(f\"  Train Target Dist: {dict(y_train_full.value_counts(normalize=True))}\"); print(f\"  Test Target Dist: {dict(y_test.value_counts(normalize=True))}\")\n",
    "        print(f\"  Using scale_pos_weight for XGB/LGBM: {scale_pos_weight_val:.4f}\")\n",
    "\n",
    "        print(\"  Grid searching base models...\") # Base model search\n",
    "        best_xgb_params, xgb_score = grid_search_base_model('xgb', BASE_XGB_PARAM_GRID, X_train_full, y_train_full, scale_pos_weight_val)\n",
    "        best_lgbm_params, lgbm_score = grid_search_base_model('lgbm', BASE_LGBM_PARAM_GRID, X_train_full, y_train_full, scale_pos_weight_val)\n",
    "        best_svm_params, svm_score = grid_search_base_model('svm', BASE_SVM_PARAM_GRID, X_train_full, y_train_full, scale_pos_weight_val)\n",
    "        print(f\"    Best XGB Params: {best_xgb_params} (CV F1: {xgb_score:.3f})\"); print(f\"    Best LGBM Params: {best_lgbm_params} (CV F1: {lgbm_score:.3f})\"); print(f\"    Best SVM Params: {best_svm_params} (CV F1: {svm_score:.3f})\")\n",
    "\n",
    "        xgb_iter_params = XGB_BASE_PARAMS.copy(); xgb_iter_params.update(best_xgb_params or {}); model_xgb_base = XGBClassifier(**xgb_iter_params, scale_pos_weight=scale_pos_weight_val)\n",
    "        lgbm_iter_params = LGBM_BASE_PARAMS.copy(); lgbm_iter_params.update(best_lgbm_params or {}); model_lgbm_base = LGBMClassifier(**lgbm_iter_params, scale_pos_weight=scale_pos_weight_val)\n",
    "        svm_iter_grid_params = best_svm_params or {}; pipeline_svm_base = Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler()), ('svm', SVC(**SVM_BASE_PARAMS, **svm_iter_grid_params))])\n",
    "        models_oof = {'xgb': model_xgb_base, 'lgbm': model_lgbm_base, 'svm': pipeline_svm_base}\n",
    "        oof_arrays = {'xgb': np.full(len(train_df), np.nan), 'lgbm': np.full(len(train_df), np.nan), 'svm': np.full(len(train_df), np.nan)}\n",
    "\n",
    "        print(f\"  Level 0: Generating OOF predictions ({N_STACKING_FOLDS}-Fold CV)...\") # OOF Generation\n",
    "        skf = StratifiedKFold(n_splits=N_STACKING_FOLDS, shuffle=True, random_state=42 + iteration_count)\n",
    "        for fold, (train_idx_k, val_idx_k) in enumerate(skf.split(X_train_full, y_train_full)):\n",
    "            X_train_k, y_train_k = X_train_full.iloc[train_idx_k], y_train_full.iloc[train_idx_k]; X_val_k, y_val_k = X_train_full.iloc[val_idx_k], y_train_full.iloc[val_idx_k]\n",
    "            if len(np.unique(y_train_k)) < 2 or len(np.unique(y_val_k)) < 2: print(f\"    Warning: Fold {fold+1} single class.\"); prior = y_train_full.mean(); [oof_arrays[key].__setitem__(val_idx_k, prior) for key in oof_arrays]; continue\n",
    "            for name, model in models_oof.items():\n",
    "                try:\n",
    "                    fit_params_k = {}\n",
    "                    if name == 'lgbm': fit_params_k = {'callbacks': [early_stopping(10, verbose=False), log_evaluation(0)], 'eval_metric': 'logloss', 'eval_set': [(X_val_k, y_val_k)]}\n",
    "                    elif name == 'xgb': fit_params_k = {'eval_set': [(X_val_k, y_val_k)], 'early_stopping_rounds': 10, 'verbose': False}\n",
    "                    model.fit(X_train_k, y_train_k, **fit_params_k); oof_arrays[name][val_idx_k] = model.predict_proba(X_val_k)[:, 1]\n",
    "                except Exception as e_kfold: print(f\"    Error K-Fold {fold+1} for {name}: {e_kfold}\"); prior = y_train_full.mean(); oof_arrays[name][val_idx_k] = prior\n",
    "        X_meta_train_dict = {}; models_failed_oof = []\n",
    "        for name in models_oof:\n",
    "            oof_array = oof_arrays[name]\n",
    "            if np.isnan(oof_array).all(): models_failed_oof.append(name)\n",
    "            mean_oof = np.nanmean(oof_array); mean_oof = 0.5 if pd.isna(mean_oof) else mean_oof\n",
    "            if np.isnan(oof_array).any(): print(f\"    Imputed NaNs in OOF for {name}\"); oof_array = np.nan_to_num(oof_array, nan=mean_oof)\n",
    "            X_meta_train_dict[f'{name}_pred'] = oof_array\n",
    "        if models_failed_oof: print(f\"  ERROR: Base models {models_failed_oof} failed OOF. Skipping.\"); current_train_start_idx += STEP_ROWS; continue\n",
    "        X_meta_train = pd.DataFrame(X_meta_train_dict, index=X_train_full.index); y_meta_train = y_train_full\n",
    "        print(f\"  Level 0 OOF Done. Meta Train Shape: {X_meta_train.shape}\")\n",
    "\n",
    "        print(f\"  Level 0: Training base models on full train data ({len(train_df)} rows)...\") # Full Base Model Training\n",
    "        models_full = {}; all_base_trained = True\n",
    "        for name, model in models_oof.items():\n",
    "            try: params = {}; model.fit(X_train_full, y_train_full, **params); models_full[name] = model\n",
    "            except Exception as e: print(f\"  ERROR training base '{name}': {e}\"); all_base_trained = False; break\n",
    "        if not all_base_trained: print(\"  Skipping iter: base model train fail.\"); current_train_start_idx += STEP_ROWS; continue\n",
    "        print(\"  Level 0 Full Training Done.\")\n",
    "\n",
    "        # --- Level 1: Meta Learner (SVM) Tuning & Threshold Tuning ---\n",
    "        print(\"  Level 1: Tuning Meta-Learner (SVM) and Probability Threshold...\")\n",
    "        best_meta_params = None # Will hold best {'C': c, 'gamma': g}\n",
    "        best_meta_score = -np.inf\n",
    "        best_meta_model_for_thresh = None\n",
    "        best_threshold_iter = 0.5; best_thresh_f1_score = -np.inf\n",
    "\n",
    "        # --- Scale Meta Features ---\n",
    "        meta_scaler = StandardScaler()\n",
    "        X_meta_train_scaled = meta_scaler.fit_transform(X_meta_train)\n",
    "        X_meta_train_scaled = pd.DataFrame(X_meta_train_scaled, index=X_meta_train.index, columns=X_meta_train.columns)\n",
    "\n",
    "        meta_val_size = int(len(X_meta_train_scaled) * META_VALIDATION_PCT)\n",
    "        if meta_val_size < 10 or (len(X_meta_train_scaled) - meta_val_size) < 10:\n",
    "            print(f\"  Warning: Meta dataset too small. Using default SVM params.\")\n",
    "            best_meta_params = list(ParameterGrid(META_SVM_PARAM_GRID))[0] if META_SVM_PARAM_GRID else {'C': 1.0, 'gamma': 'scale'} # Fallback default\n",
    "        else:\n",
    "            X_meta_train_sub = X_meta_train_scaled[:-meta_val_size]; y_meta_train_sub = y_meta_train[:-meta_val_size]\n",
    "            X_meta_val = X_meta_train_scaled[-meta_val_size:]; y_meta_val = y_meta_train[-meta_val_size:]\n",
    "            if len(y_meta_val.unique()) < 2 or len(y_meta_train_sub.unique()) < 2:\n",
    "                print(\"  Warning: Meta train/val split single class. Using default SVM params.\")\n",
    "                best_meta_params = list(ParameterGrid(META_SVM_PARAM_GRID))[0] if META_SVM_PARAM_GRID else {'C': 1.0, 'gamma': 'scale'}\n",
    "            else:\n",
    "                # Meta Grid Search (Tuning C and gamma for SVM)\n",
    "                print(f\"    Tuning meta learner over {len(list(ParameterGrid(META_SVM_PARAM_GRID)))} SVM param combinations...\")\n",
    "                for params_meta_cv in ParameterGrid(META_SVM_PARAM_GRID):\n",
    "                    try:\n",
    "                        current_meta_params = {**META_SVM_FIXED_PARAMS, **params_meta_cv}\n",
    "                        model_meta_cv = SVC(**current_meta_params)\n",
    "                        model_meta_cv.fit(X_meta_train_sub, y_meta_train_sub) # Fit on scaled sub-train\n",
    "                        y_pred_meta_val_cv = model_meta_cv.predict(X_meta_val) # Predict on scaled val\n",
    "                        meta_score = f1_score(y_meta_val, y_pred_meta_val_cv, average='binary', pos_label=1, zero_division=0)\n",
    "                        if meta_score >= best_meta_score:\n",
    "                            best_meta_score = meta_score; best_meta_params = params_meta_cv; best_meta_model_for_thresh = model_meta_cv\n",
    "                    except Exception as e_meta_cv:\n",
    "                        print(f\"    Error during Meta SVM CV with params {params_meta_cv}: {e_meta_cv}\")\n",
    "                        if best_meta_params is None: best_meta_params = list(ParameterGrid(META_SVM_PARAM_GRID))[0] if META_SVM_PARAM_GRID else {'C': 1.0, 'gamma': 'scale'}\n",
    "\n",
    "                if best_meta_params is None: best_meta_params = list(ParameterGrid(META_SVM_PARAM_GRID))[0] if META_SVM_PARAM_GRID else {'C': 1.0, 'gamma': 'scale'}\n",
    "                print(f\"    Best Meta SVM Params Found: {best_meta_params} (Validation F1: {best_meta_score:.4f})\")\n",
    "\n",
    "                # Threshold Tuning using the best SVM model found\n",
    "                if best_meta_model_for_thresh is not None:\n",
    "                    print(f\"    Tuning threshold over range {THRESHOLD_SEARCH_RANGE}...\")\n",
    "                    try:\n",
    "                        y_meta_proba_val = best_meta_model_for_thresh.predict_proba(X_meta_val)[:, 1] # Predict proba on scaled val\n",
    "                        f1_scores_thresh = {}\n",
    "                        for t in THRESHOLD_SEARCH_RANGE:\n",
    "                            y_pred_meta_val_t = (y_meta_proba_val >= t).astype(int)\n",
    "                            current_f1 = f1_score(y_meta_val, y_pred_meta_val_t, average='binary', pos_label=1, zero_division=0)\n",
    "                            f1_scores_thresh[t] = current_f1\n",
    "                            if current_f1 >= best_thresh_f1_score: best_thresh_f1_score = current_f1; best_threshold_iter = t\n",
    "                        print(f\"    Best Threshold Found: {best_threshold_iter:.2f} (Validation F1: {best_thresh_f1_score:.4f})\")\n",
    "                    except Exception as e_thresh: print(f\"    Error during threshold tuning: {e_thresh}. Using default {best_threshold_iter:.2f}.\")\n",
    "                else: print(f\"    Skipping threshold tuning. Using default {best_threshold_iter:.2f}.\")\n",
    "\n",
    "        # --- Level 1: Train Final Meta Learner (SVM) ---\n",
    "        print(\"  Level 1: Training final Meta-Learner (SVM)...\")\n",
    "        try:\n",
    "             final_meta_params = {**META_SVM_FIXED_PARAMS, **(best_meta_params or {'C': 1.0, 'gamma': 'scale'})} # Use best or default\n",
    "             meta_model_final = SVC(**final_meta_params)\n",
    "             meta_model_final.fit(X_meta_train_scaled, y_meta_train) # Fit on ENTIRE SCALED OOF data\n",
    "             print(\"  Level 1 Final Meta Training Done.\")\n",
    "        except Exception as e_meta_final: print(f\"  ERROR: Failed to train final meta-learner: {e_meta_final}\"); current_train_start_idx += STEP_ROWS; continue\n",
    "\n",
    "        # --- Prediction Phase ---\n",
    "        print(\"  Prediction: Generating final predictions...\")\n",
    "        try:\n",
    "            pred_xgb_test = models_full['xgb'].predict_proba(X_test)[:, 1]\n",
    "            pred_lgbm_test = models_full['lgbm'].predict_proba(X_test)[:, 1]\n",
    "            pred_svm_test = models_full['svm'].predict_proba(X_test)[:, 1]\n",
    "            X_meta_test = pd.DataFrame({'xgb_pred': pred_xgb_test, 'lgbm_pred': pred_lgbm_test, 'svm_pred': pred_svm_test}, index=X_test.index)\n",
    "            X_meta_test_scaled = meta_scaler.transform(X_meta_test) # Scale test meta features\n",
    "            y_proba_test = meta_model_final.predict_proba(X_meta_test_scaled)[:, 1] # Predict on scaled\n",
    "            y_pred = (y_proba_test >= best_threshold_iter).astype(int)\n",
    "            print(\"  Prediction Done.\")\n",
    "        except Exception as e_pred:\n",
    "             print(f\"  ERROR during prediction: {e_pred}\"); [all_metrics[key].append(np.nan) for key in all_metrics]; all_best_thresholds.append(np.nan); current_train_start_idx += STEP_ROWS; continue\n",
    "\n",
    "        # --- Evaluation ---\n",
    "        if len(np.unique(y_test)) < 2:\n",
    "            accuracy = accuracy_score(y_test, y_pred); precision = precision_score(y_test, y_pred, zero_division=0); recall = recall_score(y_test, y_pred, zero_division=0); f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "            print(f\"  Evaluation (Test Window {TEST_WINDOW_HOURS}h, SINGLE CLASS): Acc={accuracy:.4f}, Prc={precision:.4f}, Rec={recall:.4f}, F1={f1:.4f}\")\n",
    "        else:\n",
    "            accuracy = accuracy_score(y_test, y_pred); precision = precision_score(y_test, y_pred, zero_division=0); recall = recall_score(y_test, y_pred, zero_division=0); f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "            print(f\"  Evaluation (Test Window {TEST_WINDOW_HOURS}h): Acc={accuracy:.4f}, Prc={precision:.4f}, Rec={recall:.4f}, F1={f1:.4f}\")\n",
    "        all_metrics['accuracy'].append(accuracy); all_metrics['precision'].append(precision); all_metrics['recall'].append(recall); all_metrics['f1'].append(f1); all_best_thresholds.append(best_threshold_iter)\n",
    "\n",
    "        # --- Meta-Feature Importance (Skipped for RBF SVM) ---\n",
    "        # SVM with RBF kernel doesn't have easily interpretable feature importances like coefficients or gain.\n",
    "\n",
    "        iteration_count += 1; iter_end_time = time.time()\n",
    "        print(f\"  Iteration {iteration_count} finished in {iter_end_time - iter_start_time:.2f} seconds.\"); print(\"-\" * 20)\n",
    "        current_train_start_idx += STEP_ROWS\n",
    "\n",
    "    # --- End of Walk-Forward Loop ---\n",
    "    end_loop_time = time.time(); loop_duration_minutes = (end_loop_time - start_loop_time) / 60\n",
    "    print(\"-\" * 30); print(f\"Walk-Forward Validation (Ensemble_Method_D) finished in {end_loop_time - start_loop_time:.2f}s ({loop_duration_minutes:.2f} min).\")\n",
    "\n",
    "    # --- 6. Aggregate and Display Results ---\n",
    "    print(\"\\n--- 6. Final Results (Ensemble_Method_D) ---\")\n",
    "    if iteration_count > 0 and len(all_metrics['f1']) > 0:\n",
    "        valid_indices = [i for i, f1 in enumerate(all_metrics['f1']) if not pd.isna(f1)]\n",
    "        if valid_indices:\n",
    "            valid_accuracy = [all_metrics['accuracy'][i] for i in valid_indices]; valid_precision = [all_metrics['precision'][i] for i in valid_indices]\n",
    "            valid_recall = [all_metrics['recall'][i] for i in valid_indices]; valid_f1 = [all_metrics['f1'][i] for i in valid_indices]\n",
    "            valid_thresholds = [all_best_thresholds[i] for i in valid_indices if not pd.isna(all_best_thresholds[i])]\n",
    "            avg_accuracy = np.mean(valid_accuracy); avg_precision = np.mean(valid_precision); avg_recall = np.mean(valid_recall); avg_f1 = np.mean(valid_f1)\n",
    "            print(\"\\n--- Average Walk-Forward Results ---\")\n",
    "            print(f\"Iterations Run: {iteration_count}, Successful Evals: {len(valid_indices)}\")\n",
    "            print(f\"Target: >= {TARGET_THRESHOLD_PCT}% over {PREDICTION_WINDOW_HOURS}h\"); print(f\"Train: {TRAIN_WINDOW_HOURS}h, Eval: {TEST_WINDOW_HOURS}h, Step: {STEP_HOURS}h\")\n",
    "            print(f\"Stacking Folds: {N_STACKING_FOLDS}, Meta-Learner: SVM (RBF)\")\n",
    "            print(f\"Average Accuracy:  {avg_accuracy:.4f}\"); print(f\"Average Precision: {avg_precision:.4f}\"); print(f\"Average Recall:    {avg_recall:.4f}\"); print(f\"Average F1-Score:  {avg_f1:.4f}\")\n",
    "            std_accuracy = np.std(valid_accuracy); std_precision = np.std(valid_precision); std_recall = np.std(valid_recall); std_f1 = np.std(valid_f1)\n",
    "            print(\"\\n--- Standard Deviation of Metrics ---\"); print(f\"Std Dev Acc: {std_accuracy:.4f}\"); print(f\"Std Dev Prc: {std_precision:.4f}\"); print(f\"Std Dev Rec: {std_recall:.4f}\"); print(f\"Std Dev F1: {std_f1:.4f}\")\n",
    "            if valid_thresholds: avg_threshold = np.mean(valid_thresholds); std_threshold = np.std(valid_thresholds); print(f\"\\nAvg Best Threshold: {avg_threshold:.3f} (StdDev: {std_threshold:.3f})\")\n",
    "            else: print(\"\\nCould not determine average threshold.\")\n",
    "            print(\"\\n--- Meta-Feature Importances ---\")\n",
    "            print(\"  (Not directly available for RBF SVM meta-learner)\")\n",
    "        else: print(\"\\nNo valid metrics recorded.\")\n",
    "    else: print(\"\\nNo iterations completed or metrics generated.\")\n",
    "\n",
    "    print(\"\\nScript Ensemble_Method_D finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
