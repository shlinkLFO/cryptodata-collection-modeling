{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Selected the last 15000 rows for validation.\n",
      "Applying feature engineering...\n",
      "Feature engineering complete.\n",
      "Columns after features: 53\n",
      "Defining target variable...\n",
      "Preparing data for modeling...\n",
      "Handled NaNs. Dropped 185 rows.\n",
      "Replacing 10 infinite values with NaN...\n",
      "Dropped 10 rows after handling infinites.\n",
      "Defining Features (X) and Binary Target (y)...\n",
      "Final feature matrix shape: (14805, 49)\n",
      "Target vector shape: (14805,)\n",
      "Number of feature columns: 49\n",
      "\n",
      "--- Starting Rolling Backtest with Regularization ---\n",
      "Processed 50 predictions. Current Cumulative Accuracy: 0.6200\n",
      "Processed 100 predictions. Current Cumulative Accuracy: 0.6300\n",
      "Processed 150 predictions. Current Cumulative Accuracy: 0.6533\n",
      "Processed 200 predictions. Current Cumulative Accuracy: 0.7000\n",
      "Processed 250 predictions. Current Cumulative Accuracy: 0.6840\n",
      "Processed 300 predictions. Current Cumulative Accuracy: 0.6700\n",
      "Processed 350 predictions. Current Cumulative Accuracy: 0.6543\n",
      "Processed 400 predictions. Current Cumulative Accuracy: 0.6500\n",
      "Processed 450 predictions. Current Cumulative Accuracy: 0.6511\n",
      "Processed 500 predictions. Current Cumulative Accuracy: 0.6420\n",
      "Processed 550 predictions. Current Cumulative Accuracy: 0.6473\n",
      "\n",
      "Backtesting finished. Made 576 predictions.\n",
      "\n",
      "--- Backtesting Performance Metrics ---\n",
      "Overall Accuracy:  0.6372\n",
      "Overall Precision: 0.6229\n",
      "Overall Recall:    0.7390\n",
      "Overall F1 Score:  0.6760\n",
      "\n",
      "Plotting cumulative accuracy...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 292\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;66;03m# --- 7. Plot Cumulative Accuracy Over Time ---\u001b[39;00m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPlotting cumulative accuracy...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 292\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m    293\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(backtest_timestamps, cumulative_accuracy_list, marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m, markersize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m    294\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRolling Backtest Cumulative Accuracy Over Time (Last \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rows)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "import xgboost as xgb\n",
    "\n",
    "# Feature Engineering Imports\n",
    "import pandas_ta as ta  # Technical indicators\n",
    "\n",
    "# Modeling Imports\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier, early_stopping, log_evaluation # <--- IMPORT CALLBACKS\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler # Needed for SVM\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import ParameterGrid, StratifiedKFold # StratifiedKFold for stacking\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "from sklearn.pipeline import Pipeline # Optional: useful for SVM with scaling\n",
    "from sklearn.impute import SimpleImputer # Better imputation strategy for pipeline\n",
    "\n",
    "# --- Suppress Warnings ---\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)\n",
    "warnings.filterwarnings('ignore') # General suppression\n",
    "\n",
    "# --- 1. Load Data and Select Last 15,000 Rows ---\n",
    "print(\"Loading data...\")\n",
    "try:\n",
    "    df_full = pd.read_csv('BTCUSDrec.csv')\n",
    "    df_full = df_full.sort_values(by='unix', ascending=True).reset_index(drop=True)\n",
    "\n",
    "    if len(df_full) < 10000:\n",
    "        print(f\"Warning: Full dataset has only {len(df_full)} rows. Using all available data.\")\n",
    "        df = df_full.copy()\n",
    "    else:\n",
    "        # Select the last 15,000 rows\n",
    "        df = df_full.iloc[-15000:].reset_index(drop=True)\n",
    "        print(f\"Selected the last {len(df)} rows for validation.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: BTCUSDrec.csv not found. Please make sure the file is in the correct directory.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred loading the data: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Re-apply ALL Feature Engineering Steps ---\n",
    "print(\"Applying feature engineering...\")\n",
    "\n",
    "# Basic Features\n",
    "df['price_change_1h'] = df['close'].pct_change() * 100 # Note: Target will be recalculated later\n",
    "df['price_range_pct'] = (df['high'] - df['low']) / df['close'] * 100\n",
    "df['oc_change_pct'] = (df['close'] - df['open']) / df['open'] * 100\n",
    "\n",
    "# Volatility Features\n",
    "def garman_klass_volatility(open_, high, low, close, window):\n",
    "    log_hl = np.log(high / low)\n",
    "    log_co = np.log(close / open_)\n",
    "    gk = 0.5 * (log_hl ** 2) - (2*np.log(2) - 1) * (log_co ** 2)\n",
    "    rolling_mean = gk.rolling(window=window).mean()\n",
    "    # Handle potential negative values in rolling_mean before sqrt\n",
    "    rolling_mean = rolling_mean.clip(lower=0)\n",
    "    return np.sqrt(rolling_mean)\n",
    "\n",
    "def parkinson_volatility(high, low, window):\n",
    "    log_hl_sq = np.log(high / low) ** 2\n",
    "    rolling_sum = log_hl_sq.rolling(window=window).sum()\n",
    "    factor = 1 / (4 * np.log(2) * window)\n",
    "    return np.sqrt(factor * rolling_sum)\n",
    "\n",
    "df['garman_klass_12h'] = garman_klass_volatility(df['open'], df['high'], df['low'], df['close'], window=12)\n",
    "df['parkinson_3h'] = parkinson_volatility(df['high'], df['low'], window=3)\n",
    "\n",
    "# Moving Averages & Standard Deviations\n",
    "df['ma_3h'] = df['close'].rolling(window=3).mean()\n",
    "df['rolling_std_3h'] = df['close'].rolling(window=3).std()\n",
    "\n",
    "# Lagged Features (using the 1h price change calculated above)\n",
    "df['price_return_1h_feat'] = df['close'].pct_change() # Use for lags, distinct from target col\n",
    "lag_periods_price = [3, 6, 12, 24, 48, 72, 168]\n",
    "for lag in lag_periods_price:\n",
    "    df[f'lag_{lag}h_price_return'] = df['price_return_1h_feat'].shift(lag - 1) * 100 # Scale to %\n",
    "\n",
    "df['volume_return_1h'] = df['Volume BTC'].pct_change() * 100\n",
    "lag_periods_volume = [3, 6, 12, 24]\n",
    "for lag in lag_periods_volume:\n",
    "     df[f'lag_{lag}h_volume_return'] = df['volume_return_1h'].shift(lag - 1)\n",
    "\n",
    "# Longer MAs and STDs\n",
    "ma_periods = [6, 12, 24, 48, 72, 168]\n",
    "for p in ma_periods:\n",
    "    df[f'ma_{p}h'] = df['close'].rolling(window=p).mean()\n",
    "\n",
    "std_periods = [6, 12, 24, 48, 72, 168]\n",
    "for p in std_periods:\n",
    "    # Std Dev on returns is more common than on price directly\n",
    "    df[f'rolling_std_{p}h'] = df['price_return_1h_feat'].rolling(window=p).std() * 100 # Scale to %\n",
    "\n",
    "# ATR\n",
    "df['prev_close'] = df['close'].shift(1)\n",
    "df['high_minus_low'] = df['high'] - df['low']\n",
    "df['high_minus_prev_close'] = np.abs(df['high'] - df['prev_close'])\n",
    "df['low_minus_prev_close'] = np.abs(df['low'] - df['prev_close'])\n",
    "df['true_range'] = df[['high_minus_low', 'high_minus_prev_close', 'low_minus_prev_close']].max(axis=1)\n",
    "atr_periods = [14, 24, 48]\n",
    "for p in atr_periods:\n",
    "     df[f'atr_{p}h'] = df['true_range'].rolling(window=p).mean()\n",
    "df = df.drop(columns=['prev_close', 'high_minus_low', 'high_minus_prev_close', 'low_minus_prev_close', 'true_range'])\n",
    "\n",
    "# Trend/Interaction Features\n",
    "epsilon = 1e-9 # For safe division\n",
    "for p in [24, 48, 168]:\n",
    "    if f'ma_{p}h' in df.columns:\n",
    "        df[f'close_div_ma_{p}h'] = df['close'] / (df[f'ma_{p}h'] + epsilon)\n",
    "\n",
    "if 'ma_12h' in df.columns and 'ma_48h' in df.columns:\n",
    "     df[f'ma12_div_ma48'] = df['ma_12h'] / (df['ma_48h'] + epsilon)\n",
    "if 'ma_24h' in df.columns and 'ma_168h' in df.columns:\n",
    "     df[f'ma24_div_ma168'] = df['ma_24h'] / (df['ma_168h'] + epsilon)\n",
    "if 'rolling_std_12h' in df.columns and 'rolling_std_72h' in df.columns:\n",
    "     df['std12_div_std72'] = df['rolling_std_12h'] / (df['rolling_std_72h'] + epsilon)\n",
    "if 'price_range_pct' in df.columns:\n",
    "    df['volume_btc_x_range'] = df['Volume BTC'] * df['price_range_pct']\n",
    "\n",
    "# Non-linear Transformations\n",
    "if 'rolling_std_3h' in df.columns:\n",
    "    df['rolling_std_3h_sq'] = df['rolling_std_3h'] ** 2\n",
    "if 'price_return_1h_feat' in df.columns:\n",
    "    df['price_return_1h_sq'] = df['price_return_1h_feat'] ** 2 * 10000 # Scale %^2\n",
    "if 'rolling_std_12h' in df.columns:\n",
    "     df['rolling_std_12h_sqrt'] = np.sqrt(df['rolling_std_12h'].clip(lower=0) + epsilon) # Clip before sqrt\n",
    "\n",
    "# Drop intermediate price return used for features\n",
    "if 'price_return_1h_feat' in df.columns:\n",
    "    df = df.drop(columns=['price_return_1h_feat'])\n",
    "\n",
    "print(\"Feature engineering complete.\")\n",
    "print(f\"Columns after features: {df.shape[1]}\")\n",
    "\n",
    "\n",
    "# --- 3. Define Target Variable ---\n",
    "print(\"Defining target variable...\")\n",
    "df['price_change_12h'] = df['close'].shift(-12).sub(df['close']).div(df['close']).mul(100)\n",
    "target_col = 'price_change_12h'\n",
    "\n",
    "\n",
    "# --- 4. Prepare Data for Modeling (Handle NaNs, Select Features) ---\n",
    "print(\"Preparing data for modeling...\")\n",
    "\n",
    "# Drop non-feature columns (Keep 'date'/'unix' for now for backtest indexing)\n",
    "string_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "cols_to_drop = [col for col in string_cols if col != 'date'] # Keep date if exists\n",
    "# Also drop the original 1h price change column if it wasn't the target\n",
    "if 'price_change_1h' in df.columns and target_col != 'price_change_1h':\n",
    "    cols_to_drop.append('price_change_1h')\n",
    "\n",
    "df_model_ready = df.drop(columns=cols_to_drop)\n",
    "\n",
    "# Handle NaNs introduced by feature engineering and target shifting\n",
    "initial_rows = len(df_model_ready)\n",
    "df_model_ready = df_model_ready.dropna() # Drop rows with ANY NaN\n",
    "final_rows = len(df_model_ready)\n",
    "print(f\"Handled NaNs. Dropped {initial_rows - final_rows} rows.\")\n",
    "\n",
    "# Handle potential Infinite values just in case\n",
    "numeric_cols = df_model_ready.select_dtypes(include=np.number).columns.tolist()\n",
    "# Remove target col from numeric cols list before replacing inf\n",
    "numeric_cols_no_target = [col for col in numeric_cols if col != target_col and col != 'unix'] # Keep unix out too\n",
    "inf_count = np.isinf(df_model_ready[numeric_cols_no_target]).sum().sum()\n",
    "if inf_count > 0:\n",
    "    print(f\"Replacing {inf_count} infinite values with NaN...\")\n",
    "    df_model_ready[numeric_cols_no_target] = df_model_ready[numeric_cols_no_target].replace([np.inf, -np.inf], np.nan)\n",
    "    rows_before_inf_drop = len(df_model_ready)\n",
    "    df_model_ready = df_model_ready.dropna()\n",
    "    print(f\"Dropped {rows_before_inf_drop - len(df_model_ready)} rows after handling infinites.\")\n",
    "\n",
    "# Define Features (X) and Binary Target (y)\n",
    "print(\"Defining Features (X) and Binary Target (y)...\")\n",
    "feature_cols = [col for col in df_model_ready.columns if col not in [target_col, 'date', 'unix', 'symbol']] # Exclude identifiers and target\n",
    "X = df_model_ready[feature_cols]\n",
    "y_binary = (df_model_ready[target_col] > 0).astype(int) # Binary target: 1 if price increased, 0 otherwise\n",
    "\n",
    "# Get timestamps for results\n",
    "timestamps = df_model_ready['date'] if 'date' in df_model_ready else pd.to_datetime(df_model_ready['unix'], unit='ms')\n",
    "\n",
    "print(f\"Final feature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y_binary.shape}\")\n",
    "print(f\"Number of feature columns: {len(feature_cols)}\")\n",
    "\n",
    "\n",
    "# --- 5. Rolling Backtesting with Regularization ---\n",
    "print(\"\\n--- Starting Rolling Backtest with Regularization ---\")\n",
    "\n",
    "# Backtesting Parameters\n",
    "initial_train_size = 1000  # Use first 1000 points for initial training\n",
    "step_size = 24           # Retrain and predict every 24 hours\n",
    "reg_alpha = 0.1          # L1 Regularization\n",
    "reg_lambda = 0.1         # L2 Regularization\n",
    "n_estimators = 100       # Number of trees\n",
    "max_depth = 4            # Max depth of trees\n",
    "\n",
    "if len(X) < initial_train_size + step_size:\n",
    "     print(\"Error: Not enough data points after NaN handling for the specified initial_train_size.\")\n",
    "     exit()\n",
    "\n",
    "# Store results\n",
    "all_predictions = []\n",
    "all_actual = []\n",
    "all_probabilities = []\n",
    "backtest_timestamps = []\n",
    "cumulative_accuracy_list = []\n",
    "\n",
    "# Loop through the data\n",
    "num_predictions = 0\n",
    "num_correct_predictions = 0\n",
    "\n",
    "for i in range(initial_train_size, len(X), step_size):\n",
    "    # Define train and test indices for this iteration\n",
    "    train_idx_end = i\n",
    "    test_idx = i # Predict the point at index i\n",
    "\n",
    "    # Ensure test index does not go out of bounds\n",
    "    if test_idx >= len(X):\n",
    "        break\n",
    "\n",
    "    # Get train and test sets for this fold\n",
    "    X_train_roll = X.iloc[:train_idx_end]\n",
    "    y_train_roll = y_binary.iloc[:train_idx_end]\n",
    "    X_test_roll = X.iloc[test_idx:test_idx+1] # Test on the single next point\n",
    "    y_test_roll_actual = y_binary.iloc[test_idx]\n",
    "    current_timestamp = timestamps.iloc[test_idx]\n",
    "\n",
    "    # Initialize and Train XGBoost Model with Regularization\n",
    "    model_roll = xgb.XGBClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        learning_rate=0.1,\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='logloss', # Common metric for binary classification\n",
    "        use_label_encoder=False, # Recommended practice\n",
    "        random_state=42,\n",
    "        # --- Regularization Parameters ---\n",
    "        reg_alpha=reg_alpha,   # L1 regularization term on weights\n",
    "        reg_lambda=reg_lambda  # L2 regularization term on weights\n",
    "    )\n",
    "\n",
    "    model_roll.fit(X_train_roll, y_train_roll, verbose=False) # verbose=False to keep output clean\n",
    "\n",
    "    # Make prediction\n",
    "    pred_roll = model_roll.predict(X_test_roll)[0]\n",
    "    prob_roll = model_roll.predict_proba(X_test_roll)[0, 1] # Probability of class 1 (increase)\n",
    "\n",
    "    # Store results\n",
    "    all_predictions.append(pred_roll)\n",
    "    all_actual.append(y_test_roll_actual)\n",
    "    all_probabilities.append(prob_roll)\n",
    "    backtest_timestamps.append(current_timestamp)\n",
    "\n",
    "    # Update cumulative accuracy\n",
    "    num_predictions += 1\n",
    "    if pred_roll == y_test_roll_actual:\n",
    "        num_correct_predictions += 1\n",
    "    cumulative_accuracy = num_correct_predictions / num_predictions\n",
    "    cumulative_accuracy_list.append(cumulative_accuracy)\n",
    "\n",
    "    # Print progress (optional)\n",
    "    if num_predictions % 50 == 0:\n",
    "         print(f\"Processed {num_predictions} predictions. Current Cumulative Accuracy: {cumulative_accuracy:.4f}\")\n",
    "\n",
    "print(f\"\\nBacktesting finished. Made {num_predictions} predictions.\")\n",
    "\n",
    "# --- 6. Evaluate Backtesting Results ---\n",
    "if num_predictions > 0:\n",
    "    # Calculate final metrics\n",
    "    final_accuracy = accuracy_score(all_actual, all_predictions)\n",
    "    final_precision = precision_score(all_actual, all_predictions, zero_division=0)\n",
    "    final_recall = recall_score(all_actual, all_predictions, zero_division=0)\n",
    "    final_f1 = f1_score(all_actual, all_predictions, zero_division=0)\n",
    "\n",
    "    print(\"\\n--- Backtesting Performance Metrics ---\")\n",
    "    print(f\"Overall Accuracy:  {final_accuracy:.4f}\")\n",
    "    print(f\"Overall Precision: {final_precision:.4f}\")\n",
    "    print(f\"Overall Recall:    {final_recall:.4f}\")\n",
    "    print(f\"Overall F1 Score:  {final_f1:.4f}\")\n",
    "\n",
    "    # --- 7. Plot Cumulative Accuracy Over Time ---\n",
    "    print(\"\\nPlotting cumulative accuracy...\")\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(backtest_timestamps, cumulative_accuracy_list, marker='.', linestyle='-', markersize=4)\n",
    "    plt.title(f'Rolling Backtest Cumulative Accuracy Over Time (Last {len(df)} rows)')\n",
    "    plt.xlabel('Timestamp')\n",
    "    plt.ylabel('Cumulative Accuracy')\n",
    "    plt.ylim(0.4, 1.0) # Adjust ylim based on typical accuracy range (e.g., 0.4 to 0.8 or 1.0)\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"No predictions were made, cannot evaluate performance or plot.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['open', 'high', 'low', 'close', 'Volume BTC', 'Volume USD',\n",
       "       'price_range_pct', 'oc_change_pct', 'garman_klass_12h', 'parkinson_3h',\n",
       "       'ma_3h', 'rolling_std_3h', 'lag_3h_price_return', 'lag_6h_price_return',\n",
       "       'lag_12h_price_return', 'lag_24h_price_return', 'lag_48h_price_return',\n",
       "       'lag_72h_price_return', 'lag_168h_price_return', 'volume_return_1h',\n",
       "       'lag_3h_volume_return', 'lag_6h_volume_return', 'lag_12h_volume_return',\n",
       "       'lag_24h_volume_return', 'ma_6h', 'ma_12h', 'ma_24h', 'ma_48h',\n",
       "       'ma_72h', 'ma_168h', 'rolling_std_6h', 'rolling_std_12h',\n",
       "       'rolling_std_24h', 'rolling_std_48h', 'rolling_std_72h',\n",
       "       'rolling_std_168h', 'atr_14h', 'atr_24h', 'atr_48h', 'close_div_ma_24h',\n",
       "       'close_div_ma_48h', 'close_div_ma_168h', 'ma12_div_ma48',\n",
       "       'ma24_div_ma168', 'std12_div_std72', 'volume_btc_x_range',\n",
       "       'rolling_std_3h_sq', 'price_return_1h_sq', 'rolling_std_12h_sqrt'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What Now?\n",
    "\n",
    "This is great progress! It tells you that predicting 12-hour direction is a much more promising path with your data and feature types.\n",
    "\n",
    "Stick with the Simpler Structure (for now): Keep the single model (XGBoost) and the expanding window backtest for now.\n",
    "\n",
    "Optimize This Setup:\n",
    "\n",
    "Apply VIF: Now that you have a working model structure and a seemingly viable target, apply VIF filtering (e.g., threshold 5 or even your strict 1.69) to the features generated in this simpler script. Does reducing collinearity now improve the already decent results?\n",
    "\n",
    "Tune Hyperparameters: Tune the XGBoost parameters (n_estimators, max_depth, learning_rate, reg_alpha, reg_lambda, subsample, colsample_bytree, min_child_weight) using a method like Optuna or RandomizedSearchCV within the rolling backtest loop (similar to how the meta-learner was tuned, but now for the single main model).\n",
    "\n",
    "Experiment with Target Horizon: Is 12 hours optimal for the >0% target? Try 8 hours, 24 hours.\n",
    "\n",
    "Experiment with Training Window: Does the expanding window work best, or would a large sliding window perform better for this target?\n",
    "\n",
    "You've found a much better baseline. Now optimize it systematically!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
