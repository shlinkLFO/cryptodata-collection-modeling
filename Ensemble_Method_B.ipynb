{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Ensemble_Method_B ---\n",
      "--- 1. Data Loading & Initial Prep ---\n",
      "Loading data from: C:\\Users\\mason\\AVP\\BTCUSDrec.csv\n",
      "Raw data loaded. Shape: (15177, 9)\n",
      "Initial data prep done. Shape: (15177, 7)\n",
      "\n",
      "--- 2. Feature Engineering (Simple_Predictor_B Features) ---\n",
      "Starting calculation for 49 target columns (incl. base)...\n",
      "  Calculating features...\n",
      "    Calculating ATR, Garman-Klass, and Parkinson volatility features...\n",
      "  Assembling final dataframe...\n",
      "Selected feature calculation finished. Returning 15177 rows, 51 columns (43 calculated features). Took 0.04s.\n",
      "Feature calculation completed in 0.04 seconds.\n",
      "Using 43 features found in DataFrame for modeling.\n",
      "\n",
      "--- 3. Data Cleaning (Post-Features) ---\n",
      "Total NaNs found in 43 numeric feature columns: 1688.\n",
      "\n",
      "--- 4. Modeling Target & Final Prep ---\n",
      "Creating binary target based on 4-hour future return >= 0.25%...\n",
      "Rows after removing NaN targets/close: 15173 (Removed 4)\n",
      "Note: 183 rows have NaNs in feature columns. Models/Imputer will handle them.\n",
      "\n",
      "Target variable distribution:\n",
      "  0 (< 0.25% return): 65.16%\n",
      "  1 (>= 0.25% return): 34.84%\n",
      "Final DataFrame shape for backtesting: (15173, 52)\n",
      "\n",
      "--- 5. Starting Walk-Forward Validation (Stacking Ensemble - Ensemble_Method_B) ---\n",
      "Total rows: 15173, Train Window: 672h (672 rows), Prediction Horizon: 4h, Evaluation (Test) Window: 168h (168 rows), Step: 48h (48 rows)\n",
      "Estimated iterations: 299\n",
      "Using 43 features for modeling.\n",
      "Stacking Folds (K): 7\n",
      "Meta Learner Grid: {'max_depth': [2, 3], 'n_estimators': [40, 70], 'eta': [0.03, 0.05], 'lambda': [1.0, 1.5], 'subsample': [0.75, 0.95], 'colsample_bytree': [0.8, 1.0]}\n",
      "Threshold Search Range: [0.1  0.15 0.2  0.25 0.3  0.35 0.4  0.45 0.5  0.55 0.6  0.65 0.7  0.75\n",
      " 0.8  0.85]\n",
      "------------------------------\n",
      "\n",
      "--- Iter 1/299 ---\n",
      "  Train Indices: [0:671], Eval Indices: [672:839]\n",
      "  Train Target Dist: {0.0: 0.7678571428571429, 1.0: 0.23214285714285715}\n",
      "  Test Target Dist: {0.0: 0.8928571428571429, 1.0: 0.10714285714285714}\n",
      "  Using scale_pos_weight: 3.3077\n",
      "  Grid searching base models on current training data...\n",
      "    Best XGB Params: {'eta': 0.06, 'lambda': 1.0, 'max_depth': 3, 'n_estimators': 65} (F1: 0.532)\n",
      "    Best LGBM Params: {'learning_rate': 0.08, 'max_depth': 4, 'n_estimators': 55, 'subsample': 0.75} (F1: 0.541)\n",
      "    Best SVM Params: {'C': 3.2, 'gamma': 'scale'} (F1: 0.545)\n",
      "  Level 0 OOF Generation Done. Meta Train Shape: (672, 3)\n",
      "  Level 0: Training base models on full training data (672 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Tuning meta learner over 64 param combinations...\n",
      "    Best Meta Params Found: {'colsample_bytree': 1.0, 'eta': 0.05, 'lambda': 1.0, 'max_depth': 2, 'n_estimators': 70, 'subsample': 0.75} (Validation F1: 0.5974)\n",
      "    Tuning threshold over range [0.1  0.15 0.2  0.25 0.3  0.35 0.4  0.45 0.5  0.55 0.6  0.65 0.7  0.75\n",
      " 0.8  0.85]...\n",
      "    Best Threshold Found: 0.50 (Validation F1: 0.5974)\n",
      "  Level 1: Training final Meta-Learner on full OOF data...\n",
      "  Level 1 Final Meta Training Done.\n",
      "  Prediction: Generating final predictions on evaluation data...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.7917, Prc=0.2821, Rec=0.6111, F1=0.3860\n",
      "  Iteration 1 finished in 3117.12 seconds.\n",
      "--------------------\n",
      "\n",
      "--- Iter 2/299 ---\n",
      "  Train Indices: [48:719], Eval Indices: [720:887]\n",
      "  Train Target Dist: {0.0: 0.7857142857142857, 1.0: 0.21428571428571427}\n",
      "  Test Target Dist: {0.0: 0.9047619047619048, 1.0: 0.09523809523809523}\n",
      "  Using scale_pos_weight: 3.6667\n",
      "  Grid searching base models on current training data...\n",
      "    Best XGB Params: {'eta': 0.06, 'lambda': 3.0, 'max_depth': 3, 'n_estimators': 65} (F1: 0.481)\n",
      "    Best LGBM Params: {'learning_rate': 0.04, 'max_depth': 4, 'n_estimators': 55, 'subsample': 0.75} (F1: 0.493)\n",
      "    Best SVM Params: {'C': 3.2, 'gamma': 'scale'} (F1: 0.438)\n",
      "  Level 0 OOF Generation Done. Meta Train Shape: (672, 3)\n",
      "  Level 0: Training base models on full training data (672 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Tuning meta learner over 64 param combinations...\n",
      "    Best Meta Params Found: {'colsample_bytree': 0.8, 'eta': 0.05, 'lambda': 1.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 0.75} (Validation F1: 0.5319)\n",
      "    Tuning threshold over range [0.1  0.15 0.2  0.25 0.3  0.35 0.4  0.45 0.5  0.55 0.6  0.65 0.7  0.75\n",
      " 0.8  0.85]...\n",
      "    Best Threshold Found: 0.50 (Validation F1: 0.5319)\n",
      "  Level 1: Training final Meta-Learner on full OOF data...\n",
      "  Level 1 Final Meta Training Done.\n",
      "  Prediction: Generating final predictions on evaluation data...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.5000, Prc=0.1458, Rec=0.8750, F1=0.2500\n",
      "  Iteration 2 finished in 3122.70 seconds.\n",
      "--------------------\n",
      "\n",
      "--- Iter 3/299 ---\n",
      "  Train Indices: [96:767], Eval Indices: [768:935]\n",
      "  Train Target Dist: {0.0: 0.7991071428571429, 1.0: 0.20089285714285715}\n",
      "  Test Target Dist: {0.0: 0.8392857142857143, 1.0: 0.16071428571428573}\n",
      "  Using scale_pos_weight: 3.9778\n",
      "  Grid searching base models on current training data...\n",
      "    Best XGB Params: {'eta': 0.03, 'lambda': 3.0, 'max_depth': 3, 'n_estimators': 65} (F1: 0.467)\n",
      "    Best LGBM Params: {'learning_rate': 0.08, 'max_depth': 4, 'n_estimators': 55, 'subsample': 0.75} (F1: 0.449)\n",
      "    Best SVM Params: {'C': 1.6, 'gamma': 'scale'} (F1: 0.479)\n",
      "  Level 0 OOF Generation Done. Meta Train Shape: (672, 3)\n",
      "  Level 0: Training base models on full training data (672 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Tuning meta learner over 64 param combinations...\n",
      "    Best Meta Params Found: {'colsample_bytree': 1.0, 'eta': 0.05, 'lambda': 1.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 0.75} (Validation F1: 0.5495)\n",
      "    Tuning threshold over range [0.1  0.15 0.2  0.25 0.3  0.35 0.4  0.45 0.5  0.55 0.6  0.65 0.7  0.75\n",
      " 0.8  0.85]...\n",
      "    Best Threshold Found: 0.55 (Validation F1: 0.5588)\n",
      "  Level 1: Training final Meta-Learner on full OOF data...\n",
      "  Level 1 Final Meta Training Done.\n",
      "  Prediction: Generating final predictions on evaluation data...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.3750, Prc=0.1803, Rec=0.8148, F1=0.2953\n",
      "  Iteration 3 finished in 3128.29 seconds.\n",
      "--------------------\n",
      "\n",
      "--- Iter 4/299 ---\n",
      "  Train Indices: [144:815], Eval Indices: [816:983]\n",
      "  Train Target Dist: {0.0: 0.7946428571428571, 1.0: 0.20535714285714285}\n",
      "  Test Target Dist: {0.0: 0.8511904761904762, 1.0: 0.1488095238095238}\n",
      "  Using scale_pos_weight: 3.8696\n",
      "  Grid searching base models on current training data...\n",
      "    Best XGB Params: {'eta': 0.06, 'lambda': 3.0, 'max_depth': 3, 'n_estimators': 65} (F1: 0.456)\n",
      "    Best LGBM Params: {'learning_rate': 0.04, 'max_depth': 4, 'n_estimators': 95, 'subsample': 0.75} (F1: 0.468)\n",
      "    Best SVM Params: {'C': 3.2, 'gamma': 'scale'} (F1: 0.481)\n",
      "  Level 0 OOF Generation Done. Meta Train Shape: (672, 3)\n",
      "  Level 0: Training base models on full training data (672 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Tuning meta learner over 64 param combinations...\n",
      "    Best Meta Params Found: {'colsample_bytree': 1.0, 'eta': 0.05, 'lambda': 1.5, 'max_depth': 2, 'n_estimators': 40, 'subsample': 0.95} (Validation F1: 0.6122)\n",
      "    Tuning threshold over range [0.1  0.15 0.2  0.25 0.3  0.35 0.4  0.45 0.5  0.55 0.6  0.65 0.7  0.75\n",
      " 0.8  0.85]...\n",
      "    Best Threshold Found: 0.50 (Validation F1: 0.6122)\n",
      "  Level 1: Training final Meta-Learner on full OOF data...\n",
      "  Level 1 Final Meta Training Done.\n",
      "  Prediction: Generating final predictions on evaluation data...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.6607, Prc=0.0556, Rec=0.0800, F1=0.0656\n",
      "  Iteration 4 finished in 3133.98 seconds.\n",
      "--------------------\n",
      "\n",
      "--- Iter 5/299 ---\n",
      "  Train Indices: [192:863], Eval Indices: [864:1031]\n",
      "  Train Target Dist: {0.0: 0.8169642857142857, 1.0: 0.18303571428571427}\n",
      "  Test Target Dist: {0.0: 0.7857142857142857, 1.0: 0.21428571428571427}\n",
      "  Using scale_pos_weight: 4.4634\n",
      "  Grid searching base models on current training data...\n",
      "    Best XGB Params: {'eta': 0.06, 'lambda': 1.0, 'max_depth': 2, 'n_estimators': 65} (F1: 0.418)\n",
      "    Best LGBM Params: {'learning_rate': 0.08, 'max_depth': 4, 'n_estimators': 55, 'subsample': 0.75} (F1: 0.417)\n",
      "    Best SVM Params: {'C': 3.2, 'gamma': 'scale'} (F1: 0.424)\n",
      "  Level 0 OOF Generation Done. Meta Train Shape: (672, 3)\n",
      "  Level 0: Training base models on full training data (672 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Tuning meta learner over 64 param combinations...\n",
      "    Best Meta Params Found: {'colsample_bytree': 1.0, 'eta': 0.05, 'lambda': 1.5, 'max_depth': 2, 'n_estimators': 70, 'subsample': 0.95} (Validation F1: 0.5455)\n",
      "    Tuning threshold over range [0.1  0.15 0.2  0.25 0.3  0.35 0.4  0.45 0.5  0.55 0.6  0.65 0.7  0.75\n",
      " 0.8  0.85]...\n",
      "    Best Threshold Found: 0.50 (Validation F1: 0.5455)\n",
      "  Level 1: Training final Meta-Learner on full OOF data...\n",
      "  Level 1 Final Meta Training Done.\n",
      "  Prediction: Generating final predictions on evaluation data...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.2857, Prc=0.2237, Rec=0.9444, F1=0.3617\n",
      "  Iteration 5 finished in 3139.33 seconds.\n",
      "--------------------\n",
      "\n",
      "--- Iter 6/299 ---\n",
      "  Train Indices: [240:911], Eval Indices: [912:1079]\n",
      "  Train Target Dist: {0.0: 0.8169642857142857, 1.0: 0.18303571428571427}\n",
      "  Test Target Dist: {0.0: 0.7976190476190477, 1.0: 0.20238095238095238}\n",
      "  Using scale_pos_weight: 4.4634\n",
      "  Grid searching base models on current training data...\n",
      "    Best XGB Params: {'eta': 0.06, 'lambda': 1.0, 'max_depth': 2, 'n_estimators': 65} (F1: 0.459)\n",
      "    Best LGBM Params: {'learning_rate': 0.04, 'max_depth': 4, 'n_estimators': 95, 'subsample': 0.75} (F1: 0.470)\n",
      "    Best SVM Params: {'C': 3.2, 'gamma': 'scale'} (F1: 0.418)\n",
      "  Level 0 OOF Generation Done. Meta Train Shape: (672, 3)\n",
      "  Level 0: Training base models on full training data (672 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Tuning meta learner over 64 param combinations...\n",
      "    Best Meta Params Found: {'colsample_bytree': 1.0, 'eta': 0.05, 'lambda': 1.0, 'max_depth': 2, 'n_estimators': 40, 'subsample': 0.75} (Validation F1: 0.5806)\n",
      "    Tuning threshold over range [0.1  0.15 0.2  0.25 0.3  0.35 0.4  0.45 0.5  0.55 0.6  0.65 0.7  0.75\n",
      " 0.8  0.85]...\n",
      "    Best Threshold Found: 0.50 (Validation F1: 0.5806)\n",
      "  Level 1: Training final Meta-Learner on full OOF data...\n",
      "  Level 1 Final Meta Training Done.\n",
      "  Prediction: Generating final predictions on evaluation data...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.4762, Prc=0.2300, Rec=0.6765, F1=0.3433\n",
      "  Iteration 6 finished in 3144.68 seconds.\n",
      "--------------------\n",
      "\n",
      "--- Iter 7/299 ---\n",
      "  Train Indices: [288:959], Eval Indices: [960:1127]\n",
      "  Train Target Dist: {0.0: 0.8169642857142857, 1.0: 0.18303571428571427}\n",
      "  Test Target Dist: {0.0: 0.8154761904761905, 1.0: 0.18452380952380953}\n",
      "  Using scale_pos_weight: 4.4634\n",
      "  Grid searching base models on current training data...\n",
      "    Best XGB Params: {'eta': 0.06, 'lambda': 3.0, 'max_depth': 3, 'n_estimators': 65} (F1: 0.403)\n",
      "    Best LGBM Params: {'learning_rate': 0.04, 'max_depth': 4, 'n_estimators': 95, 'subsample': 0.75} (F1: 0.460)\n",
      "    Best SVM Params: {'C': 3.2, 'gamma': 'scale'} (F1: 0.391)\n",
      "  Level 0 OOF Generation Done. Meta Train Shape: (672, 3)\n",
      "  Level 0: Training base models on full training data (672 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Tuning meta learner over 64 param combinations...\n",
      "    Best Meta Params Found: {'colsample_bytree': 0.8, 'eta': 0.03, 'lambda': 1.5, 'max_depth': 3, 'n_estimators': 70, 'subsample': 0.95} (Validation F1: 0.4444)\n",
      "    Tuning threshold over range [0.1  0.15 0.2  0.25 0.3  0.35 0.4  0.45 0.5  0.55 0.6  0.65 0.7  0.75\n",
      " 0.8  0.85]...\n",
      "    Best Threshold Found: 0.70 (Validation F1: 0.4444)\n",
      "  Level 1: Training final Meta-Learner on full OOF data...\n",
      "  Level 1 Final Meta Training Done.\n",
      "  Prediction: Generating final predictions on evaluation data...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.7976, Prc=0.3636, Rec=0.1290, F1=0.1905\n",
      "  Iteration 7 finished in 3150.18 seconds.\n",
      "--------------------\n",
      "\n",
      "--- Iter 8/299 ---\n",
      "  Train Indices: [336:1007], Eval Indices: [1008:1175]\n",
      "  Train Target Dist: {0.0: 0.8139880952380952, 1.0: 0.18601190476190477}\n",
      "  Test Target Dist: {0.0: 0.8273809523809523, 1.0: 0.17261904761904762}\n",
      "  Using scale_pos_weight: 4.3760\n",
      "  Grid searching base models on current training data...\n",
      "    Best XGB Params: {'eta': 0.06, 'lambda': 1.0, 'max_depth': 3, 'n_estimators': 65} (F1: 0.492)\n",
      "    Best LGBM Params: {'learning_rate': 0.08, 'max_depth': 4, 'n_estimators': 95, 'subsample': 0.75} (F1: 0.517)\n",
      "    Best SVM Params: {'C': 3.2, 'gamma': 'scale'} (F1: 0.465)\n",
      "  Level 0 OOF Generation Done. Meta Train Shape: (672, 3)\n",
      "  Level 0: Training base models on full training data (672 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Tuning meta learner over 64 param combinations...\n",
      "    Best Meta Params Found: {'colsample_bytree': 1.0, 'eta': 0.05, 'lambda': 1.5, 'max_depth': 2, 'n_estimators': 70, 'subsample': 0.95} (Validation F1: 0.5000)\n",
      "    Tuning threshold over range [0.1  0.15 0.2  0.25 0.3  0.35 0.4  0.45 0.5  0.55 0.6  0.65 0.7  0.75\n",
      " 0.8  0.85]...\n",
      "    Best Threshold Found: 0.65 (Validation F1: 0.5135)\n",
      "  Level 1: Training final Meta-Learner on full OOF data...\n",
      "  Level 1 Final Meta Training Done.\n",
      "  Prediction: Generating final predictions on evaluation data...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.6845, Prc=0.2778, Rec=0.5172, F1=0.3614\n",
      "  Iteration 8 finished in 3155.46 seconds.\n",
      "--------------------\n",
      "\n",
      "--- Iter 9/299 ---\n",
      "  Train Indices: [384:1055], Eval Indices: [1056:1223]\n",
      "  Train Target Dist: {0.0: 0.8199404761904762, 1.0: 0.1800595238095238}\n",
      "  Test Target Dist: {0.0: 0.8511904761904762, 1.0: 0.1488095238095238}\n",
      "  Using scale_pos_weight: 4.5537\n",
      "  Grid searching base models on current training data...\n",
      "    Best XGB Params: {'eta': 0.06, 'lambda': 3.0, 'max_depth': 3, 'n_estimators': 65} (F1: 0.452)\n",
      "    Best LGBM Params: {'learning_rate': 0.04, 'max_depth': 4, 'n_estimators': 95, 'subsample': 0.75} (F1: 0.481)\n",
      "    Best SVM Params: {'C': 3.2, 'gamma': 'scale'} (F1: 0.431)\n",
      "  Level 0 OOF Generation Done. Meta Train Shape: (672, 3)\n",
      "  Level 0: Training base models on full training data (672 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Tuning meta learner over 64 param combinations...\n",
      "    Best Meta Params Found: {'colsample_bytree': 0.8, 'eta': 0.05, 'lambda': 1.5, 'max_depth': 2, 'n_estimators': 70, 'subsample': 0.75} (Validation F1: 0.5714)\n",
      "    Tuning threshold over range [0.1  0.15 0.2  0.25 0.3  0.35 0.4  0.45 0.5  0.55 0.6  0.65 0.7  0.75\n",
      " 0.8  0.85]...\n",
      "    Best Threshold Found: 0.60 (Validation F1: 0.5773)\n",
      "  Level 1: Training final Meta-Learner on full OOF data...\n",
      "  Level 1 Final Meta Training Done.\n",
      "  Prediction: Generating final predictions on evaluation data...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.8036, Prc=0.3750, Rec=0.4800, F1=0.4211\n",
      "  Iteration 9 finished in 3160.75 seconds.\n",
      "--------------------\n",
      "\n",
      "--- Iter 10/299 ---\n",
      "  Train Indices: [432:1103], Eval Indices: [1104:1271]\n",
      "  Train Target Dist: {0.0: 0.8169642857142857, 1.0: 0.18303571428571427}\n",
      "  Test Target Dist: {0.0: 0.8452380952380952, 1.0: 0.15476190476190477}\n",
      "  Using scale_pos_weight: 4.4634\n",
      "  Grid searching base models on current training data...\n",
      "    Best XGB Params: {'eta': 0.06, 'lambda': 1.0, 'max_depth': 3, 'n_estimators': 65} (F1: 0.459)\n",
      "    Best LGBM Params: {'learning_rate': 0.04, 'max_depth': 4, 'n_estimators': 95, 'subsample': 0.75} (F1: 0.485)\n",
      "    Best SVM Params: {'C': 3.2, 'gamma': 'scale'} (F1: 0.480)\n",
      "  Level 0 OOF Generation Done. Meta Train Shape: (672, 3)\n",
      "  Level 0: Training base models on full training data (672 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Tuning meta learner over 64 param combinations...\n",
      "    Best Meta Params Found: {'colsample_bytree': 1.0, 'eta': 0.05, 'lambda': 1.5, 'max_depth': 3, 'n_estimators': 70, 'subsample': 0.75} (Validation F1: 0.6923)\n",
      "    Tuning threshold over range [0.1  0.15 0.2  0.25 0.3  0.35 0.4  0.45 0.5  0.55 0.6  0.65 0.7  0.75\n",
      " 0.8  0.85]...\n",
      "    Best Threshold Found: 0.50 (Validation F1: 0.6923)\n",
      "  Level 1: Training final Meta-Learner on full OOF data...\n",
      "  Level 1 Final Meta Training Done.\n",
      "  Prediction: Generating final predictions on evaluation data...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.6667, Prc=0.2414, Rec=0.5385, F1=0.3333\n",
      "  Iteration 10 finished in 3166.24 seconds.\n",
      "--------------------\n",
      "\n",
      "--- Iter 11/299 ---\n",
      "  Train Indices: [480:1151], Eval Indices: [1152:1319]\n",
      "  Train Target Dist: {0.0: 0.8169642857142857, 1.0: 0.18303571428571427}\n",
      "  Test Target Dist: {0.0: 0.8869047619047619, 1.0: 0.1130952380952381}\n",
      "  Using scale_pos_weight: 4.4634\n",
      "  Grid searching base models on current training data...\n",
      "    Best XGB Params: {'eta': 0.03, 'lambda': 1.0, 'max_depth': 3, 'n_estimators': 65} (F1: 0.532)\n",
      "    Best LGBM Params: {'learning_rate': 0.08, 'max_depth': 4, 'n_estimators': 55, 'subsample': 0.75} (F1: 0.602)\n",
      "    Best SVM Params: {'C': 3.2, 'gamma': 'scale'} (F1: 0.466)\n",
      "  Level 0 OOF Generation Done. Meta Train Shape: (672, 3)\n",
      "  Level 0: Training base models on full training data (672 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Tuning meta learner over 64 param combinations...\n",
      "    Best Meta Params Found: {'colsample_bytree': 1.0, 'eta': 0.05, 'lambda': 1.5, 'max_depth': 3, 'n_estimators': 70, 'subsample': 0.95} (Validation F1: 0.6667)\n",
      "    Tuning threshold over range [0.1  0.15 0.2  0.25 0.3  0.35 0.4  0.45 0.5  0.55 0.6  0.65 0.7  0.75\n",
      " 0.8  0.85]...\n",
      "    Best Threshold Found: 0.50 (Validation F1: 0.6667)\n",
      "  Level 1: Training final Meta-Learner on full OOF data...\n",
      "  Level 1 Final Meta Training Done.\n",
      "  Prediction: Generating final predictions on evaluation data...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.4048, Prc=0.1478, Rec=0.8947, F1=0.2537\n",
      "  Iteration 11 finished in 3171.59 seconds.\n",
      "--------------------\n",
      "\n",
      "--- Iter 12/299 ---\n",
      "  Train Indices: [528:1199], Eval Indices: [1200:1367]\n",
      "  Train Target Dist: {0.0: 0.8303571428571429, 1.0: 0.16964285714285715}\n",
      "  Test Target Dist: {0.0: 0.8214285714285714, 1.0: 0.17857142857142858}\n",
      "  Using scale_pos_weight: 4.8947\n",
      "  Grid searching base models on current training data...\n",
      "    Best XGB Params: {'eta': 0.06, 'lambda': 3.0, 'max_depth': 3, 'n_estimators': 65} (F1: 0.534)\n",
      "    Best LGBM Params: {'learning_rate': 0.08, 'max_depth': 4, 'n_estimators': 95, 'subsample': 0.75} (F1: 0.552)\n",
      "    Best SVM Params: {'C': 3.2, 'gamma': 'scale'} (F1: 0.506)\n",
      "  Level 0 OOF Generation Done. Meta Train Shape: (672, 3)\n",
      "  Level 0: Training base models on full training data (672 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Tuning meta learner over 64 param combinations...\n",
      "    Best Meta Params Found: {'colsample_bytree': 0.8, 'eta': 0.03, 'lambda': 1.0, 'max_depth': 2, 'n_estimators': 40, 'subsample': 0.75} (Validation F1: 0.5974)\n",
      "    Tuning threshold over range [0.1  0.15 0.2  0.25 0.3  0.35 0.4  0.45 0.5  0.55 0.6  0.65 0.7  0.75\n",
      " 0.8  0.85]...\n",
      "    Best Threshold Found: 0.55 (Validation F1: 0.6000)\n",
      "  Level 1: Training final Meta-Learner on full OOF data...\n",
      "  Level 1 Final Meta Training Done.\n",
      "  Prediction: Generating final predictions on evaluation data...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.3869, Prc=0.2214, Rec=0.9667, F1=0.3602\n",
      "  Iteration 12 finished in 3176.94 seconds.\n",
      "--------------------\n",
      "\n",
      "--- Iter 13/299 ---\n",
      "  Train Indices: [576:1247], Eval Indices: [1248:1415]\n",
      "  Train Target Dist: {0.0: 0.8273809523809523, 1.0: 0.17261904761904762}\n",
      "  Test Target Dist: {0.0: 0.8035714285714286, 1.0: 0.19642857142857142}\n",
      "  Using scale_pos_weight: 4.7931\n",
      "  Grid searching base models on current training data...\n",
      "    Best XGB Params: {'eta': 0.06, 'lambda': 3.0, 'max_depth': 3, 'n_estimators': 65} (F1: 0.473)\n",
      "    Best LGBM Params: {'learning_rate': 0.08, 'max_depth': 4, 'n_estimators': 55, 'subsample': 0.75} (F1: 0.515)\n",
      "    Best SVM Params: {'C': 1.6, 'gamma': 'scale'} (F1: 0.546)\n",
      "  Level 0 OOF Generation Done. Meta Train Shape: (672, 3)\n",
      "  Level 0: Training base models on full training data (672 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Tuning meta learner over 64 param combinations...\n",
      "    Best Meta Params Found: {'colsample_bytree': 1.0, 'eta': 0.05, 'lambda': 1.5, 'max_depth': 3, 'n_estimators': 70, 'subsample': 0.75} (Validation F1: 0.5625)\n",
      "    Tuning threshold over range [0.1  0.15 0.2  0.25 0.3  0.35 0.4  0.45 0.5  0.55 0.6  0.65 0.7  0.75\n",
      " 0.8  0.85]...\n",
      "    Best Threshold Found: 0.55 (Validation F1: 0.5667)\n",
      "  Level 1: Training final Meta-Learner on full OOF data...\n",
      "  Level 1 Final Meta Training Done.\n",
      "  Prediction: Generating final predictions on evaluation data...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.5119, Prc=0.2793, Rec=0.9394, F1=0.4306\n",
      "  Iteration 13 finished in 3182.37 seconds.\n",
      "--------------------\n",
      "\n",
      "--- Iter 14/299 ---\n",
      "  Train Indices: [624:1295], Eval Indices: [1296:1463]\n",
      "  Train Target Dist: {0.0: 0.8273809523809523, 1.0: 0.17261904761904762}\n",
      "  Test Target Dist: {0.0: 0.8095238095238095, 1.0: 0.19047619047619047}\n",
      "  Using scale_pos_weight: 4.7931\n",
      "  Grid searching base models on current training data...\n",
      "    Best XGB Params: {'eta': 0.06, 'lambda': 3.0, 'max_depth': 3, 'n_estimators': 65} (F1: 0.516)\n",
      "    Best LGBM Params: {'learning_rate': 0.08, 'max_depth': 4, 'n_estimators': 95, 'subsample': 0.75} (F1: 0.539)\n",
      "    Best SVM Params: {'C': 1.6, 'gamma': 'scale'} (F1: 0.449)\n",
      "  Level 0 OOF Generation Done. Meta Train Shape: (672, 3)\n",
      "  Level 0: Training base models on full training data (672 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Tuning meta learner over 64 param combinations...\n",
      "    Best Meta Params Found: {'colsample_bytree': 0.8, 'eta': 0.05, 'lambda': 1.0, 'max_depth': 3, 'n_estimators': 70, 'subsample': 0.75} (Validation F1: 0.5152)\n",
      "    Tuning threshold over range [0.1  0.15 0.2  0.25 0.3  0.35 0.4  0.45 0.5  0.55 0.6  0.65 0.7  0.75\n",
      " 0.8  0.85]...\n",
      "    Best Threshold Found: 0.60 (Validation F1: 0.5246)\n",
      "  Level 1: Training final Meta-Learner on full OOF data...\n",
      "  Level 1 Final Meta Training Done.\n",
      "  Prediction: Generating final predictions on evaluation data...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.5952, Prc=0.3043, Rec=0.8750, F1=0.4516\n",
      "  Iteration 14 finished in 3188.10 seconds.\n",
      "--------------------\n",
      "\n",
      "--- Iter 15/299 ---\n",
      "  Train Indices: [672:1343], Eval Indices: [1344:1511]\n",
      "  Train Target Dist: {0.0: 0.8452380952380952, 1.0: 0.15476190476190477}\n",
      "  Test Target Dist: {0.0: 0.7321428571428571, 1.0: 0.26785714285714285}\n",
      "  Using scale_pos_weight: 5.4615\n",
      "  Grid searching base models on current training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function DMatrix.__del__ at 0x00000264E0294C20>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mason\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\core.py\", line 797, in __del__\n",
      "    _check_call(_LIB.XGDMatrixFree(self.handle))\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Best XGB Params: {'eta': 0.06, 'lambda': 3.0, 'max_depth': 2, 'n_estimators': 65} (F1: 0.487)\n",
      "    Best LGBM Params: {'learning_rate': 0.08, 'max_depth': 4, 'n_estimators': 95, 'subsample': 0.75} (F1: 0.554)\n",
      "    Best SVM Params: {'C': 3.2, 'gamma': 'scale'} (F1: 0.463)\n",
      "  Level 0 OOF Generation Done. Meta Train Shape: (672, 3)\n",
      "  Level 0: Training base models on full training data (672 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Tuning meta learner over 64 param combinations...\n",
      "    Best Meta Params Found: {'colsample_bytree': 0.8, 'eta': 0.05, 'lambda': 1.0, 'max_depth': 3, 'n_estimators': 40, 'subsample': 0.95} (Validation F1: 0.3902)\n",
      "    Tuning threshold over range [0.1  0.15 0.2  0.25 0.3  0.35 0.4  0.45 0.5  0.55 0.6  0.65 0.7  0.75\n",
      " 0.8  0.85]...\n",
      "    Best Threshold Found: 0.60 (Validation F1: 0.4127)\n",
      "  Level 1: Training final Meta-Learner on full OOF data...\n",
      "  Level 1 Final Meta Training Done.\n",
      "  Prediction: Generating final predictions on evaluation data...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.6429, Prc=0.3913, Rec=0.6000, F1=0.4737\n",
      "  Iteration 15 finished in 3193.56 seconds.\n",
      "--------------------\n",
      "\n",
      "--- Iter 16/299 ---\n",
      "  Train Indices: [720:1391], Eval Indices: [1392:1559]\n",
      "  Train Target Dist: {0.0: 0.8318452380952381, 1.0: 0.16815476190476192}\n",
      "  Test Target Dist: {0.0: 0.7202380952380952, 1.0: 0.27976190476190477}\n",
      "  Using scale_pos_weight: 4.9469\n",
      "  Grid searching base models on current training data...\n",
      "    Best XGB Params: {'eta': 0.06, 'lambda': 1.0, 'max_depth': 3, 'n_estimators': 65} (F1: 0.490)\n",
      "    Best LGBM Params: {'learning_rate': 0.08, 'max_depth': 4, 'n_estimators': 95, 'subsample': 0.75} (F1: 0.558)\n",
      "    Best SVM Params: {'C': 1.6, 'gamma': 'scale'} (F1: 0.459)\n",
      "  Level 0 OOF Generation Done. Meta Train Shape: (672, 3)\n",
      "  Level 0: Training base models on full training data (672 rows)...\n",
      "  Level 0 Full Training Done.\n",
      "  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\n",
      "    Tuning meta learner over 64 param combinations...\n",
      "    Best Meta Params Found: {'colsample_bytree': 1.0, 'eta': 0.05, 'lambda': 1.0, 'max_depth': 3, 'n_estimators': 70, 'subsample': 0.95} (Validation F1: 0.5647)\n",
      "    Tuning threshold over range [0.1  0.15 0.2  0.25 0.3  0.35 0.4  0.45 0.5  0.55 0.6  0.65 0.7  0.75\n",
      " 0.8  0.85]...\n",
      "    Best Threshold Found: 0.70 (Validation F1: 0.6111)\n",
      "  Level 1: Training final Meta-Learner on full OOF data...\n",
      "  Level 1 Final Meta Training Done.\n",
      "  Prediction: Generating final predictions on evaluation data...\n",
      "  Prediction Done.\n",
      "  Evaluation Metrics (Test Window Size: 168h): Acc=0.6667, Prc=0.3953, Rec=0.3617, F1=0.3778\n",
      "  Iteration 16 finished in 3198.99 seconds.\n",
      "--------------------\n",
      "\n",
      "--- Iter 17/299 ---\n",
      "  Train Indices: [768:1439], Eval Indices: [1440:1607]\n",
      "  Train Target Dist: {0.0: 0.8273809523809523, 1.0: 0.17261904761904762}\n",
      "  Test Target Dist: {0.0: 0.6904761904761905, 1.0: 0.30952380952380953}\n",
      "  Using scale_pos_weight: 4.7931\n",
      "  Grid searching base models on current training data...\n"
     ]
    }
   ],
   "source": [
    "# Ensemble_Method_B.py\n",
    "# Combined Script: Load CSV -> Feature Engineering -> Rolling Origin XGB Modeling\n",
    "# Uses the feature set from Simple_Predictor_B\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "\n",
    "# Feature Engineering Imports\n",
    "import pandas_ta as ta  # Technical indicators\n",
    "\n",
    "# Modeling Imports\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier, early_stopping, log_evaluation  # <--- IMPORT CALLBACKS\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler  # Needed for SVM\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import ParameterGrid, StratifiedKFold  # StratifiedKFold for stacking\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "from sklearn.pipeline import Pipeline  # Optional: useful for SVM with scaling\n",
    "from sklearn.impute import SimpleImputer  # Better imputation strategy for pipeline\n",
    "\n",
    "# --- Suppress Warnings ---\n",
    "warnings.filterwarnings('ignore', category=UndefinedMetricWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)\n",
    "warnings.filterwarnings('ignore')  # General suppression\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "# Data Loading\n",
    "CSV_FILE_PATH = r'C:\\Users\\mason\\AVP\\BTCUSDrec.csv'  # Use raw string for Windows paths\n",
    "SYMBOL_NAME = 'BTCUSD'  # Define the symbol represented in the CSV\n",
    "\n",
    "# Feature Selection (From Simple_Predictor_B - 49 Features)\n",
    "SELECTED_FEATURE_NAMES = [\n",
    "    'open', 'high', 'low', 'close', 'Volume BTC', 'Volume USD',  # Base columns (will be selected later if present)\n",
    "    'price_range_pct', 'oc_change_pct', 'garman_klass_12h', 'parkinson_3h',\n",
    "    'ma_3h', 'rolling_std_3h', 'lag_3h_price_return', 'lag_6h_price_return',\n",
    "    'lag_12h_price_return', 'lag_24h_price_return', 'lag_48h_price_return',\n",
    "    'lag_72h_price_return', 'lag_168h_price_return', 'volume_return_1h',\n",
    "    'lag_3h_volume_return', 'lag_6h_volume_return', 'lag_12h_volume_return',\n",
    "    'lag_24h_volume_return', 'ma_6h', 'ma_12h', 'ma_24h', 'ma_48h',\n",
    "    'ma_72h', 'ma_168h', 'rolling_std_6h', 'rolling_std_12h',\n",
    "    'rolling_std_24h', 'rolling_std_48h', 'rolling_std_72h',\n",
    "    'rolling_std_168h', 'atr_14h', 'atr_24h', 'atr_48h', 'close_div_ma_24h',\n",
    "    'close_div_ma_48h', 'close_div_ma_168h', 'ma12_div_ma48',\n",
    "    'ma24_div_ma168', 'std12_div_std72', 'volume_btc_x_range',\n",
    "    'rolling_std_3h_sq', 'price_return_1h_sq', 'rolling_std_12h_sqrt'\n",
    "]\n",
    "# Remove the base OHLCV columns from the list for modeling.\n",
    "MODEL_FEATURE_COLS = [f for f in SELECTED_FEATURE_NAMES if f not in ['open', 'high', 'low', 'close', 'Volume BTC', 'Volume USD']]\n",
    "\n",
    "# Modeling & Walk-Forward\n",
    "TARGET_THRESHOLD_PCT = 0.25  # Target: >= 0% increase over next 12h\n",
    "\n",
    "# Define separate prediction horizon for target\n",
    "PREDICTION_WINDOW_HOURS = 4  # Predict outcome 12 hours ahead\n",
    "PREDICTION_WINDOW_ROWS = PREDICTION_WINDOW_HOURS\n",
    "\n",
    "# Walk-forward params\n",
    "TRAIN_WINDOW_HOURS = int(24 * 7 * 4)  # Training size (in hours)\n",
    "TEST_WINDOW_HOURS = 24 * 7           # Evaluation window (in hours)\n",
    "STEP_HOURS = 48                     # Retrain and predict daily\n",
    "\n",
    "TRAIN_WINDOW_ROWS = TRAIN_WINDOW_HOURS\n",
    "TEST_WINDOW_ROWS = TEST_WINDOW_HOURS  # Evaluation window size (in rows)\n",
    "STEP_ROWS = STEP_HOURS\n",
    "\n",
    "# Stacking Configuration\n",
    "N_STACKING_FOLDS = 7  # Number of folds for generating Level 0 predictions\n",
    "\n",
    "# --- New Base Model Parameter Grids (max ~16 combinations each) ---\n",
    "BASE_XGB_PARAM_GRID = {\n",
    "    'max_depth': [2, 3],\n",
    "    'n_estimators': [35, 65],\n",
    "    'eta': [0.03, 0.06],\n",
    "    'lambda': [1.0, 3.0]\n",
    "}\n",
    "\n",
    "BASE_LGBM_PARAM_GRID = {\n",
    "    'max_depth': [2, 4],\n",
    "    'n_estimators': [55, 95],\n",
    "    'learning_rate': [0.04, 0.08],\n",
    "    'subsample': [0.75, 0.9]\n",
    "}\n",
    "\n",
    "BASE_SVM_PARAM_GRID = {\n",
    "    'C': [1.6, 3.2],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# --- Base Model Static Hyperparameters (default values) ---\n",
    "XGB_BASE_PARAMS = {\n",
    "    'objective': 'binary:logistic', \n",
    "    'eval_metric': 'logloss',\n",
    "    #'eta': 0.05, \n",
    "    #'max_depth': 3, \n",
    "    #'n_estimators': 85,\n",
    "    'subsample': 0.8, \n",
    "    'colsample_bytree': 0.7, \n",
    "    'min_child_weight': 3,\n",
    "    'gamma': 0.1, \n",
    "    #'lambda': 3, \n",
    "    'alpha': 0.1,\n",
    "    'random_state': 42, \n",
    "    'n_jobs': -1, \n",
    "    'tree_method': 'hist',\n",
    "    'use_label_encoder': False,\n",
    "}\n",
    "LGBM_BASE_PARAMS = {\n",
    "    'objective': 'binary', \n",
    "    'metric': 'logloss',\n",
    "    #'learning_rate': 0.1, \n",
    "    #'n_estimators': 105, \n",
    "    #'max_depth': 4,\n",
    "    'num_leaves': 8,\n",
    "    #'subsample': 0.8, \n",
    "    'colsample_bytree': 0.7, \n",
    "    'min_child_samples': 5,\n",
    "    'reg_alpha': 0.1, \n",
    "    'reg_lambda': 1.5,\n",
    "    'random_state': 42, \n",
    "    'n_jobs': -1, \n",
    "    'boosting_type': 'gbdt',\n",
    "    'verbose': -1\n",
    "}\n",
    "SVM_BASE_PARAMS = {\n",
    "    'kernel': 'rbf',\n",
    "    #'C': 2.4,\n",
    "    'probability': True,\n",
    "    'max_iter': 5000,\n",
    "    'random_state': 42,\n",
    "    'class_weight': 'balanced'\n",
    "}\n",
    "\n",
    "# --- Meta Learner Configuration ---\n",
    "META_LEARNER_IS_XGB = True\n",
    "META_XGB_PARAM_GRID = {\n",
    "    'max_depth': [2, 3],\n",
    "    'n_estimators': [40, 70],\n",
    "    'eta': [0.03, 0.05],\n",
    "    'lambda': [1.0, 1.5],\n",
    "    'subsample': [0.75, 0.95],\n",
    "    'colsample_bytree': [0.80, 1.0]\n",
    "}\n",
    "META_XGB_FIXED_PARAMS = {\n",
    "    'objective': 'binary:logistic', \n",
    "    'eval_metric': 'logloss',\n",
    "    'gamma': 0.0, \n",
    "    'alpha': 0.1,\n",
    "    'random_state': 42, \n",
    "    'n_jobs': -1, \n",
    "    'tree_method': 'hist',\n",
    "    'use_label_encoder': False,\n",
    "    'min_child_weight': 3\n",
    "}\n",
    "\n",
    "# --- Probability Threshold Tuning Configuration ---\n",
    "THRESHOLD_SEARCH_RANGE = np.arange(0.10, 0.90, 0.05)\n",
    "META_VALIDATION_PCT = 0.25\n",
    "\n",
    "# --- Feature Engineering Function (Revised for Simple_Predictor_B Features) ---\n",
    "def garman_klass_volatility(open_, high, low, close, window):\n",
    "    log_hl = np.log(high / low)\n",
    "    log_co = np.log(close / open_)\n",
    "    gk = 0.5 * (log_hl ** 2) - (2 * np.log(2) - 1) * (log_co ** 2)\n",
    "    rolling_mean = gk.rolling(window=window).mean()\n",
    "    rolling_mean = rolling_mean.clip(lower=0)\n",
    "    return np.sqrt(rolling_mean)\n",
    "\n",
    "def parkinson_volatility(high, low, window):\n",
    "    log_hl_sq = np.log(high / low) ** 2\n",
    "    rolling_sum = log_hl_sq.rolling(window=window).sum()\n",
    "    factor = 1 / (4 * np.log(2) * window)\n",
    "    return np.sqrt(factor * rolling_sum)\n",
    "\n",
    "def calculate_selected_features(df, symbol):\n",
    "    \"\"\"\n",
    "    Calculates the features required by Simple_Predictor_B using custom methods for\n",
    "    ATR, Garman-Klass, and Parkinson volatility.\n",
    "    \"\"\"\n",
    "    print(f\"Starting calculation for {len(SELECTED_FEATURE_NAMES)} target columns (incl. base)...\")\n",
    "    start_time = time.time()\n",
    "    if df is None or len(df) < 3:\n",
    "        return pd.DataFrame()\n",
    "    df = df.copy()\n",
    "    df['symbol'] = symbol\n",
    "\n",
    "    # --- Timestamp and Index ---\n",
    "    if 'timestamp' not in df.columns:\n",
    "        print(\"Error: 'timestamp' column not found.\")\n",
    "        return pd.DataFrame()\n",
    "    try:\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting timestamp: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    df = df.sort_values('timestamp').dropna(subset=['timestamp'])\n",
    "    df = df.set_index('timestamp', drop=False)\n",
    "\n",
    "    # --- Volume Columns ---\n",
    "    original_vol_btc_name = 'Volume BTC'\n",
    "    original_vol_usd_name = 'Volume USD'\n",
    "    if original_vol_btc_name not in df.columns:\n",
    "        df[original_vol_btc_name] = 0\n",
    "    if original_vol_usd_name not in df.columns:\n",
    "        df[original_vol_usd_name] = 0\n",
    "    df[original_vol_btc_name] = pd.to_numeric(df[original_vol_btc_name], errors='coerce').fillna(0)\n",
    "    df[original_vol_usd_name] = pd.to_numeric(df[original_vol_usd_name], errors='coerce').fillna(0)\n",
    "\n",
    "    # --- Basic Checks (OHLC) ---\n",
    "    required_ohlc = ['open', 'high', 'low', 'close']\n",
    "    if not all(col in df.columns for col in required_ohlc):\n",
    "        print(f\"Error: Missing required OHLC columns: {required_ohlc}\")\n",
    "        return pd.DataFrame()\n",
    "    for col in required_ohlc:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    if df[required_ohlc].isnull().any().any():\n",
    "        print(\"Warning: NaNs found in OHLC data. Dropping affected rows.\")\n",
    "        df = df.dropna(subset=required_ohlc)\n",
    "    if df.empty:\n",
    "        print(\"DataFrame empty after OHLC checks.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(\"  Calculating features...\")\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        df['price_range_pct'] = (df['high'] - df['low']) / df['close']\n",
    "        df['oc_change_pct'] = (df['close'] - df['open']) / df['open']\n",
    "        df['price_return_1h'] = df['close'].pct_change()\n",
    "        df['volume_return_1h'] = df[original_vol_btc_name].pct_change()\n",
    "\n",
    "    # --- Lagged Returns ---\n",
    "    lag_price_hours = [3, 6, 12, 24, 48, 72, 168]\n",
    "    lag_volume_hours = [3, 6, 12, 24]\n",
    "    for hours in lag_price_hours:\n",
    "        df[f'lag_{hours}h_price_return'] = df['close'].pct_change(periods=hours)\n",
    "    for hours in lag_volume_hours:\n",
    "        df[f'lag_{hours}h_volume_return'] = df[original_vol_btc_name].pct_change(periods=hours)\n",
    "\n",
    "    # --- Moving Averages ---\n",
    "    ma_hours = [3, 6, 12, 24, 48, 72, 168]\n",
    "    for hours in ma_hours:\n",
    "        df[f'ma_{hours}h'] = df['close'].rolling(window=hours, min_periods=hours).mean()\n",
    "\n",
    "    # --- Rolling Standard Deviations ---\n",
    "    std_hours = [3, 6, 12, 24, 48, 72, 168]\n",
    "    for hours in std_hours:\n",
    "        df[f'rolling_std_{hours}h'] = df['price_return_1h'].rolling(window=hours, min_periods=hours).std() * 100\n",
    "\n",
    "    # --- ATR Calculation ---\n",
    "    print(\"    Calculating ATR, Garman-Klass, and Parkinson volatility features...\")\n",
    "    df['prev_close'] = df['close'].shift(1)\n",
    "    df['high_minus_low'] = df['high'] - df['low']\n",
    "    df['high_minus_prev_close'] = np.abs(df['high'] - df['prev_close'])\n",
    "    df['low_minus_prev_close'] = np.abs(df['low'] - df['prev_close'])\n",
    "    df['true_range'] = df[['high_minus_low', 'high_minus_prev_close', 'low_minus_prev_close']].max(axis=1)\n",
    "    for p in [14, 24, 48]:\n",
    "         df[f'atr_{p}h'] = df['true_range'].rolling(window=p, min_periods=p).mean()\n",
    "    df = df.drop(columns=['prev_close', 'high_minus_low', 'high_minus_prev_close', 'low_minus_prev_close', 'true_range'])\n",
    "\n",
    "    # --- Garman-Klass and Parkinson Volatility ---\n",
    "    df['garman_klass_12h'] = garman_klass_volatility(df['open'], df['high'], df['low'], df['close'], window=12)\n",
    "    df['parkinson_3h'] = parkinson_volatility(df['high'], df['low'], window=3)\n",
    "\n",
    "    # --- Ratio Features ---\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        for hours in [24, 48, 168]:\n",
    "            ma_col = f'ma_{hours}h'\n",
    "            if ma_col in df.columns:\n",
    "                df[f'close_div_ma_{hours}h'] = df['close'] / df[ma_col].replace(0, np.nan)\n",
    "            else:\n",
    "                df[f'close_div_ma_{hours}h'] = np.nan\n",
    "        if 'ma_12h' in df.columns and 'ma_48h' in df.columns:\n",
    "            df['ma12_div_ma48'] = df['ma_12h'] / df['ma_48h'].replace(0, np.nan)\n",
    "        else:\n",
    "            df['ma12_div_ma48'] = np.nan\n",
    "        if 'ma_24h' in df.columns and 'ma_168h' in df.columns:\n",
    "            df['ma24_div_ma168'] = df['ma_24h'] / df['ma_168h'].replace(0, np.nan)\n",
    "        else:\n",
    "            df['ma24_div_ma168'] = np.nan\n",
    "        if 'rolling_std_12h' in df.columns and 'rolling_std_72h' in df.columns:\n",
    "            df['std12_div_std72'] = df['rolling_std_12h'] / df['rolling_std_72h'].replace(0, np.nan)\n",
    "        else:\n",
    "            df['std12_div_std72'] = np.nan\n",
    "        if 'price_range_pct' in df.columns:\n",
    "            df['volume_btc_x_range'] = df[original_vol_btc_name] * df['price_range_pct']\n",
    "        else:\n",
    "            df['volume_btc_x_range'] = np.nan\n",
    "\n",
    "    # --- Non-linear Transformations ---\n",
    "    if 'rolling_std_3h' in df.columns:\n",
    "        df['rolling_std_3h_sq'] = df['rolling_std_3h'] ** 2\n",
    "    else:\n",
    "        df['rolling_std_3h_sq'] = np.nan\n",
    "    if 'price_return_1h' in df.columns:\n",
    "        df['price_return_1h_sq'] = (df['price_return_1h'] ** 2) * 10000\n",
    "    else:\n",
    "        df['price_return_1h_sq'] = np.nan\n",
    "    if 'rolling_std_12h' in df.columns:\n",
    "        epsilon = 1e-9\n",
    "        df['rolling_std_12h_sqrt'] = np.sqrt(df['rolling_std_12h'].clip(lower=0) + epsilon)\n",
    "    else:\n",
    "        df['rolling_std_12h_sqrt'] = np.nan\n",
    "\n",
    "    # Drop intermediate column used for lag calculation\n",
    "    if 'price_return_1h' in df.columns:\n",
    "        df = df.drop(columns=['price_return_1h'])\n",
    "\n",
    "    print(\"  Assembling final dataframe...\")\n",
    "    final_cols_present = [col for col in SELECTED_FEATURE_NAMES if col in df.columns]\n",
    "    df_final = df[final_cols_present + ['timestamp', 'symbol']].copy()\n",
    "    missing_final_cols = set(SELECTED_FEATURE_NAMES) - set(df_final.columns)\n",
    "    if missing_final_cols:\n",
    "        print(f\"  Final Warning: {len(missing_final_cols)} target columns missing: {missing_final_cols}\")\n",
    "    df_final = df_final.reset_index(drop=True)\n",
    "    df_final = df_final.replace([np.inf, -np.inf], np.nan)\n",
    "    end_time = time.time()\n",
    "    actual_feature_count = len([col for col in df_final.columns if col not in ['timestamp', 'symbol', 'open', 'high', 'low', 'close', 'Volume BTC', 'Volume USD']])\n",
    "    print(f\"Selected feature calculation finished. Returning {len(df_final)} rows, {len(df_final.columns)} columns \"\n",
    "          f\"({actual_feature_count} calculated features). Took {end_time - start_time:.2f}s.\")\n",
    "    return df_final\n",
    "\n",
    "# --- New Helper: Grid Search for Base Models ---\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "def grid_search_base_model(model_type, base_param_grid, X, y, scale_pos_weight_val):\n",
    "    \"\"\"Performs a simple 3-fold grid search using F1 score as metric.\n",
    "       Returns the best parameter combination and best mean F1.\n",
    "    \"\"\"\n",
    "    best_score = -np.inf\n",
    "    best_params = None\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    for params in ParameterGrid(base_param_grid):\n",
    "        scores = []\n",
    "        for train_idx, val_idx in cv.split(X, y):\n",
    "            X_train_inner, y_train_inner = X.iloc[train_idx], y.iloc[train_idx]\n",
    "            X_val_inner, y_val_inner = X.iloc[val_idx], y.iloc[val_idx]\n",
    "            try:\n",
    "                if model_type == 'xgb':\n",
    "                    model = XGBClassifier(**params,\n",
    "                                          scale_pos_weight=scale_pos_weight_val,\n",
    "                                          random_state=42,\n",
    "                                          use_label_encoder=False,\n",
    "                                          n_jobs=-1,\n",
    "                                          tree_method='hist')\n",
    "                elif model_type == 'lgbm':\n",
    "                    model = LGBMClassifier(**params,\n",
    "                                           scale_pos_weight=scale_pos_weight_val,\n",
    "                                           random_state=42,\n",
    "                                           n_jobs=-1)\n",
    "                elif model_type == 'svm':\n",
    "                    model = Pipeline([\n",
    "                        ('imputer', SimpleImputer(strategy='median')),\n",
    "                        ('scaler', StandardScaler()),\n",
    "                        ('svm', SVC(kernel='rbf',\n",
    "                                    probability=True,\n",
    "                                    random_state=42,\n",
    "                                    class_weight='balanced',\n",
    "                                    max_iter=5000,\n",
    "                                    **params))\n",
    "                    ])\n",
    "                else:\n",
    "                    continue\n",
    "                model.fit(X_train_inner, y_train_inner)\n",
    "                y_pred_inner = model.predict(X_val_inner)\n",
    "                score = f1_score(y_val_inner, y_pred_inner, zero_division=0)\n",
    "                scores.append(score)\n",
    "            except Exception as e:\n",
    "                scores.append(0)\n",
    "        mean_score = np.mean(scores) if scores else 0\n",
    "        if mean_score > best_score:\n",
    "            best_score = mean_score\n",
    "            best_params = params\n",
    "    return best_params, best_score\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"--- Ensemble_Method_B ---\")\n",
    "    print(\"--- 1. Data Loading & Initial Prep ---\")\n",
    "    try:\n",
    "        print(f\"Loading data from: {CSV_FILE_PATH}\")\n",
    "        col_names = ['unix', 'date', 'symbol_csv', 'open', 'high', 'low', 'close', 'Volume BTC', 'Volume USD']\n",
    "        df_raw = pd.read_csv(CSV_FILE_PATH, header=0, names=col_names)\n",
    "        print(f\"Raw data loaded. Shape: {df_raw.shape}\")\n",
    "        df_raw['timestamp'] = pd.to_datetime(df_raw['date'])\n",
    "        df_raw = df_raw.drop(['unix', 'date', 'symbol_csv'], axis=1)\n",
    "        df_raw = df_raw.sort_values('timestamp').reset_index(drop=True)\n",
    "        if df_raw.empty: exit(\"DataFrame empty after loading. Exiting.\")\n",
    "        print(f\"Initial data prep done. Shape: {df_raw.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or processing CSV: {e}\")\n",
    "        traceback.print_exc()\n",
    "        exit()\n",
    "\n",
    "    print(\"\\n--- 2. Feature Engineering (Simple_Predictor_B Features) ---\")\n",
    "    feature_calc_start = time.time()\n",
    "    df_features = calculate_selected_features(df_raw, symbol=SYMBOL_NAME)\n",
    "    feature_calc_end = time.time()\n",
    "    if df_features.empty: exit(\"Feature calculation failed. Exiting.\")\n",
    "    print(f\"Feature calculation completed in {feature_calc_end - feature_calc_start:.2f} seconds.\")\n",
    "    CURRENT_FEATURE_COLS = [f for f in MODEL_FEATURE_COLS if f in df_features.columns]\n",
    "    if len(CURRENT_FEATURE_COLS) == 0:\n",
    "        exit(\"ERROR: No modeling features found in the DataFrame after calculation.\")\n",
    "    if len(CURRENT_FEATURE_COLS) < len(MODEL_FEATURE_COLS):\n",
    "         print(f\"Warning: Only {len(CURRENT_FEATURE_COLS)} out of {len(MODEL_FEATURE_COLS)} requested modeling features were found/generated.\")\n",
    "    print(f\"Using {len(CURRENT_FEATURE_COLS)} features found in DataFrame for modeling.\")\n",
    "\n",
    "    print(\"\\n--- 3. Data Cleaning (Post-Features) ---\")\n",
    "    numeric_feature_cols = df_features[CURRENT_FEATURE_COLS].select_dtypes(include=np.number).columns.tolist()\n",
    "    df_features[numeric_feature_cols] = df_features[numeric_feature_cols].replace([np.inf, -np.inf], np.nan)\n",
    "    nan_check = df_features[numeric_feature_cols].isnull().sum()\n",
    "    total_nans = nan_check.sum()\n",
    "    print(f\"Total NaNs found in {len(numeric_feature_cols)} numeric feature columns: {total_nans}.\")\n",
    "\n",
    "    print(\"\\n--- 4. Modeling Target & Final Prep ---\")\n",
    "    TARGET_COLUMN = 'target'\n",
    "    df = df_features.copy()\n",
    "    df = df.sort_values('timestamp')\n",
    "    if 'close' not in df.columns: exit(\"ERROR: 'close' column missing before target creation.\")\n",
    "    print(f\"Creating binary target based on {PREDICTION_WINDOW_HOURS}-hour future return >= {TARGET_THRESHOLD_PCT}%...\")\n",
    "    df['future_price'] = df['close'].shift(-PREDICTION_WINDOW_ROWS)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "         df['price_return_future'] = (df['future_price'] - df['close']) / df['close'].replace(0, np.nan) * 100\n",
    "    df[TARGET_COLUMN] = np.where(df['price_return_future'] >= TARGET_THRESHOLD_PCT, 1, 0)\n",
    "    df.loc[df['price_return_future'].isnull(), TARGET_COLUMN] = np.nan\n",
    "    df = df.drop(['future_price', 'price_return_future'], axis=1)\n",
    "    initial_rows = len(df)\n",
    "    essential_check_cols = ['close', TARGET_COLUMN]\n",
    "    df = df.dropna(subset=essential_check_cols)\n",
    "    print(f\"Rows after removing NaN targets/close: {len(df)} (Removed {initial_rows - len(df)})\")\n",
    "    rows_before_feature_nan_check = len(df)\n",
    "    rows_after_feature_nan_dropna = len(df.dropna(subset=CURRENT_FEATURE_COLS))\n",
    "    potential_feature_nan_loss = rows_before_feature_nan_check - rows_after_feature_nan_dropna\n",
    "    if potential_feature_nan_loss > 0:\n",
    "        print(f\"Note: {potential_feature_nan_loss} rows have NaNs in feature columns. Models/Imputer will handle them.\")\n",
    "    if df.empty: exit(\"DataFrame empty after target creation/NaN drop. Exiting.\")\n",
    "    target_counts = df[TARGET_COLUMN].value_counts(normalize=True) * 100\n",
    "    print(\"\\nTarget variable distribution:\")\n",
    "    print(f\"  0 (< {TARGET_THRESHOLD_PCT}% return): {target_counts.get(0, 0):.2f}%\")\n",
    "    print(f\"  1 (>= {TARGET_THRESHOLD_PCT}% return): {target_counts.get(1, 0):.2f}%\")\n",
    "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "    print(f\"Final DataFrame shape for backtesting: {df.shape}\")\n",
    "\n",
    "    print(\"\\n--- 5. Starting Walk-Forward Validation (Stacking Ensemble - Ensemble_Method_B) ---\")\n",
    "    all_metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
    "    all_best_thresholds = []\n",
    "    meta_feature_names = ['xgb_pred', 'lgbm_pred', 'svm_pred']\n",
    "    meta_feature_importances = {meta_feat: [] for meta_feat in meta_feature_names}\n",
    "    iteration_count = 0\n",
    "    n_rows_total = len(df)\n",
    "    current_train_start_idx = 0\n",
    "    total_iterations_estimate = max(0, (n_rows_total - TRAIN_WINDOW_ROWS - TEST_WINDOW_ROWS) // STEP_ROWS + 1) if STEP_ROWS > 0 else 0\n",
    "    print(f\"Total rows: {n_rows_total}, Train Window: {TRAIN_WINDOW_HOURS}h ({TRAIN_WINDOW_ROWS} rows), Prediction Horizon: {PREDICTION_WINDOW_HOURS}h, Evaluation (Test) Window: {TEST_WINDOW_HOURS}h ({TEST_WINDOW_ROWS} rows), Step: {STEP_HOURS}h ({STEP_ROWS} rows)\")\n",
    "    print(f\"Estimated iterations: {total_iterations_estimate}\")\n",
    "    print(f\"Using {len(CURRENT_FEATURE_COLS)} features for modeling.\")\n",
    "    print(f\"Stacking Folds (K): {N_STACKING_FOLDS}\")\n",
    "    print(f\"Meta Learner Grid: {META_XGB_PARAM_GRID}\")\n",
    "    print(f\"Threshold Search Range: {THRESHOLD_SEARCH_RANGE}\")\n",
    "    print(\"-\" * 30)\n",
    "    start_loop_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        train_end_idx = current_train_start_idx + TRAIN_WINDOW_ROWS\n",
    "        test_start_idx = train_end_idx\n",
    "        test_end_idx = test_start_idx + TEST_WINDOW_ROWS\n",
    "        if test_end_idx > n_rows_total:\n",
    "             print(f\"\\nStopping: Evaluation window end ({test_end_idx}) exceeds total rows ({n_rows_total}). Last start index: {current_train_start_idx}\")\n",
    "             break\n",
    "        if current_train_start_idx >= n_rows_total:\n",
    "             print(f\"\\nStopping: Train start index ({current_train_start_idx}) reached end.\")\n",
    "             break\n",
    "\n",
    "        train_df = df.iloc[current_train_start_idx : train_end_idx].copy()\n",
    "        test_df = df.iloc[test_start_idx : test_end_idx].copy()\n",
    "        min_train_samples = max(50, int(0.1 * TRAIN_WINDOW_ROWS), N_STACKING_FOLDS * 2)\n",
    "        min_test_samples = 5\n",
    "        if len(train_df) < min_train_samples or len(test_df) < min_test_samples:\n",
    "            print(f\"Skipping iter {iteration_count + 1}: Insufficient data train ({len(train_df)}/{min_train_samples}) or test ({len(test_df)}/{min_test_samples}). Moving step...\")\n",
    "            current_train_start_idx += STEP_ROWS\n",
    "            continue\n",
    "\n",
    "        X_train_full = train_df[CURRENT_FEATURE_COLS]\n",
    "        y_train_full = train_df[TARGET_COLUMN]\n",
    "        X_test = test_df[CURRENT_FEATURE_COLS]\n",
    "        y_test = test_df[TARGET_COLUMN]\n",
    "\n",
    "        if len(y_train_full.unique()) < 2:\n",
    "            print(f\"Skipping iter {iteration_count + 1}: Training data (size {len(train_df)}) has only one class: {y_train_full.unique()}. Moving step...\")\n",
    "            current_train_start_idx += STEP_ROWS\n",
    "            continue\n",
    "        if len(y_test.unique()) < 2:\n",
    "             print(f\"Warning iter {iteration_count + 1}: Evaluation test data (size {len(test_df)}) has only one class: {y_test.unique()}. Metrics will be affected.\")\n",
    "\n",
    "        neg_count = y_train_full.value_counts().get(0, 0)\n",
    "        pos_count = y_train_full.value_counts().get(1, 0)\n",
    "        scale_pos_weight_val = neg_count / pos_count if pos_count > 0 else 1.0\n",
    "\n",
    "        print(f\"\\n--- Iter {iteration_count + 1}/{total_iterations_estimate} ---\")\n",
    "        print(f\"  Train Indices: [{current_train_start_idx}:{train_end_idx-1}], Eval Indices: [{test_start_idx}:{test_end_idx-1}]\")\n",
    "        print(f\"  Train Target Dist: {dict(y_train_full.value_counts(normalize=True))}\")\n",
    "        print(f\"  Test Target Dist: {dict(y_test.value_counts(normalize=True))}\")\n",
    "        print(f\"  Using scale_pos_weight: {scale_pos_weight_val:.4f}\")\n",
    "\n",
    "        # --- Grid Search for Base Models ---\n",
    "        print(\"  Grid searching base models on current training data...\")\n",
    "        best_xgb_params, xgb_score = grid_search_base_model('xgb', BASE_XGB_PARAM_GRID, X_train_full, y_train_full, scale_pos_weight_val)\n",
    "        best_lgbm_params, lgbm_score = grid_search_base_model('lgbm', BASE_LGBM_PARAM_GRID, X_train_full, y_train_full, scale_pos_weight_val)\n",
    "        best_svm_params, svm_score = grid_search_base_model('svm', BASE_SVM_PARAM_GRID, X_train_full, y_train_full, scale_pos_weight_val)\n",
    "        print(f\"    Best XGB Params: {best_xgb_params} (F1: {xgb_score:.3f})\")\n",
    "        print(f\"    Best LGBM Params: {best_lgbm_params} (F1: {lgbm_score:.3f})\")\n",
    "        print(f\"    Best SVM Params: {best_svm_params} (F1: {svm_score:.3f})\")\n",
    "\n",
    "        # --- Define Base Models using Best Parameters ---\n",
    "        xgb_params_iter = XGB_BASE_PARAMS.copy()\n",
    "        if best_xgb_params is not None:\n",
    "            xgb_params_iter.update(best_xgb_params)\n",
    "        xgb_params_iter['scale_pos_weight'] = scale_pos_weight_val\n",
    "        model_xgb_base = XGBClassifier(**xgb_params_iter)\n",
    "\n",
    "        lgbm_params_iter = LGBM_BASE_PARAMS.copy()\n",
    "        if best_lgbm_params is not None:\n",
    "            lgbm_params_iter.update(best_lgbm_params)\n",
    "        lgbm_params_iter['scale_pos_weight'] = scale_pos_weight_val\n",
    "        model_lgbm_base = LGBMClassifier(**lgbm_params_iter)\n",
    "\n",
    "        svm_params = {}\n",
    "        if best_svm_params is not None:\n",
    "            svm_params.update(best_svm_params)\n",
    "        pipeline_svm_base = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('svm', SVC(kernel='rbf', probability=True, random_state=42, class_weight='balanced', max_iter=5000, **svm_params))\n",
    "        ])\n",
    "        models_oof = {'xgb': model_xgb_base, 'lgbm': model_lgbm_base, 'svm': pipeline_svm_base}\n",
    "        oof_arrays = {'xgb': np.full(len(train_df), np.nan),\n",
    "                      'lgbm': np.full(len(train_df), np.nan),\n",
    "                      'svm': np.full(len(train_df), np.nan)}\n",
    "\n",
    "        # --- K-Fold OOF Generation ---\n",
    "        skf = StratifiedKFold(n_splits=N_STACKING_FOLDS, shuffle=True, random_state=42 + iteration_count)\n",
    "        for fold, (train_idx_k, val_idx_k) in enumerate(skf.split(X_train_full, y_train_full)):\n",
    "            X_train_k, y_train_k = X_train_full.iloc[train_idx_k], y_train_full.iloc[train_idx_k]\n",
    "            X_val_k, y_val_k = X_train_full.iloc[val_idx_k], y_train_full.iloc[val_idx_k]\n",
    "            if len(np.unique(y_train_k)) < 2 or len(np.unique(y_val_k)) < 2:\n",
    "                print(f\"    Warning: Fold {fold+1} has single class in train or val. Assigning prior.\")\n",
    "                prior = y_train_full.mean()\n",
    "                for key in oof_arrays:\n",
    "                    oof_arrays[key][val_idx_k] = prior\n",
    "                continue\n",
    "            for name, model in models_oof.items():\n",
    "                try:\n",
    "                    fit_params_k = {}\n",
    "                    if name == 'lgbm':\n",
    "                        fit_params_k['callbacks'] = [early_stopping(10, verbose=False), log_evaluation(0)]\n",
    "                        fit_params_k['eval_metric'] = 'logloss'\n",
    "                        fit_params_k['eval_set'] = [(X_val_k, y_val_k)]\n",
    "                    elif name == 'xgb':\n",
    "                        fit_params_k['eval_set'] = [(X_val_k, y_val_k)]\n",
    "                        fit_params_k['early_stopping_rounds'] = 10\n",
    "                        fit_params_k['verbose'] = False\n",
    "                    model.fit(X_train_k, y_train_k, **fit_params_k)\n",
    "                    oof_arrays[name][val_idx_k] = model.predict_proba(X_val_k)[:, 1]\n",
    "                except Exception as e_kfold:\n",
    "                    print(f\"    Error during K-Fold {fold+1} for {name}: {e_kfold}\")\n",
    "                    prior = y_train_full.mean()\n",
    "                    oof_arrays[name][val_idx_k] = prior\n",
    "\n",
    "        # Impute any remaining NaNs in OOF arrays.\n",
    "        X_meta_train_dict = {}\n",
    "        for name in models_oof:\n",
    "            oof_array = oof_arrays[name]\n",
    "            if np.isnan(oof_array).any():\n",
    "                mean_oof = np.nanmean(oof_array)\n",
    "                if pd.isna(mean_oof): mean_oof = 0.5\n",
    "                oof_array = np.nan_to_num(oof_array, nan=mean_oof)\n",
    "                print(f\"    Imputed NaNs in OOF for {name} with mean {mean_oof:.4f}\")\n",
    "            X_meta_train_dict[f'{name}_pred'] = oof_array\n",
    "        X_meta_train = pd.DataFrame(X_meta_train_dict, index=X_train_full.index)\n",
    "        y_meta_train = y_train_full\n",
    "        print(f\"  Level 0 OOF Generation Done. Meta Train Shape: {X_meta_train.shape}\")\n",
    "\n",
    "        # --- Train Base Models on Full Training Data ---\n",
    "        print(f\"  Level 0: Training base models on full training data ({len(train_df)} rows)...\")\n",
    "        models_full = {}\n",
    "        all_base_trained = True\n",
    "        for name, model in models_oof.items():\n",
    "            try:\n",
    "                params = {}\n",
    "                if name == 'xgb':\n",
    "                    params['verbose'] = False\n",
    "                model.fit(X_train_full, y_train_full, **params)\n",
    "                models_full[name] = model\n",
    "            except Exception as e_full_fit:\n",
    "                print(f\"  ERROR: Failed to train base model '{name}': {e_full_fit}\")\n",
    "                all_base_trained = False\n",
    "                break\n",
    "        if not all_base_trained:\n",
    "            print(\"  Skipping iteration due to base model training failure. Moving step...\")\n",
    "            current_train_start_idx += STEP_ROWS\n",
    "            continue\n",
    "        print(\"  Level 0 Full Training Done.\")\n",
    "\n",
    "        # --- Level 1: Meta Learner Tuning & Threshold Tuning ---\n",
    "        print(\"  Level 1: Tuning Meta-Learner (XGBoost) and Probability Threshold...\")\n",
    "        best_meta_params = None\n",
    "        best_meta_score = -np.inf\n",
    "        best_meta_model_for_thresh = None\n",
    "        best_threshold_iter = 0.5\n",
    "        best_thresh_f1_score = -np.inf\n",
    "        meta_val_size = int(len(X_meta_train) * META_VALIDATION_PCT)\n",
    "        if meta_val_size < max(N_STACKING_FOLDS, 10) or (len(X_meta_train) - meta_val_size) < max(N_STACKING_FOLDS, 10):\n",
    "            print(f\"  Warning: Meta dataset too small. Using defaults.\")\n",
    "            best_meta_params = list(ParameterGrid(META_XGB_PARAM_GRID))[0] if META_XGB_PARAM_GRID else {}\n",
    "        else:\n",
    "            X_meta_train_sub = X_meta_train[:-meta_val_size]\n",
    "            y_meta_train_sub = y_meta_train[:-meta_val_size]\n",
    "            X_meta_val = X_meta_train[-meta_val_size:]\n",
    "            y_meta_val = y_meta_train[-meta_val_size:]\n",
    "            if len(y_meta_val.unique()) < 2 or len(y_meta_train_sub.unique()) < 2:\n",
    "                print(\"  Warning: Meta split has single class. Using defaults.\")\n",
    "                best_meta_params = list(ParameterGrid(META_XGB_PARAM_GRID))[0] if META_XGB_PARAM_GRID else {}\n",
    "            else:\n",
    "                print(f\"    Tuning meta learner over {len(list(ParameterGrid(META_XGB_PARAM_GRID)))} param combinations...\")\n",
    "                meta_scale_pos_weight_sub = y_meta_train_sub.value_counts().get(0, 0) / y_meta_train_sub.value_counts().get(1, 1) if y_meta_train_sub.value_counts().get(1, 1) > 0 else 1.0\n",
    "                for params_meta_cv in ParameterGrid(META_XGB_PARAM_GRID):\n",
    "                    try:\n",
    "                        current_meta_params = {**META_XGB_FIXED_PARAMS, **params_meta_cv}\n",
    "                        model_meta_cv = XGBClassifier(**current_meta_params, scale_pos_weight=meta_scale_pos_weight_sub)\n",
    "                        model_meta_cv.fit(X_meta_train_sub, y_meta_train_sub,\n",
    "                                          eval_set=[(X_meta_val, y_meta_val)],\n",
    "                                          early_stopping_rounds=10, verbose=False)\n",
    "                        y_pred_meta_val_cv = model_meta_cv.predict(X_meta_val)\n",
    "                        meta_score = f1_score(y_meta_val, y_pred_meta_val_cv, zero_division=0)\n",
    "                        if meta_score >= best_meta_score:\n",
    "                            best_meta_score = meta_score\n",
    "                            best_meta_params = params_meta_cv\n",
    "                            best_meta_model_for_thresh = model_meta_cv\n",
    "                    except Exception as e_meta_cv:\n",
    "                        print(f\"    Error during Meta CV with params {params_meta_cv}: {e_meta_cv}\")\n",
    "                        if best_meta_params is None:\n",
    "                            best_meta_params = list(ParameterGrid(META_XGB_PARAM_GRID))[0] if META_XGB_PARAM_GRID else {}\n",
    "                print(f\"    Best Meta Params Found: {best_meta_params} (Validation F1: {best_meta_score:.4f})\")\n",
    "                if best_meta_model_for_thresh is not None:\n",
    "                    print(f\"    Tuning threshold over range {THRESHOLD_SEARCH_RANGE}...\")\n",
    "                    try:\n",
    "                        y_meta_proba_val = best_meta_model_for_thresh.predict_proba(X_meta_val)[:, 1]\n",
    "                        f1_scores_thresh = {}\n",
    "                        for t in THRESHOLD_SEARCH_RANGE:\n",
    "                            y_pred_meta_val_t = (y_meta_proba_val >= t).astype(int)\n",
    "                            current_f1 = f1_score(y_meta_val, y_pred_meta_val_t, zero_division=0)\n",
    "                            f1_scores_thresh[t] = current_f1\n",
    "                            if current_f1 >= best_thresh_f1_score:\n",
    "                                best_thresh_f1_score = current_f1\n",
    "                                best_threshold_iter = t\n",
    "                        print(f\"    Best Threshold Found: {best_threshold_iter:.2f} (Validation F1: {best_thresh_f1_score:.4f})\")\n",
    "                    except Exception as e_thresh:\n",
    "                        print(f\"    Error during threshold tuning: {e_thresh}. Using default threshold {best_threshold_iter:.2f}.\")\n",
    "                else:\n",
    "                    print(f\"    Skipping threshold tuning. Using default threshold {best_threshold_iter:.2f}.\")\n",
    "\n",
    "        print(\"  Level 1: Training final Meta-Learner on full OOF data...\")\n",
    "        try:\n",
    "            final_meta_params = {**META_XGB_FIXED_PARAMS, **best_meta_params}\n",
    "            final_meta_scale_pos_weight = y_meta_train.value_counts().get(0, 0) / y_meta_train.value_counts().get(1, 1) if y_meta_train.value_counts().get(1, 1) > 0 else 1.0\n",
    "            meta_model_final = XGBClassifier(**final_meta_params, scale_pos_weight=final_meta_scale_pos_weight)\n",
    "            meta_model_final.fit(X_meta_train, y_meta_train, verbose=False)\n",
    "            print(\"  Level 1 Final Meta Training Done.\")\n",
    "        except Exception as e_meta_final:\n",
    "            print(f\"  ERROR: Failed to train final meta-learner: {e_meta_final}\")\n",
    "            current_train_start_idx += STEP_ROWS\n",
    "            continue\n",
    "\n",
    "        print(\"  Prediction: Generating final predictions on evaluation data...\")\n",
    "        try:\n",
    "            pred_xgb_test = models_full['xgb'].predict_proba(X_test)[:, 1]\n",
    "            pred_lgbm_test = models_full['lgbm'].predict_proba(X_test)[:, 1]\n",
    "            pred_svm_test = models_full['svm'].predict_proba(X_test)[:, 1]\n",
    "            X_meta_test = pd.DataFrame({\n",
    "                'xgb_pred': pred_xgb_test,\n",
    "                'lgbm_pred': pred_lgbm_test,\n",
    "                'svm_pred': pred_svm_test\n",
    "            }, index=X_test.index)\n",
    "            y_proba_test = meta_model_final.predict_proba(X_meta_test)[:, 1]\n",
    "            y_pred = (y_proba_test >= best_threshold_iter).astype(int)\n",
    "            print(\"  Prediction Done.\")\n",
    "        except Exception as e_pred:\n",
    "            print(f\"  ERROR during prediction phase: {e_pred}\")\n",
    "            for key in all_metrics: all_metrics[key].append(np.nan)\n",
    "            all_best_thresholds.append(np.nan)\n",
    "            current_train_start_idx += STEP_ROWS\n",
    "            continue\n",
    "\n",
    "        if len(np.unique(y_test)) < 2:\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "            recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "            f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "            print(f\"  Evaluation Metrics (Test Window Size: {TEST_WINDOW_HOURS}h, SINGLE CLASS {y_test.unique()[0]}): Acc={accuracy:.4f}, Prc={precision:.4f}, Rec={recall:.4f}, F1={f1:.4f}\")\n",
    "        else:\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "            recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "            f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "            print(f\"  Evaluation Metrics (Test Window Size: {TEST_WINDOW_HOURS}h): Acc={accuracy:.4f}, Prc={precision:.4f}, Rec={recall:.4f}, F1={f1:.4f}\")\n",
    "\n",
    "        all_metrics['accuracy'].append(accuracy)\n",
    "        all_metrics['precision'].append(precision)\n",
    "        all_metrics['recall'].append(recall)\n",
    "        all_metrics['f1'].append(f1)\n",
    "        all_best_thresholds.append(best_threshold_iter)\n",
    "\n",
    "        # (Meta-feature importance code remains unchanged)\n",
    "\n",
    "        iteration_count += 1\n",
    "        iter_end_time = time.time()\n",
    "        print(f\"  Iteration {iteration_count} finished in {iter_end_time - iter_start_time:.2f} seconds.\")\n",
    "        print(\"-\" * 20)\n",
    "        current_train_start_idx += STEP_ROWS\n",
    "\n",
    "    end_loop_time = time.time()\n",
    "    loop_duration_minutes = (end_loop_time - start_loop_time) / 60\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Walk-Forward Validation (Stacking - Ensemble_Method_B) finished in {end_loop_time - start_loop_time:.2f} seconds ({loop_duration_minutes:.2f} minutes).\")\n",
    "\n",
    "    # --- Aggregate and Display Results (unchanged) ---\n",
    "    print(\"\\n--- Final Results (Ensemble_Method_B) ---\")\n",
    "    if iteration_count > 0 and len(all_metrics['f1']) > 0:\n",
    "        valid_indices = [i for i, f1 in enumerate(all_metrics['f1']) if not pd.isna(f1)]\n",
    "        if valid_indices:\n",
    "            valid_accuracy = [all_metrics['accuracy'][i] for i in valid_indices]\n",
    "            valid_precision = [all_metrics['precision'][i] for i in valid_indices]\n",
    "            valid_recall = [all_metrics['recall'][i] for i in valid_indices]\n",
    "            valid_f1 = [all_metrics['f1'][i] for i in valid_indices]\n",
    "            valid_thresholds = [all_best_thresholds[i] for i in valid_indices if not pd.isna(all_best_thresholds[i])]\n",
    "            avg_accuracy = np.mean(valid_accuracy)\n",
    "            avg_precision = np.mean(valid_precision)\n",
    "            avg_recall = np.mean(valid_recall)\n",
    "            avg_f1 = np.mean(valid_f1)\n",
    "            print(\"\\n--- Average Walk-Forward Validation Results ---\")\n",
    "            print(f\"Total Iterations Run: {iteration_count}, Successful Evaluations: {len(valid_indices)}\")\n",
    "            print(f\"Target: >= {TARGET_THRESHOLD_PCT}% increase over {PREDICTION_WINDOW_HOURS} hours (Prediction Horizon)\")\n",
    "            print(f\"Train Window: {TRAIN_WINDOW_HOURS} hours, Evaluation Window: {TEST_WINDOW_HOURS} hours, Step: {STEP_HOURS} hours\")\n",
    "            print(f\"Stacking Folds: {N_STACKING_FOLDS}\")\n",
    "            print(f\"Average Accuracy:  {avg_accuracy:.4f}\")\n",
    "            print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "            print(f\"Average Recall:    {avg_recall:.4f}\")\n",
    "            print(f\"Average F1-Score:  {avg_f1:.4f}\")\n",
    "            std_accuracy = np.std(valid_accuracy)\n",
    "            std_precision = np.std(valid_precision)\n",
    "            std_recall = np.std(valid_recall)\n",
    "            std_f1 = np.std(valid_f1)\n",
    "            print(\"\\n--- Standard Deviation of Metrics ---\")\n",
    "            print(f\"Std Dev Accuracy:  {std_accuracy:.4f}\")\n",
    "            print(f\"Std Dev Precision: {std_precision:.4f}\")\n",
    "            print(f\"Std Dev Recall:    {std_recall:.4f}\")\n",
    "            print(f\"Std Dev F1-Score:  {std_f1:.4f}\")\n",
    "            if valid_thresholds:\n",
    "                avg_threshold = np.mean(valid_thresholds)\n",
    "                std_threshold = np.std(valid_thresholds)\n",
    "                print(f\"\\nAverage Best Threshold: {avg_threshold:.3f} (StdDev: {std_threshold:.3f}) over {len(valid_thresholds)} folds\")\n",
    "            else:\n",
    "                print(\"\\nCould not determine average threshold.\")\n",
    "        else:\n",
    "            print(\"\\nNo valid metrics recorded.\")\n",
    "    else:\n",
    "        print(\"\\nNo iterations were successfully completed or no metrics were generated.\")\n",
    "\n",
    "    print(\"\\nScript Ensemble_Method_B finished.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
