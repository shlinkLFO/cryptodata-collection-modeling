{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Loading ---\n",
      "Loading data from: BTCUSD.csv\n",
      "Selected last 15000 rows.\n",
      "\n",
      "--- Feature Engineering ---\n",
      "Feature engineering complete. Took 0.02 seconds.\n",
      "Columns after features: 51\n",
      "\n",
      "--- Target Definition ---\n",
      "Defining target as 24h future return >= 2.5%...\n",
      "\n",
      "--- Data Preparation ---\n",
      "NaN Handling: Dropped 197 rows.\n",
      "Replacing 8 infinites...\n",
      "Dropped 8 more rows.\n",
      "Final feature matrix shape: (14795, 42)\n",
      "Target vector shape: (14795,)\n",
      "Using 42 features.\n",
      "\n",
      "--- Starting SLIDING Window Backtest with Per-Step HParam Tuning ---\n",
      "!!! WARNING: This will be significantly slower due to GridSearchCV in each step !!!\n",
      "Train Window: 1344 rows, Step: 24 rows, Test Window: 268 rows, Tuning Grid Size: 48\n",
      "\n",
      "--- Step 1 (Predicting for window starting 2023-09-21 02:00:00) ---\n",
      "  Training window: [0:1343]; Testing window: [1344:1611]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 5.10s.\n",
      "  Best Params: {'colsample_bytree': 0.72, 'max_depth': 7, 'n_estimators': 147, 'reg_alpha': 0.26, 'subsample': 0.92}, Best CV F1: 0.8034\n",
      "  Step 1 finished in 5.20s total.\n",
      "\n",
      "--- Step 2 (Predicting for window starting 2023-09-22 02:00:00) ---\n",
      "  Training window: [24:1367]; Testing window: [1368:1635]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.12s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 7, 'n_estimators': 147, 'reg_alpha': 0.26, 'subsample': 0.92}, Best CV F1: 0.7817\n",
      "  Step 2 finished in 2.22s total.\n",
      "\n",
      "--- Step 3 (Predicting for window starting 2023-09-23 02:00:00) ---\n",
      "  Training window: [48:1391]; Testing window: [1392:1659]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.05s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 112, 'reg_alpha': 0.26, 'subsample': 0.92}, Best CV F1: 0.8117\n",
      "  Step 3 finished in 2.12s total.\n",
      "\n",
      "--- Step 4 (Predicting for window starting 2023-09-24 02:00:00) ---\n",
      "  Training window: [72:1415]; Testing window: [1416:1683]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.07s.\n",
      "  Best Params: {'colsample_bytree': 0.72, 'max_depth': 6, 'n_estimators': 147, 'reg_alpha': 0.14, 'subsample': 0.83}, Best CV F1: 0.7285\n",
      "  Step 4 finished in 2.16s total.\n",
      "\n",
      "--- Step 5 (Predicting for window starting 2023-09-25 02:00:00) ---\n",
      "  Training window: [96:1439]; Testing window: [1440:1707]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.03s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 9, 'n_estimators': 147, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.6876\n",
      "  Step 5 finished in 2.11s total.\n",
      "\n",
      "--- Step 6 (Predicting for window starting 2023-09-26 02:00:00) ---\n",
      "  Training window: [120:1463]; Testing window: [1464:1731]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.03s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 112, 'reg_alpha': 0.14, 'subsample': 0.83}, Best CV F1: 0.7438\n",
      "  Step 6 finished in 2.09s total.\n",
      "\n",
      "--- Step 7 (Predicting for window starting 2023-09-27 02:00:00) ---\n",
      "  Training window: [144:1487]; Testing window: [1488:1755]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 1.98s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 112, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.7890\n",
      "  Step 7 finished in 2.05s total.\n",
      "\n",
      "--- Step 8 (Predicting for window starting 2023-09-28 02:00:00) ---\n",
      "  Training window: [168:1511]; Testing window: [1512:1779]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.17s.\n",
      "  Best Params: {'colsample_bytree': 0.72, 'max_depth': 7, 'n_estimators': 147, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8363\n",
      "  Step 8 finished in 2.26s total.\n",
      "\n",
      "--- Step 9 (Predicting for window starting 2023-09-29 02:00:00) ---\n",
      "  Training window: [192:1535]; Testing window: [1536:1803]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.19s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 147, 'reg_alpha': 0.26, 'subsample': 0.92}, Best CV F1: 0.7966\n",
      "  Step 9 finished in 2.27s total.\n",
      "\n",
      "--- Step 10 (Predicting for window starting 2023-09-30 02:00:00) ---\n",
      "  Training window: [216:1559]; Testing window: [1560:1827]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.17s.\n",
      "  Best Params: {'colsample_bytree': 0.72, 'max_depth': 9, 'n_estimators': 112, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.7947\n",
      "  Step 10 finished in 2.25s total.\n",
      "\n",
      "--- Step 11 (Predicting for window starting 2023-10-01 02:00:00) ---\n",
      "  Training window: [240:1583]; Testing window: [1584:1851]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.23s.\n",
      "  Best Params: {'colsample_bytree': 0.72, 'max_depth': 9, 'n_estimators': 147, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8259\n",
      "  Step 11 finished in 2.34s total.\n",
      "\n",
      "--- Step 12 (Predicting for window starting 2023-10-02 02:00:00) ---\n",
      "  Training window: [264:1607]; Testing window: [1608:1875]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.18s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 112, 'reg_alpha': 0.14, 'subsample': 0.83}, Best CV F1: 0.7884\n",
      "  Step 12 finished in 2.24s total.\n",
      "\n",
      "--- Step 13 (Predicting for window starting 2023-10-03 02:00:00) ---\n",
      "  Training window: [288:1631]; Testing window: [1632:1899]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.12s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 112, 'reg_alpha': 0.26, 'subsample': 0.92}, Best CV F1: 0.8831\n",
      "  Step 13 finished in 2.19s total.\n",
      "\n",
      "--- Step 14 (Predicting for window starting 2023-10-04 02:00:00) ---\n",
      "  Training window: [312:1655]; Testing window: [1656:1923]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.11s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 112, 'reg_alpha': 0.26, 'subsample': 0.92}, Best CV F1: 0.8350\n",
      "  Step 14 finished in 2.18s total.\n",
      "\n",
      "--- Step 15 (Predicting for window starting 2023-10-05 02:00:00) ---\n",
      "  Training window: [336:1679]; Testing window: [1680:1947]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.08s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 112, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8778\n",
      "  Step 15 finished in 2.15s total.\n",
      "\n",
      "--- Step 16 (Predicting for window starting 2023-10-06 02:00:00) ---\n",
      "  Training window: [360:1703]; Testing window: [1704:1971]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.11s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 112, 'reg_alpha': 0.26, 'subsample': 0.92}, Best CV F1: 0.8379\n",
      "  Step 16 finished in 2.18s total.\n",
      "\n",
      "--- Step 17 (Predicting for window starting 2023-10-07 02:00:00) ---\n",
      "  Training window: [384:1727]; Testing window: [1728:1995]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.09s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 112, 'reg_alpha': 0.26, 'subsample': 0.92}, Best CV F1: 0.8720\n",
      "  Step 17 finished in 2.16s total.\n",
      "\n",
      "--- Step 18 (Predicting for window starting 2023-10-08 02:00:00) ---\n",
      "  Training window: [408:1751]; Testing window: [1752:2019]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.15s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 147, 'reg_alpha': 0.26, 'subsample': 0.92}, Best CV F1: 0.8507\n",
      "  Step 18 finished in 2.25s total.\n",
      "\n",
      "--- Step 19 (Predicting for window starting 2023-10-09 02:00:00) ---\n",
      "  Training window: [432:1775]; Testing window: [1776:2043]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.03s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 112, 'reg_alpha': 0.26, 'subsample': 0.92}, Best CV F1: 0.8473\n",
      "  Step 19 finished in 2.10s total.\n",
      "\n",
      "--- Step 20 (Predicting for window starting 2023-10-10 02:00:00) ---\n",
      "  Training window: [456:1799]; Testing window: [1800:2067]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.08s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 112, 'reg_alpha': 0.14, 'subsample': 0.83}, Best CV F1: 0.8555\n",
      "  Step 20 finished in 2.15s total.\n",
      "\n",
      "--- Step 21 (Predicting for window starting 2023-10-11 02:00:00) ---\n",
      "  Training window: [480:1823]; Testing window: [1824:2091]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.18s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 112, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8442\n",
      "  Step 21 finished in 2.26s total.\n",
      "\n",
      "--- Step 22 (Predicting for window starting 2023-10-12 02:00:00) ---\n",
      "  Training window: [504:1847]; Testing window: [1848:2115]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.29s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 7, 'n_estimators': 147, 'reg_alpha': 0.14, 'subsample': 0.83}, Best CV F1: 0.8557\n",
      "  Step 22 finished in 2.38s total.\n",
      "\n",
      "--- Step 23 (Predicting for window starting 2023-10-13 02:00:00) ---\n",
      "  Training window: [528:1871]; Testing window: [1872:2139]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.03s.\n",
      "  Best Params: {'colsample_bytree': 0.72, 'max_depth': 7, 'n_estimators': 112, 'reg_alpha': 0.26, 'subsample': 0.92}, Best CV F1: 0.8700\n",
      "  Step 23 finished in 2.11s total.\n",
      "\n",
      "--- Step 24 (Predicting for window starting 2023-10-14 02:00:00) ---\n",
      "  Training window: [552:1895]; Testing window: [1896:2163]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.13s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 112, 'reg_alpha': 0.14, 'subsample': 0.83}, Best CV F1: 0.8610\n",
      "  Step 24 finished in 2.21s total.\n",
      "\n",
      "--- Step 25 (Predicting for window starting 2023-10-15 02:00:00) ---\n",
      "  Training window: [576:1919]; Testing window: [1920:2187]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.18s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 112, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8660\n",
      "  Step 25 finished in 2.26s total.\n",
      "\n",
      "--- Step 26 (Predicting for window starting 2023-10-16 02:00:00) ---\n",
      "  Training window: [600:1943]; Testing window: [1944:2211]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.19s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 112, 'reg_alpha': 0.26, 'subsample': 0.83}, Best CV F1: 0.8647\n",
      "  Step 26 finished in 2.25s total.\n",
      "\n",
      "--- Step 27 (Predicting for window starting 2023-10-17 02:00:00) ---\n",
      "  Training window: [624:1967]; Testing window: [1968:2235]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.22s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 112, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.9002\n",
      "  Step 27 finished in 2.30s total.\n",
      "\n",
      "--- Step 28 (Predicting for window starting 2023-10-18 02:00:00) ---\n",
      "  Training window: [648:1991]; Testing window: [1992:2259]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.03s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 112, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.9034\n",
      "  Step 28 finished in 2.10s total.\n",
      "\n",
      "--- Step 29 (Predicting for window starting 2023-10-19 02:00:00) ---\n",
      "  Training window: [672:2015]; Testing window: [2016:2283]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.02s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 112, 'reg_alpha': 0.26, 'subsample': 0.83}, Best CV F1: 0.9229\n",
      "  Step 29 finished in 2.10s total.\n",
      "\n",
      "--- Step 30 (Predicting for window starting 2023-10-20 02:00:00) ---\n",
      "  Training window: [696:2039]; Testing window: [2040:2307]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.39s.\n",
      "  Best Params: {'colsample_bytree': 0.72, 'max_depth': 6, 'n_estimators': 147, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.9060\n",
      "  Step 30 finished in 2.48s total.\n",
      "\n",
      "--- Step 31 (Predicting for window starting 2023-10-21 02:00:00) ---\n",
      "  Training window: [720:2063]; Testing window: [2064:2331]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.40s.\n",
      "  Best Params: {'colsample_bytree': 0.72, 'max_depth': 6, 'n_estimators': 112, 'reg_alpha': 0.26, 'subsample': 0.92}, Best CV F1: 0.9263\n",
      "  Step 31 finished in 2.48s total.\n",
      "\n",
      "--- Step 32 (Predicting for window starting 2023-10-22 02:00:00) ---\n",
      "  Training window: [744:2087]; Testing window: [2088:2355]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.36s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 7, 'n_estimators': 112, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.9131\n",
      "  Step 32 finished in 2.44s total.\n",
      "\n",
      "--- Step 33 (Predicting for window starting 2023-10-23 02:00:00) ---\n",
      "  Training window: [768:2111]; Testing window: [2112:2379]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.48s.\n",
      "  Best Params: {'colsample_bytree': 0.72, 'max_depth': 9, 'n_estimators': 112, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8907\n",
      "  Step 33 finished in 2.57s total.\n",
      "\n",
      "--- Step 34 (Predicting for window starting 2023-10-24 02:00:00) ---\n",
      "  Training window: [792:2135]; Testing window: [2136:2403]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.65s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 7, 'n_estimators': 112, 'reg_alpha': 0.26, 'subsample': 0.83}, Best CV F1: 0.9063\n",
      "  Step 34 finished in 2.73s total.\n",
      "\n",
      "--- Step 35 (Predicting for window starting 2023-10-25 02:00:00) ---\n",
      "  Training window: [816:2159]; Testing window: [2160:2427]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.69s.\n",
      "  Best Params: {'colsample_bytree': 0.72, 'max_depth': 6, 'n_estimators': 112, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.9025\n",
      "  Step 35 finished in 2.79s total.\n",
      "\n",
      "--- Step 36 (Predicting for window starting 2023-10-26 02:00:00) ---\n",
      "  Training window: [840:2183]; Testing window: [2184:2451]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.65s.\n",
      "  Best Params: {'colsample_bytree': 0.72, 'max_depth': 9, 'n_estimators': 112, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.9005\n",
      "  Step 36 finished in 2.73s total.\n",
      "\n",
      "--- Step 37 (Predicting for window starting 2023-10-27 02:00:00) ---\n",
      "  Training window: [864:2207]; Testing window: [2208:2475]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.66s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 147, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8910\n",
      "  Step 37 finished in 2.75s total.\n",
      "\n",
      "--- Step 38 (Predicting for window starting 2023-10-28 02:00:00) ---\n",
      "  Training window: [888:2231]; Testing window: [2232:2499]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.64s.\n",
      "  Best Params: {'colsample_bytree': 0.72, 'max_depth': 6, 'n_estimators': 112, 'reg_alpha': 0.26, 'subsample': 0.92}, Best CV F1: 0.8735\n",
      "  Step 38 finished in 2.72s total.\n",
      "\n",
      "--- Step 39 (Predicting for window starting 2023-10-29 02:00:00) ---\n",
      "  Training window: [912:2255]; Testing window: [2256:2523]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.69s.\n",
      "  Best Params: {'colsample_bytree': 0.72, 'max_depth': 6, 'n_estimators': 112, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8956\n",
      "  Step 39 finished in 2.76s total.\n",
      "\n",
      "--- Step 40 (Predicting for window starting 2023-10-30 02:00:00) ---\n",
      "  Training window: [936:2279]; Testing window: [2280:2547]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.72s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 7, 'n_estimators': 147, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8858\n",
      "  Step 40 finished in 2.85s total.\n",
      "\n",
      "--- Step 41 (Predicting for window starting 2023-10-31 02:00:00) ---\n",
      "  Training window: [960:2303]; Testing window: [2304:2571]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.71s.\n",
      "  Best Params: {'colsample_bytree': 0.72, 'max_depth': 7, 'n_estimators': 112, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.9031\n",
      "  Step 41 finished in 2.79s total.\n",
      "\n",
      "--- Step 42 (Predicting for window starting 2023-11-01 02:00:00) ---\n",
      "  Training window: [984:2327]; Testing window: [2328:2595]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.76s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 7, 'n_estimators': 112, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8942\n",
      "  Step 42 finished in 2.86s total.\n",
      "\n",
      "--- Step 43 (Predicting for window starting 2023-11-02 02:00:00) ---\n",
      "  Training window: [1008:2351]; Testing window: [2352:2619]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.86s.\n",
      "  Best Params: {'colsample_bytree': 0.72, 'max_depth': 7, 'n_estimators': 147, 'reg_alpha': 0.14, 'subsample': 0.83}, Best CV F1: 0.8700\n",
      "  Step 43 finished in 2.99s total.\n",
      "\n",
      "--- Step 44 (Predicting for window starting 2023-11-03 02:00:00) ---\n",
      "  Training window: [1032:2375]; Testing window: [2376:2643]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.79s.\n",
      "  Best Params: {'colsample_bytree': 0.72, 'max_depth': 6, 'n_estimators': 112, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8766\n",
      "  Step 44 finished in 2.87s total.\n",
      "\n",
      "--- Step 45 (Predicting for window starting 2023-11-04 02:00:00) ---\n",
      "  Training window: [1056:2399]; Testing window: [2400:2667]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.86s.\n",
      "  Best Params: {'colsample_bytree': 0.72, 'max_depth': 7, 'n_estimators': 147, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8895\n",
      "  Step 45 finished in 2.96s total.\n",
      "\n",
      "--- Step 46 (Predicting for window starting 2023-11-05 02:00:00) ---\n",
      "  Training window: [1080:2423]; Testing window: [2424:2691]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.80s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 9, 'n_estimators': 112, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8897\n",
      "  Step 46 finished in 2.88s total.\n",
      "\n",
      "--- Step 47 (Predicting for window starting 2023-11-06 02:00:00) ---\n",
      "  Training window: [1104:2447]; Testing window: [2448:2715]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.82s.\n",
      "  Best Params: {'colsample_bytree': 0.72, 'max_depth': 6, 'n_estimators': 147, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8063\n",
      "  Step 47 finished in 2.92s total.\n",
      "\n",
      "--- Step 48 (Predicting for window starting 2023-11-07 02:00:00) ---\n",
      "  Training window: [1128:2471]; Testing window: [2472:2739]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.69s.\n",
      "  Best Params: {'colsample_bytree': 0.72, 'max_depth': 9, 'n_estimators': 112, 'reg_alpha': 0.26, 'subsample': 0.92}, Best CV F1: 0.8658\n",
      "  Step 48 finished in 2.79s total.\n",
      "\n",
      "--- Step 49 (Predicting for window starting 2023-11-08 02:00:00) ---\n",
      "  Training window: [1152:2495]; Testing window: [2496:2763]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.72s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 9, 'n_estimators': 112, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8836\n",
      "  Step 49 finished in 2.83s total.\n",
      "\n",
      "--- Step 50 (Predicting for window starting 2023-11-09 02:00:00) ---\n",
      "  Training window: [1176:2519]; Testing window: [2520:2787]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.87s.\n",
      "  Best Params: {'colsample_bytree': 0.72, 'max_depth': 7, 'n_estimators': 112, 'reg_alpha': 0.14, 'subsample': 0.83}, Best CV F1: 0.8844\n",
      "  Step 50 finished in 2.95s total.\n",
      "\n",
      "--- Step 51 (Predicting for window starting 2023-11-10 02:00:00) ---\n",
      "  Training window: [1200:2543]; Testing window: [2544:2811]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.97s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 7, 'n_estimators': 147, 'reg_alpha': 0.26, 'subsample': 0.92}, Best CV F1: 0.8777\n",
      "  Step 51 finished in 3.10s total.\n",
      "\n",
      "--- Step 52 (Predicting for window starting 2023-11-11 02:00:00) ---\n",
      "  Training window: [1224:2567]; Testing window: [2568:2835]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.93s.\n",
      "  Best Params: {'colsample_bytree': 0.72, 'max_depth': 9, 'n_estimators': 112, 'reg_alpha': 0.26, 'subsample': 0.83}, Best CV F1: 0.8584\n",
      "  Step 52 finished in 3.02s total.\n",
      "\n",
      "--- Step 53 (Predicting for window starting 2023-11-12 02:00:00) ---\n",
      "  Training window: [1248:2591]; Testing window: [2592:2859]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.95s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 7, 'n_estimators': 112, 'reg_alpha': 0.26, 'subsample': 0.92}, Best CV F1: 0.8619\n",
      "  Step 53 finished in 3.05s total.\n",
      "\n",
      "--- Step 54 (Predicting for window starting 2023-11-13 02:00:00) ---\n",
      "  Training window: [1272:2615]; Testing window: [2616:2883]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.83s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 112, 'reg_alpha': 0.26, 'subsample': 0.92}, Best CV F1: 0.8740\n",
      "  Step 54 finished in 2.93s total.\n",
      "\n",
      "--- Step 55 (Predicting for window starting 2023-11-14 02:00:00) ---\n",
      "  Training window: [1296:2639]; Testing window: [2640:2907]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.86s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 147, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8109\n",
      "  Step 55 finished in 2.96s total.\n",
      "\n",
      "--- Step 56 (Predicting for window starting 2023-11-15 02:00:00) ---\n",
      "  Training window: [1320:2663]; Testing window: [2664:2931]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.98s.\n",
      "  Best Params: {'colsample_bytree': 0.72, 'max_depth': 6, 'n_estimators': 147, 'reg_alpha': 0.26, 'subsample': 0.92}, Best CV F1: 0.8583\n",
      "  Step 56 finished in 3.09s total.\n",
      "\n",
      "--- Step 57 (Predicting for window starting 2023-11-16 02:00:00) ---\n",
      "  Training window: [1344:2687]; Testing window: [2688:2955]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.99s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 147, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8922\n",
      "  Step 57 finished in 3.09s total.\n",
      "\n",
      "--- Step 58 (Predicting for window starting 2023-11-17 02:00:00) ---\n",
      "  Training window: [1368:2711]; Testing window: [2712:2979]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.99s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 147, 'reg_alpha': 0.26, 'subsample': 0.92}, Best CV F1: 0.8554\n",
      "  Step 58 finished in 3.09s total.\n",
      "\n",
      "--- Step 59 (Predicting for window starting 2023-11-18 02:00:00) ---\n",
      "  Training window: [1392:2735]; Testing window: [2736:3003]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 2.99s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 147, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8397\n",
      "  Step 59 finished in 3.08s total.\n",
      "\n",
      "--- Step 60 (Predicting for window starting 2023-11-19 02:00:00) ---\n",
      "  Training window: [1416:2759]; Testing window: [2760:3027]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.02s.\n",
      "  Best Params: {'colsample_bytree': 0.72, 'max_depth': 7, 'n_estimators': 112, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8526\n",
      "  Step 60 finished in 3.11s total.\n",
      "\n",
      "--- Step 61 (Predicting for window starting 2023-11-20 02:00:00) ---\n",
      "  Training window: [1440:2783]; Testing window: [2784:3051]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.07s.\n",
      "  Best Params: {'colsample_bytree': 0.72, 'max_depth': 7, 'n_estimators': 147, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8918\n",
      "  Step 61 finished in 3.18s total.\n",
      "\n",
      "--- Step 62 (Predicting for window starting 2023-11-21 02:00:00) ---\n",
      "  Training window: [1464:2807]; Testing window: [2808:3075]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.16s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 112, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8809\n",
      "  Step 62 finished in 3.26s total.\n",
      "\n",
      "--- Step 63 (Predicting for window starting 2023-11-22 02:00:00) ---\n",
      "  Training window: [1488:2831]; Testing window: [2832:3099]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.18s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 7, 'n_estimators': 147, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8718\n",
      "  Step 63 finished in 3.30s total.\n",
      "\n",
      "--- Step 64 (Predicting for window starting 2023-11-23 02:00:00) ---\n",
      "  Training window: [1512:2855]; Testing window: [2856:3123]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.11s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 147, 'reg_alpha': 0.26, 'subsample': 0.92}, Best CV F1: 0.8535\n",
      "  Step 64 finished in 3.22s total.\n",
      "\n",
      "--- Step 65 (Predicting for window starting 2023-11-24 02:00:00) ---\n",
      "  Training window: [1536:2879]; Testing window: [2880:3147]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.18s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 147, 'reg_alpha': 0.26, 'subsample': 0.92}, Best CV F1: 0.8720\n",
      "  Step 65 finished in 3.29s total.\n",
      "\n",
      "--- Step 66 (Predicting for window starting 2023-11-25 02:00:00) ---\n",
      "  Training window: [1560:2903]; Testing window: [2904:3171]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.10s.\n",
      "  Best Params: {'colsample_bytree': 0.72, 'max_depth': 7, 'n_estimators': 147, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8594\n",
      "  Step 66 finished in 3.23s total.\n",
      "\n",
      "--- Step 67 (Predicting for window starting 2023-11-26 02:00:00) ---\n",
      "  Training window: [1584:2927]; Testing window: [2928:3195]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.09s.\n",
      "  Best Params: {'colsample_bytree': 0.72, 'max_depth': 6, 'n_estimators': 147, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8680\n",
      "  Step 67 finished in 3.21s total.\n",
      "\n",
      "--- Step 68 (Predicting for window starting 2023-11-27 02:00:00) ---\n",
      "  Training window: [1608:2951]; Testing window: [2952:3219]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.04s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 7, 'n_estimators': 112, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8496\n",
      "  Step 68 finished in 3.12s total.\n",
      "\n",
      "--- Step 69 (Predicting for window starting 2023-11-28 02:00:00) ---\n",
      "  Training window: [1632:2975]; Testing window: [2976:3243]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.08s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 147, 'reg_alpha': 0.26, 'subsample': 0.92}, Best CV F1: 0.8343\n",
      "  Step 69 finished in 3.19s total.\n",
      "\n",
      "--- Step 70 (Predicting for window starting 2023-11-29 02:00:00) ---\n",
      "  Training window: [1656:2999]; Testing window: [3000:3267]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.18s.\n",
      "  Best Params: {'colsample_bytree': 0.72, 'max_depth': 6, 'n_estimators': 147, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8326\n",
      "  Step 70 finished in 3.29s total.\n",
      "\n",
      "--- Step 71 (Predicting for window starting 2023-11-30 02:00:00) ---\n",
      "  Training window: [1680:3023]; Testing window: [3024:3291]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.15s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 147, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8440\n",
      "  Step 71 finished in 3.28s total.\n",
      "\n",
      "--- Step 72 (Predicting for window starting 2023-12-01 02:00:00) ---\n",
      "  Training window: [1704:3047]; Testing window: [3048:3315]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.26s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 9, 'n_estimators': 147, 'reg_alpha': 0.26, 'subsample': 0.92}, Best CV F1: 0.8284\n",
      "  Step 72 finished in 3.39s total.\n",
      "\n",
      "--- Step 73 (Predicting for window starting 2023-12-02 02:00:00) ---\n",
      "  Training window: [1728:3071]; Testing window: [3072:3339]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.19s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 147, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8473\n",
      "  Step 73 finished in 3.29s total.\n",
      "\n",
      "--- Step 74 (Predicting for window starting 2023-12-03 02:00:00) ---\n",
      "  Training window: [1752:3095]; Testing window: [3096:3363]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.20s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 7, 'n_estimators': 112, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8211\n",
      "  Step 74 finished in 3.29s total.\n",
      "\n",
      "--- Step 75 (Predicting for window starting 2023-12-04 02:00:00) ---\n",
      "  Training window: [1776:3119]; Testing window: [3120:3387]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.28s.\n",
      "  Best Params: {'colsample_bytree': 0.72, 'max_depth': 7, 'n_estimators': 112, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8440\n",
      "  Step 75 finished in 3.40s total.\n",
      "\n",
      "--- Step 76 (Predicting for window starting 2023-12-05 02:00:00) ---\n",
      "  Training window: [1800:3143]; Testing window: [3144:3411]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.29s.\n",
      "  Best Params: {'colsample_bytree': 0.72, 'max_depth': 6, 'n_estimators': 147, 'reg_alpha': 0.26, 'subsample': 0.92}, Best CV F1: 0.8518\n",
      "  Step 76 finished in 3.40s total.\n",
      "\n",
      "--- Step 77 (Predicting for window starting 2023-12-06 02:00:00) ---\n",
      "  Training window: [1824:3167]; Testing window: [3168:3435]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.38s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 7, 'n_estimators': 112, 'reg_alpha': 0.26, 'subsample': 0.92}, Best CV F1: 0.8750\n",
      "  Step 77 finished in 3.47s total.\n",
      "\n",
      "--- Step 78 (Predicting for window starting 2023-12-07 02:00:00) ---\n",
      "  Training window: [1848:3191]; Testing window: [3192:3459]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.29s.\n",
      "  Best Params: {'colsample_bytree': 0.72, 'max_depth': 6, 'n_estimators': 147, 'reg_alpha': 0.26, 'subsample': 0.83}, Best CV F1: 0.8693\n",
      "  Step 78 finished in 3.40s total.\n",
      "\n",
      "--- Step 79 (Predicting for window starting 2023-12-08 02:00:00) ---\n",
      "  Training window: [1872:3215]; Testing window: [3216:3483]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.33s.\n",
      "  Best Params: {'colsample_bytree': 0.72, 'max_depth': 9, 'n_estimators': 147, 'reg_alpha': 0.26, 'subsample': 0.92}, Best CV F1: 0.8194\n",
      "  Step 79 finished in 3.47s total.\n",
      "\n",
      "--- Step 80 (Predicting for window starting 2023-12-09 02:00:00) ---\n",
      "  Training window: [1896:3239]; Testing window: [3240:3507]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.32s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 9, 'n_estimators': 147, 'reg_alpha': 0.26, 'subsample': 0.92}, Best CV F1: 0.8386\n",
      "  Step 80 finished in 3.47s total.\n",
      "\n",
      "--- Step 81 (Predicting for window starting 2023-12-10 02:00:00) ---\n",
      "  Training window: [1920:3263]; Testing window: [3264:3531]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.37s.\n",
      "  Best Params: {'colsample_bytree': 0.72, 'max_depth': 6, 'n_estimators': 147, 'reg_alpha': 0.26, 'subsample': 0.92}, Best CV F1: 0.8390\n",
      "  Step 81 finished in 3.48s total.\n",
      "\n",
      "--- Step 82 (Predicting for window starting 2023-12-11 02:00:00) ---\n",
      "  Training window: [1944:3287]; Testing window: [3288:3555]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.25s.\n",
      "  Best Params: {'colsample_bytree': 0.72, 'max_depth': 7, 'n_estimators': 147, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8461\n",
      "  Step 82 finished in 3.37s total.\n",
      "\n",
      "--- Step 83 (Predicting for window starting 2023-12-12 02:00:00) ---\n",
      "  Training window: [1968:3311]; Testing window: [3312:3579]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.35s.\n",
      "  Best Params: {'colsample_bytree': 0.72, 'max_depth': 6, 'n_estimators': 147, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8787\n",
      "  Step 83 finished in 3.48s total.\n",
      "\n",
      "--- Step 84 (Predicting for window starting 2023-12-13 02:00:00) ---\n",
      "  Training window: [1992:3335]; Testing window: [3336:3603]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.28s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 147, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8511\n",
      "  Step 84 finished in 3.39s total.\n",
      "\n",
      "--- Step 85 (Predicting for window starting 2023-12-14 02:00:00) ---\n",
      "  Training window: [2016:3359]; Testing window: [3360:3627]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.18s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 147, 'reg_alpha': 0.26, 'subsample': 0.92}, Best CV F1: 0.8715\n",
      "  Step 85 finished in 3.29s total.\n",
      "\n",
      "--- Step 86 (Predicting for window starting 2023-12-15 02:00:00) ---\n",
      "  Training window: [2040:3383]; Testing window: [3384:3651]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.26s.\n",
      "  Best Params: {'colsample_bytree': 0.72, 'max_depth': 6, 'n_estimators': 147, 'reg_alpha': 0.26, 'subsample': 0.83}, Best CV F1: 0.8346\n",
      "  Step 86 finished in 3.39s total.\n",
      "\n",
      "--- Step 87 (Predicting for window starting 2023-12-16 02:00:00) ---\n",
      "  Training window: [2064:3407]; Testing window: [3408:3675]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.26s.\n",
      "  Best Params: {'colsample_bytree': 0.72, 'max_depth': 7, 'n_estimators': 147, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8394\n",
      "  Step 87 finished in 3.38s total.\n",
      "\n",
      "--- Step 88 (Predicting for window starting 2023-12-17 02:00:00) ---\n",
      "  Training window: [2088:3431]; Testing window: [3432:3699]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.21s.\n",
      "  Best Params: {'colsample_bytree': 0.72, 'max_depth': 7, 'n_estimators': 147, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8396\n",
      "  Step 88 finished in 3.34s total.\n",
      "\n",
      "--- Step 89 (Predicting for window starting 2023-12-18 02:00:00) ---\n",
      "  Training window: [2112:3455]; Testing window: [3456:3723]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.20s.\n",
      "  Best Params: {'colsample_bytree': 0.72, 'max_depth': 6, 'n_estimators': 147, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8162\n",
      "  Step 89 finished in 3.33s total.\n",
      "\n",
      "--- Step 90 (Predicting for window starting 2023-12-19 02:00:00) ---\n",
      "  Training window: [2136:3479]; Testing window: [3480:3747]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.14s.\n",
      "  Best Params: {'colsample_bytree': 0.72, 'max_depth': 6, 'n_estimators': 147, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8236\n",
      "  Step 90 finished in 3.25s total.\n",
      "\n",
      "--- Step 91 (Predicting for window starting 2023-12-20 02:00:00) ---\n",
      "  Training window: [2160:3503]; Testing window: [3504:3771]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.06s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 6, 'n_estimators': 147, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8454\n",
      "  Step 91 finished in 3.18s total.\n",
      "\n",
      "--- Step 92 (Predicting for window starting 2023-12-21 02:00:00) ---\n",
      "  Training window: [2184:3527]; Testing window: [3528:3795]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.16s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 9, 'n_estimators': 147, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8096\n",
      "  Step 92 finished in 3.30s total.\n",
      "\n",
      "--- Step 93 (Predicting for window starting 2023-12-22 02:00:00) ---\n",
      "  Training window: [2208:3551]; Testing window: [3552:3819]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.20s.\n",
      "  Best Params: {'colsample_bytree': 0.72, 'max_depth': 6, 'n_estimators': 112, 'reg_alpha': 0.14, 'subsample': 0.83}, Best CV F1: 0.8369\n",
      "  Step 93 finished in 3.28s total.\n",
      "\n",
      "--- Step 94 (Predicting for window starting 2023-12-23 02:00:00) ---\n",
      "  Training window: [2232:3575]; Testing window: [3576:3843]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n",
      "  GridSearchCV finished in 3.22s.\n",
      "  Best Params: {'colsample_bytree': 0.68, 'max_depth': 9, 'n_estimators': 147, 'reg_alpha': 0.14, 'subsample': 0.92}, Best CV F1: 0.8222\n",
      "  Step 94 finished in 3.36s total.\n",
      "\n",
      "--- Step 95 (Predicting for window starting 2023-12-24 02:00:00) ---\n",
      "  Training window: [2256:3599]; Testing window: [3600:3867]\n",
      "  Running GridSearchCV (cv=3, scoring='f1')...\n"
     ]
    }
   ],
   "source": [
    "# B3_Configurable.py\n",
    "# Simple Predictor with Sliding Window, Per-Step HParam Tuning, and PTT\n",
    "# Configurable parameters at the top. Uses an evaluation window per step.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Modeling Imports\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, ParameterGrid\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "# --- Suppress Warnings ---\n",
    "warnings.filterwarnings('ignore', category=UndefinedMetricWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==============================================================================\n",
    "# --- Configuration ---\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Data ---\n",
    "CSV_FILE_PATH = 'BTCUSD.csv'\n",
    "N_ROWS_INPUT = 15000          # Number of most recent rows to load from CSV\n",
    "\n",
    "# --- Target Definition ---\n",
    "PREDICTION_WINDOW_HOURS = 24  # How many hours ahead to predict\n",
    "TARGET_THRESHOLD_PCT = 2.5   # Target threshold for positive class (>= this value is 1)\n",
    "\n",
    "# --- Backtesting Windowing ---\n",
    "TRAIN_WINDOW_HOURS = 24 * 7 * 8  # Size of the sliding training window (e.g., 8 weeks)\n",
    "STEP_HOURS = 24                  # How often to retrain/predict (e.g., daily)\n",
    "# Test window size as a fraction of the training window\n",
    "TEST_WINDOW_FRACTION = 0.2       # e.g., 0.2 means test window is 20% of train window\n",
    "\n",
    "# --- Model & Tuning ---\n",
    "# Fixed XGBoost parameters (not tuned in grid search)\n",
    "XGB_FIXED_PARAMS = {\n",
    "    'learning_rate': 0.086,\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'use_label_encoder': False, # Deprecated, use False\n",
    "    'random_state': 42,\n",
    "    'reg_lambda': 4.0,         # L2 Regularization (Example value)\n",
    "    'n_jobs': -1               # Use all available CPU cores\n",
    "}\n",
    "\n",
    "# Parameter grid for GridSearchCV (Keep combinations reasonable)\n",
    "XGB_PARAM_GRID_TUNE = {\n",
    "    'max_depth': [6, 7, 9],          # 3 options\n",
    "    'n_estimators': [112, 147],       # 2 options\n",
    "    'subsample': [0.83, 0.92],         # 2 options\n",
    "    'colsample_bytree': [0.68, 0.72],  # 2 options\n",
    "    'reg_alpha': [0.14, 0.26]         # L1 Regularization (2 options)\n",
    "} # Total combinations: 3 * 2 * 2 * 2 * 2 = 48\n",
    "\n",
    "# Probability Threshold Tuning Range\n",
    "PROBABILITY_THRESHOLD_RANGE = (0.10, 0.90) # Start to end (end exclusive)\n",
    "PROBABILITY_THRESHOLD_STEP = 0.05\n",
    "\n",
    "# ==============================================================================\n",
    "# --- Derived Variables (Do not change these directly) ---\n",
    "# ==============================================================================\n",
    "TRAIN_WINDOW_ROWS = TRAIN_WINDOW_HOURS\n",
    "STEP_ROWS = STEP_HOURS\n",
    "PREDICTION_WINDOW_ROWS = PREDICTION_WINDOW_HOURS\n",
    "TEST_WINDOW_ROWS = max(1, int(TEST_WINDOW_FRACTION * TRAIN_WINDOW_ROWS)) # Ensure at least 1 row\n",
    "THRESHOLD_SEARCH_RANGE = np.arange(\n",
    "    PROBABILITY_THRESHOLD_RANGE[0],\n",
    "    PROBABILITY_THRESHOLD_RANGE[1],\n",
    "    PROBABILITY_THRESHOLD_STEP\n",
    ")\n",
    "try:\n",
    "    grid_combinations = len(list(ParameterGrid(XGB_PARAM_GRID_TUNE))) # Calculate grid size\n",
    "except TypeError: # Handle case where grid might be None or empty\n",
    "    grid_combinations = 1\n",
    "\n",
    "# ==============================================================================\n",
    "# --- Script Start ---\n",
    "# ==============================================================================\n",
    "\n",
    "# --- 1. Load Data and Select Last Rows ---\n",
    "print(\"--- Data Loading ---\")\n",
    "print(f\"Loading data from: {CSV_FILE_PATH}\")\n",
    "try:\n",
    "    df_full = pd.read_csv(CSV_FILE_PATH)\n",
    "    df_full = df_full.sort_values(by='unix', ascending=True).reset_index(drop=True)\n",
    "    if 'date' in df_full.columns:\n",
    "        try: df_full['date'] = pd.to_datetime(df_full['date'])\n",
    "        except Exception as e_date: print(f\"Warning: Date parse error: {e_date}\")\n",
    "\n",
    "    if len(df_full) < N_ROWS_INPUT:\n",
    "        print(f\"Warning: Full dataset ({len(df_full)} rows) < {N_ROWS_INPUT}. Using all data.\")\n",
    "        df = df_full.copy()\n",
    "    else:\n",
    "        df = df_full.iloc[-N_ROWS_INPUT:].reset_index(drop=True)\n",
    "        print(f\"Selected last {len(df)} rows.\")\n",
    "except FileNotFoundError: print(f\"Error: {CSV_FILE_PATH} not found.\"); exit()\n",
    "except Exception as e: print(f\"Error loading data: {e}\"); exit()\n",
    "\n",
    "# --- 2. Feature Engineering ---\n",
    "print(\"\\n--- Feature Engineering ---\")\n",
    "start_fe = time.time()\n",
    "base_cols_numeric = ['open', 'high', 'low', 'close', 'Volume BTC', 'Volume USD']\n",
    "for col in base_cols_numeric:\n",
    "    if col in df.columns: df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    else: print(f\"Warning: Missing base column '{col}'\"); df[col] = 0\n",
    "if df[['open', 'high', 'low', 'close']].isnull().any().any():\n",
    "    print(\"Warning: OHLC NaNs found. Dropping rows.\"); df = df.dropna(subset=['open', 'high', 'low', 'close'])\n",
    "if df.empty: exit(\"Error: Empty DF after initial OHLC NaN drop.\")\n",
    "# --- Feature Functions ---\n",
    "def garman_klass_volatility(o, h, l, c, w):\n",
    "    with np.errstate(divide='ignore', invalid='ignore'): log_hl=np.log(h/l.replace(0,np.nan)); log_co=np.log(c/o.replace(0,np.nan))\n",
    "    gk = 0.5*(log_hl**2) - (2*np.log(2)-1)*(log_co**2); gk = gk.fillna(0)\n",
    "    rm = gk.rolling(w, min_periods=max(1,w//2)).mean(); rm = rm.clip(lower=0); return np.sqrt(rm)\n",
    "def parkinson_volatility(h, l, w):\n",
    "    with np.errstate(divide='ignore', invalid='ignore'): log_hl_sq = np.log(h/l.replace(0,np.nan))**2\n",
    "    log_hl_sq = log_hl_sq.fillna(0); rs = log_hl_sq.rolling(w,min_periods=max(1,w//2)).sum()\n",
    "    f = 1/(4*np.log(2)*w) if w>0 else 0; return np.sqrt(f*rs)\n",
    "# --- Feature Calculations (Example subset, add all relevant ones from B2_V2) ---\n",
    "df['price_change_1h_temp'] = df['close'].pct_change()\n",
    "with np.errstate(divide='ignore', invalid='ignore'):\n",
    "    df['price_range_pct'] = (df['high'] - df['low']) / df['close'].replace(0, np.nan) * 100\n",
    "    df['oc_change_pct'] = (df['close'] - df['open']) / df['open'].replace(0, np.nan) * 100\n",
    "df['garman_klass_12h'] = garman_klass_volatility(df['open'],df['high'],df['low'],df['close'],12)\n",
    "df['parkinson_3h'] = parkinson_volatility(df['high'],df['low'],3)\n",
    "min_periods_rolling = 2\n",
    "df['ma_3h'] = df['close'].rolling(3, min_periods=min_periods_rolling).mean()\n",
    "df['rolling_std_3h'] = df['close'].rolling(3, min_periods=min_periods_rolling).std()\n",
    "lag_periods_price = [3, 6, 12, 24, 48, 72, 168]; lag_periods_volume = [3, 6, 12, 24]\n",
    "for lag in lag_periods_price: df[f'lag_{lag}h_price_return'] = df['price_change_1h_temp'].shift(lag) * 100\n",
    "df['volume_return_1h'] = df['Volume BTC'].pct_change() * 100\n",
    "for lag in lag_periods_volume: df[f'lag_{lag}h_volume_return'] = df['volume_return_1h'].shift(lag)\n",
    "ma_periods = [6, 12, 24, 48, 72, 168]; std_periods = [6, 12, 24, 48, 72, 168]\n",
    "for p in ma_periods: df[f'ma_{p}h'] = df['close'].rolling(p, min_periods=max(min_periods_rolling, p//2)).mean()\n",
    "for p in std_periods: df[f'rolling_std_{p}h'] = df['price_change_1h_temp'].rolling(p, min_periods=max(min_periods_rolling, p//2)).std() * 100\n",
    "df['prev_close']=df['close'].shift(1); df['hml']=df['high']-df['low']; df['hmpc']=np.abs(df['high']-df['prev_close']); df['lmpc']=np.abs(df['low']-df['prev_close'])\n",
    "df['tr']=df[['hml','hmpc','lmpc']].max(axis=1)\n",
    "atr_periods = [14, 24, 48]\n",
    "for p in atr_periods: df[f'atr_{p}h'] = df['tr'].rolling(p, min_periods=max(1,p//2)).mean()\n",
    "df = df.drop(columns=['prev_close', 'hml', 'hmpc', 'lmpc', 'tr'])\n",
    "epsilon = 1e-9\n",
    "for p in [24, 48, 168]:\n",
    "    mc=f'ma_{p}h'; df[f'close_div_ma_{p}h'] = df['close']/(df[mc]+epsilon) if mc in df else np.nan\n",
    "if 'ma_12h' in df and 'ma_48h' in df: df['ma12_div_ma48'] = df['ma_12h']/(df['ma_48h']+epsilon)\n",
    "else: df['ma12_div_ma48']=np.nan\n",
    "if 'ma_24h' in df and 'ma_168h' in df: df['ma24_div_ma168'] = df['ma_24h']/(df['ma_168h']+epsilon)\n",
    "else: df['ma24_div_ma168']=np.nan\n",
    "if 'rolling_std_12h' in df and 'rolling_std_72h' in df: df['std12_div_std72'] = df['rolling_std_12h']/(df['rolling_std_72h']+epsilon)\n",
    "else: df['std12_div_std72']=np.nan\n",
    "if 'price_range_pct' in df: df['volume_btc_x_range'] = df['Volume BTC'] * df['price_range_pct']\n",
    "else: df['volume_btc_x_range']=np.nan\n",
    "if 'rolling_std_3h' in df: df['rolling_std_3h_sq'] = df['rolling_std_3h']**2\n",
    "else: df['rolling_std_3h_sq']=np.nan\n",
    "if 'price_change_1h_temp' in df: df['price_return_1h_sq'] = df['price_change_1h_temp']**2 * 10000\n",
    "else: df['price_return_1h_sq']=np.nan\n",
    "if 'rolling_std_12h' in df: df['rolling_std_12h_sqrt'] = np.sqrt(df['rolling_std_12h'].clip(lower=0)+epsilon)\n",
    "else: df['rolling_std_12h_sqrt']=np.nan\n",
    "cols_to_drop_intermediate = ['price_change_1h_temp', 'volume_return_1h']\n",
    "df = df.drop(columns=[col for col in cols_to_drop_intermediate if col in df.columns])\n",
    "# Add 'symbol' column (if not already present from CSV header)\n",
    "if 'symbol' not in df.columns: df['symbol'] = 'BTCUSD' # Example\n",
    "print(f\"Feature engineering complete. Took {time.time() - start_fe:.2f} seconds.\")\n",
    "print(f\"Columns after features: {df.shape[1]}\")\n",
    "\n",
    "# --- 3. Define Target Variable ---\n",
    "print(\"\\n--- Target Definition ---\")\n",
    "print(f\"Defining target as {PREDICTION_WINDOW_HOURS}h future return >= {TARGET_THRESHOLD_PCT}%...\")\n",
    "target_col = f'target_return_{PREDICTION_WINDOW_HOURS}h' # Dynamic target column name\n",
    "df[target_col] = df['close'].shift(-PREDICTION_WINDOW_ROWS).sub(df['close']).div(df['close'].replace(0, np.nan)).mul(100)\n",
    "\n",
    "# --- 4. Prepare Data for Modeling ---\n",
    "print(\"\\n--- Data Preparation ---\")\n",
    "cols_to_keep_final = ['unix', 'date', target_col, 'symbol']\n",
    "potential_feature_cols = [col for col in df.columns if col not in cols_to_keep_final]\n",
    "numeric_feature_cols = df[potential_feature_cols].select_dtypes(include=np.number).columns.tolist()\n",
    "final_feature_cols = [col for col in numeric_feature_cols if col not in base_cols_numeric]\n",
    "cols_to_select = final_feature_cols + [col for col in cols_to_keep_final if col in df.columns]\n",
    "df_model_ready = df[cols_to_select].copy()\n",
    "\n",
    "initial_rows = len(df_model_ready); df_model_ready = df_model_ready.dropna(); final_rows = len(df_model_ready)\n",
    "print(f\"NaN Handling: Dropped {initial_rows - final_rows} rows.\")\n",
    "\n",
    "numeric_cols_final = df_model_ready[final_feature_cols].select_dtypes(include=np.number).columns.tolist()\n",
    "inf_mask = np.isinf(df_model_ready[numeric_cols_final]); inf_count = inf_mask.sum().sum()\n",
    "if inf_count > 0:\n",
    "    print(f\"Replacing {inf_count} infinites...\"); df_model_ready.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    rows_b4 = len(df_model_ready); df_model_ready = df_model_ready.dropna(); print(f\"Dropped {rows_b4 - len(df_model_ready)} more rows.\")\n",
    "if df_model_ready.empty: exit(\"Error: DataFrame empty after NaN/Inf handling.\")\n",
    "\n",
    "X = df_model_ready[final_feature_cols]\n",
    "y_binary = (df_model_ready[target_col] >= TARGET_THRESHOLD_PCT).astype(int)\n",
    "\n",
    "if 'date' in df_model_ready.columns and pd.api.types.is_datetime64_any_dtype(df_model_ready['date']): timestamps = df_model_ready['date']\n",
    "elif 'unix' in df_model_ready.columns: timestamps = pd.to_datetime(df_model_ready['unix'], unit='ms')\n",
    "else: print(\"Warning: No date/unix. Using index.\"); timestamps = pd.Series(df_model_ready.index)\n",
    "\n",
    "print(f\"Final feature matrix shape: {X.shape}\"); print(f\"Target vector shape: {y_binary.shape}\"); print(f\"Using {len(final_feature_cols)} features.\")\n",
    "\n",
    "# --- 5. SLIDING Window Backtesting with Per-Step HParam Tuning ---\n",
    "print(\"\\n--- Starting SLIDING Window Backtest with Per-Step HParam Tuning ---\")\n",
    "print(\"!!! WARNING: This will be significantly slower due to GridSearchCV in each step !!!\")\n",
    "if len(X) < TRAIN_WINDOW_ROWS + STEP_ROWS:\n",
    "    print(f\"Error: Not enough data ({len(X)}) for train window ({TRAIN_WINDOW_ROWS}) + step ({STEP_ROWS}).\"); exit()\n",
    "\n",
    "all_predictions_proba = []; all_actual = []; backtest_timestamps = []\n",
    "all_best_params = [] # Store best params for each step\n",
    "num_steps = 0\n",
    "start_index_loop = TRAIN_WINDOW_ROWS; end_index_loop = len(X) - TEST_WINDOW_ROWS + 1 # Adjust end index for test window\n",
    "\n",
    "print(f\"Train Window: {TRAIN_WINDOW_ROWS} rows, Step: {STEP_ROWS} rows, Test Window: {TEST_WINDOW_ROWS} rows, Tuning Grid Size: {grid_combinations}\")\n",
    "loop_start_time = time.time()\n",
    "\n",
    "for i in range(start_index_loop, end_index_loop, STEP_ROWS):\n",
    "    step_start_time = time.time()\n",
    "    train_idx_start = i - TRAIN_WINDOW_ROWS\n",
    "    train_idx_end = i\n",
    "    test_idx_start = i\n",
    "    test_idx_end = i + TEST_WINDOW_ROWS # Define end of test window\n",
    "\n",
    "    if test_idx_end > len(X): # Ensure test window doesn't exceed data\n",
    "        print(f\"Stopping loop: Test window end ({test_idx_end}) exceeds available data ({len(X)}).\")\n",
    "        break\n",
    "\n",
    "    # Get train and test sets\n",
    "    X_train_roll = X.iloc[train_idx_start : train_idx_end]\n",
    "    y_train_roll = y_binary.iloc[train_idx_start : train_idx_end]\n",
    "    X_test_roll = X.iloc[test_idx_start : test_idx_end]  # Test on the window\n",
    "    y_test_roll_actual_series = y_binary.iloc[test_idx_start : test_idx_end] # Actual labels for the window\n",
    "    current_timestamp = timestamps.iloc[test_idx_start] # Timestamp for the start of the test window\n",
    "\n",
    "    if X_train_roll.empty or len(np.unique(y_train_roll)) < 2:\n",
    "        print(f\"Warning: Skipping step starting at index {i}. Invalid training data.\"); continue\n",
    "\n",
    "    print(f\"\\n--- Step {num_steps + 1} (Predicting for window starting {current_timestamp}) ---\")\n",
    "    print(f\"  Training window: [{train_idx_start}:{train_idx_end-1}]; Testing window: [{test_idx_start}:{test_idx_end-1}]\")\n",
    "\n",
    "    # --- Hyperparameter Tuning ---\n",
    "    print(f\"  Running GridSearchCV (cv=3, scoring='f1')...\")\n",
    "    grid_search_start_time = time.time()\n",
    "    try:\n",
    "        estimator = xgb.XGBClassifier(**XGB_FIXED_PARAMS)\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=estimator, param_grid=XGB_PARAM_GRID_TUNE, scoring='f1',\n",
    "            cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=i),\n",
    "            n_jobs=-1, verbose=0\n",
    "        )\n",
    "        grid_search.fit(X_train_roll, y_train_roll) # Fit GridSearch\n",
    "        best_params_step = grid_search.best_params_\n",
    "        best_score_step = grid_search.best_score_\n",
    "        print(f\"  GridSearchCV finished in {time.time() - grid_search_start_time:.2f}s.\")\n",
    "        print(f\"  Best Params: {best_params_step}, Best CV F1: {best_score_step:.4f}\")\n",
    "        all_best_params.append(best_params_step) # Store best params\n",
    "\n",
    "        # --- Fit final model for the step ---\n",
    "        final_model_params = {**XGB_FIXED_PARAMS, **best_params_step}\n",
    "        model_roll = xgb.XGBClassifier(**final_model_params)\n",
    "        model_roll.fit(X_train_roll, y_train_roll, verbose=False) # Fit final model\n",
    "\n",
    "        # --- Predict probabilities for the entire test window ---\n",
    "        prob_roll_window = model_roll.predict_proba(X_test_roll)[:, 1] # Get probs for all points in test window\n",
    "\n",
    "        # --- Store results ---\n",
    "        # Store ALL probabilities and actuals for the test window for later PTT evaluation\n",
    "        all_predictions_proba.extend(prob_roll_window)\n",
    "        all_actual.extend(y_test_roll_actual_series.tolist())\n",
    "        backtest_timestamps.extend(timestamps.iloc[test_idx_start : test_idx_end].tolist()) # Store all timestamps\n",
    "        num_steps += 1\n",
    "\n",
    "    except Exception as e_step:\n",
    "        print(f\"!! Error during GridSearch/Fit/Predict at step starting {i}: {e_step}\"); traceback.print_exc(); continue\n",
    "\n",
    "    step_end_time = time.time()\n",
    "    # Note: num_steps counts completed training/prediction cycles\n",
    "    print(f\"  Step {num_steps} finished in {step_end_time - step_start_time:.2f}s total.\")\n",
    "\n",
    "loop_end_time = time.time()\n",
    "print(f\"\\nBacktesting loop finished. Completed {num_steps} steps (each predicting {TEST_WINDOW_ROWS} points) in {(loop_end_time - loop_start_time)/60:.2f} minutes.\")\n",
    "\n",
    "# --- 6. Evaluate Backtesting Results with PTT ---\n",
    "# Now PTT is applied across ALL individual predictions made during the backtest\n",
    "if num_steps > 0 and len(all_predictions_proba) == len(all_actual):\n",
    "    print(\"\\n--- Evaluating Results with Probability Threshold Tuning (on all points) ---\")\n",
    "    print(f\"Threshold search range: {THRESHOLD_SEARCH_RANGE}\")\n",
    "    best_threshold = 0.5; best_f1_thresh = -1.0\n",
    "    results_per_threshold = {}\n",
    "    probabilities_np = np.array(all_predictions_proba); actual_np = np.array(all_actual)\n",
    "\n",
    "    for t in THRESHOLD_SEARCH_RANGE:\n",
    "        predictions_thresh = (probabilities_np >= t).astype(int)\n",
    "        # Handle edge cases for metrics calculation (same logic as before)\n",
    "        if np.sum(actual_np) == 0 and np.sum(predictions_thresh) == 0: acc_t, pre_t, rec_t, f1_t = 1.0, 1.0, 1.0, 1.0\n",
    "        elif np.sum(actual_np) > 0 and np.sum(predictions_thresh) == 0: acc_t = accuracy_score(actual_np, predictions_thresh); pre_t, rec_t, f1_t = 0.0, 0.0, 0.0\n",
    "        elif np.sum(actual_np) == 0 and np.sum(predictions_thresh) > 0: acc_t = accuracy_score(actual_np, predictions_thresh); pre_t, rec_t, f1_t = 0.0, 0.0, 0.0\n",
    "        else:\n",
    "             acc_t = accuracy_score(actual_np, predictions_thresh)\n",
    "             pre_t = precision_score(actual_np, predictions_thresh, zero_division=0)\n",
    "             rec_t = recall_score(actual_np, predictions_thresh, zero_division=0)\n",
    "             f1_t = f1_score(actual_np, predictions_thresh, zero_division=0)\n",
    "        results_per_threshold[round(t, 2)] = {'f1': f1_t, 'acc': acc_t, 'pre': pre_t, 'rec': rec_t}\n",
    "        if f1_t >= best_f1_thresh: best_f1_thresh = f1_t; best_threshold = t\n",
    "\n",
    "    print(f\"\\nBest Threshold found: {best_threshold:.2f} (Yielding F1 Score: {best_f1_thresh:.4f})\")\n",
    "\n",
    "    final_predictions_optimized = (probabilities_np >= best_threshold).astype(int)\n",
    "    final_accuracy = accuracy_score(actual_np, final_predictions_optimized)\n",
    "    final_precision = precision_score(actual_np, final_predictions_optimized, zero_division=0)\n",
    "    final_recall = recall_score(actual_np, final_predictions_optimized, zero_division=0)\n",
    "    final_f1 = f1_score(actual_np, final_predictions_optimized, zero_division=0)\n",
    "\n",
    "    print(\"\\n--- Final Performance Metrics (Optimized Threshold on all points) ---\")\n",
    "    print(f\"Target: {PREDICTION_WINDOW_HOURS}h return >= {TARGET_THRESHOLD_PCT}%\")\n",
    "    print(f\"Windowing: Train={TRAIN_WINDOW_ROWS} rows, Step={STEP_ROWS} rows, Test Window={TEST_WINDOW_ROWS} rows\")\n",
    "    print(f\"Total Individual Predictions Evaluated: {len(actual_np)}\")\n",
    "    print(f\"Overall Accuracy:  {final_accuracy:.4f}\")\n",
    "    print(f\"Overall Precision: {final_precision:.4f}\")\n",
    "    print(f\"Overall Recall:    {final_recall:.4f}\")\n",
    "    print(f\"Overall F1 Score:  {final_f1:.4f}\")\n",
    "\n",
    "    if 0.5 in results_per_threshold:\n",
    "        res_def = results_per_threshold[0.5]\n",
    "        print(f\"(Compare: Default 0.5 Thresh -> F1:{res_def['f1']:.4f})\")\n",
    "\n",
    "    # --- 7. Plot Cumulative Accuracy ---\n",
    "    print(\"\\nPlotting cumulative accuracy (optimized threshold)...\")\n",
    "    # Ensure backtest_timestamps matches the length of predictions/actuals\n",
    "    if len(backtest_timestamps) != len(actual_np):\n",
    "         print(f\"Warning: Timestamp length ({len(backtest_timestamps)}) mismatch with prediction length ({len(actual_np)}). Skipping plot.\")\n",
    "    else:\n",
    "        # Calculate cumulative accuracy based on OPTIMIZED predictions\n",
    "        cumulative_accuracy_list_optimized = (np.cumsum(final_predictions_optimized == actual_np) / np.arange(1, len(actual_np) + 1))\n",
    "        try:\n",
    "            plt.figure(figsize=(14, 7))\n",
    "            plt.plot(backtest_timestamps, cumulative_accuracy_list_optimized, marker='.', linestyle='-', markersize=2, label='Cumulative Accuracy (Optimized)') # Smaller markersize\n",
    "            rolling_window_plot = min(max(len(actual_np) // 10, 50), 500) # Adjust rolling window size\n",
    "            if len(actual_np) > rolling_window_plot:\n",
    "                 # Create a DataFrame for easier rolling calculation with time index\n",
    "                 results_df = pd.DataFrame({'actual': actual_np, 'pred': final_predictions_optimized}, index=pd.to_datetime(backtest_timestamps))\n",
    "                 results_df['correct'] = (results_df['actual'] == results_df['pred']).astype(int)\n",
    "                 rolling_acc = results_df['correct'].rolling(window=f'{rolling_window_plot}H').mean() # Use time window if possible, else row window\n",
    "                 # rolling_acc = pd.Series(cumulative_accuracy_list_optimized).rolling(window=rolling_window_plot).mean() # Alternative row-based rolling\n",
    "\n",
    "                 # Plot rolling accuracy, ensuring index alignment\n",
    "                 plt.plot(rolling_acc.index, rolling_acc, linestyle='--', color='red', label=f'Rolling Acc ({rolling_window_plot} points/hours)')\n",
    "\n",
    "            plt.title(f'B3 Backtest (Train:{TRAIN_WINDOW_ROWS}, Step:{STEP_ROWS}, Test:{TEST_WINDOW_ROWS}, Tuned) - Best Thresh: {best_threshold:.2f}')\n",
    "            plt.xlabel('Timestamp'); plt.ylabel('Accuracy')\n",
    "            min_y_plot = max(0.0, np.min(cumulative_accuracy_list_optimized) - 0.05 if len(cumulative_accuracy_list_optimized)>0 else 0.4)\n",
    "            max_y_plot = min(1.0, np.max(cumulative_accuracy_list_optimized) + 0.05 if len(cumulative_accuracy_list_optimized)>0 else 0.8)\n",
    "            plt.ylim(min_y_plot, max_y_plot)\n",
    "            plt.grid(True, linestyle='--', alpha=0.6); plt.legend(); plt.xticks(rotation=30, ha='right'); plt.tight_layout(); plt.show()\n",
    "        except Exception as e_plot: print(f\"Error plotting: {e_plot}\")\n",
    "\n",
    "else:\n",
    "    print(\"No predictions were made/stored, cannot evaluate or plot.\")\n",
    "\n",
    "print(\"\\nScript B3_Configurable.py finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['open', 'high', 'low', 'close', 'Volume BTC', 'Volume USD',\n",
       "       'price_range_pct', 'oc_change_pct', 'garman_klass_12h', 'parkinson_3h',\n",
       "       'ma_3h', 'rolling_std_3h', 'lag_3h_price_return', 'lag_6h_price_return',\n",
       "       'lag_12h_price_return', 'lag_24h_price_return', 'lag_48h_price_return',\n",
       "       'lag_72h_price_return', 'lag_168h_price_return', 'volume_return_1h',\n",
       "       'lag_3h_volume_return', 'lag_6h_volume_return', 'lag_12h_volume_return',\n",
       "       'lag_24h_volume_return', 'ma_6h', 'ma_12h', 'ma_24h', 'ma_48h',\n",
       "       'ma_72h', 'ma_168h', 'rolling_std_6h', 'rolling_std_12h',\n",
       "       'rolling_std_24h', 'rolling_std_48h', 'rolling_std_72h',\n",
       "       'rolling_std_168h', 'atr_14h', 'atr_24h', 'atr_48h', 'close_div_ma_24h',\n",
       "       'close_div_ma_48h', 'close_div_ma_168h', 'ma12_div_ma48',\n",
       "       'ma24_div_ma168', 'std12_div_std72', 'volume_btc_x_range',\n",
       "       'rolling_std_3h_sq', 'price_return_1h_sq', 'rolling_std_12h_sqrt'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What Now?\n",
    "\n",
    "This is great progress! It tells you that predicting 12-hour direction is a much more promising path with your data and feature types.\n",
    "\n",
    "Stick with the Simpler Structure (for now): Keep the single model (XGBoost) and the expanding window backtest for now.\n",
    "\n",
    "Optimize This Setup:\n",
    "\n",
    "Apply VIF: Now that you have a working model structure and a seemingly viable target, apply VIF filtering (e.g., threshold 5 or even your strict 1.69) to the features generated in this simpler script. Does reducing collinearity now improve the already decent results?\n",
    "\n",
    "Tune Hyperparameters: Tune the XGBoost parameters (n_estimators, max_depth, learning_rate, reg_alpha, reg_lambda, subsample, colsample_bytree, min_child_weight) using a method like Optuna or RandomizedSearchCV within the rolling backtest loop (similar to how the meta-learner was tuned, but now for the single main model).\n",
    "\n",
    "Experiment with Target Horizon: Is 12 hours optimal for the >0% target? Try 8 hours, 24 hours.\n",
    "\n",
    "Experiment with Training Window: Does the expanding window work best, or would a large sliding window perform better for this target?\n",
    "\n",
    "You've found a much better baseline. Now optimize it systematically!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
